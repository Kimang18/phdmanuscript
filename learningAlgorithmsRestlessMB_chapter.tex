\begingroup
\let\clearpage\relax

%\dominitoc
\chapter{Learning in Average Reward Restless Markovian Bandits}
\label{ch:learning_restless}

In the previous chapter, we adapt a few learning algorithms to discounted rested Markovian bandits and showed that their regrets, which are upper bounded sublinearly in the number of arms, match the minimax Bayesian regret that we derived in the chapter.
We also showed that any optimistic index policy that computes optimistic index on each arm independently from the other arms does not guarantee the optimism in face of uncertainty (OFU) principle.
In this chapter, we consider the learning problem in restless Markovian bandit with average reward criterion.
Such a bandit suffers from the curse of dimensionality and general-purpose learning algorithm are not efficient when applied to restless bandit. Recently, a few learning algorithms have been specifically designed to work for restless bandits. Yet, they most often work only for very particular subclasses of restless bandits, or they hold under conditions that are very hard to verify.
Hence, in this chapter, we provide some explanations why the performance of a learning algorithm for a restless bandit depends, in general, on some structural conditions (like diameter or bias) of the global MDP. We show that the properties of the local arms (like ergodic or small diameter) do not, in general, imply similar properties for the global MDPs. This implies that defining a class of restless bandit that has desirable properties (like small diameter) is difficult. 

In Section~\ref{}, we...

\section{Introduction}

Restless Markovian bandits are the specific MDPs that manifest the curse of dimensionality.
The obvious effect of the curse is the bandit state size that is exponential in the number of arms.
%Moreover, not only the state size that is cursed, but also the MDP parameters such as diameter.
%However, to the best of our knowledge, there is yet no clear dependency between the diameter or mixing time of the global MDP and the number of arms in general restless Markovian bandit.

For learning generic MDPs with average reward criterion, the current best algorithms have a regret over $T$ time steps bounded by $\landauO(\sqrt{DSAT})$ in the unknown MDP with state size $S$, action size $A$, and diameter $D$ (see Table~\ref{ch:rl:tab:infinite} for more algorithms with regret guarantee).
This regret is bound valid for communicating MDPs.
To what we have observed, the parameters of the MDP that appear in regret upper bound are:
\begin{itemize}
    \item diameter $D$, defined by Definition~\ref{ch:rl:defn:diameter}, in \eg, \cite{jaksch2010near, fruit2020improved, tossou2019near}
    \item span of optimal bias $H\ge sp(\vh^*)$ in \eg, \cite{bartlett2012regal, ouyang2017learning, fruit2018efficient, zhang2019regret}
    \item mixing time $t_{mix}$, defined by Definition~\ref{ch:restless:defn:mixing_time}, in \eg, \cite{ortner2020regret}.
\end{itemize}
Applying these algorithms to learn restless Markovian bandits with average reward criterion provokes a few critical issues:
\begin{itemize}
    %\item verifying the structure of restless bandit is computationally ``expensive'' 
    \item the state size of restless bandit is exponential in the number of arms which makes the regret not scalable with the number of arms
    \item the diameter $D$ (if defined), mixing time $t_{mix}$ (if defined), or span of the optimal bias $H$ may also be exponential in the number of arms
    \item compute an optimal policy in restless bandit is PSPACE-hard \cite{papadimitriou1994complexity}, let alone the computation of optimistic policy
\end{itemize}
%These parameters are finite for MDPs with specific structure.
%That is, the mixing time is defined for ergodic MDPs and the diameter of a MDP is finite if and only if the MDP is communicating.
%These algorithms are then applicable to restless Markovian bandits only if the latter have the specified structure.

%It is then reasonable to assume some structure properties of the bandit.
For partially observed restless bandit, the work of \cite{ortner2012regret} derives colored-UCRL algorithm, a modified version of UCRL2, that achieves a regret sublinear in time.
Colored-UCRL successfully removes the exponentiality in the number of arms from the state size of the bandit in its regret.
However, the mixing time parameter yet appears in the regret bound and the dependency between the mixing time and the number of arms is unclear.
The same discussion goes to the work of \cite{jung2019thompson} that adapts TSDE to the same setting of restless bandit in \cite{ortner2012regret}.
To the best of our knowledge, Restless-UCB of \cite{wang2020restless} is the first algorithm that has a regret provably bounded linearly in the number of arms for partially observed restless bandit.
Yet, we must mention that this result is valid for a very restrictive class of restless bandit in which each arm is the birth-death Markov reward process.
Also, Restless-UCB requires a generative model to be able to uniformly explore the dynamic of each arm before commiting on its optimistic planning.
All of these works also assume an oracle that knows how to compute an optimal policy given a restless bandit.

The work of \cite{akbarzadeh2022learning} adapts TSDE to the fully observed restless bandit problem and successfully removes the exponentiality in the number of arms from the state size of the bandit in the Bayesian regret of the modified version, namely Restless-TSDE.
However, the ergodicity coefficient of the global MDP appears in the regret bound and the dependency between such a coefficient and the number of arms is unclear.
In general, the ergodicity coefficient provides an upper bound on the mixing time of the MDP.

Finally, due to the curse of dimensionality, verifying the structure of the bandit may be computationally expensive, let alone the learning aspect.
This inspires us to study the implication of structure of arms in the structure of the bandit because verifying the structure of all arms is done linearly in the number of arms.

%That is, in average reward criterion, the structure of the MDP plays an important role in the performance of the learning algorithms.


%Reinforcement learning algorithms work depending on the assumption of MDP structure such as ergodic, communicating, and weakly communicating.

%(here should motivate the problem + add related work about learning and why it is related to diameters and other properties)

%(We should define local properties and global properties).

We show that:
\begin{itemize}
    \item Local ergodic does not imply global weakly communicating because of periodicity 
    \item Local ergodic (= recurrent + aperiodic) does not imply globally ergodic (for now, our only example is unichain. Can we do more?)
    \item For globally ergodic, diameter can be exponential in the number of arms
\end{itemize}

We recall Table~\ref{ch:rl:tab:infinite} below

\begin{table}[ht]
\begin{tabular}{|l|l|l|l|}
\hline
Algorithm & Regret & Assump. on $M$ & Policy comp.            \\ \hline
UCRL2 \cite{jaksch2010near}       & $\tilde{\landauO}(DS\sqrt{AT})$ & comm.         & EVI \\ 
REGAL \cite{bartlett2012regal}      & $\tilde{\landauO}(HS\sqrt{AT})$  & weakly comm. & Intractable   \\ 
TSDE \cite{ouyang2017learning}        & $\tilde{\landauO}(HS\sqrt{AT})$ & weakly comm. & VI       \\ 
SCAL \cite{fruit2018efficient}        & $\tilde{\landauO}(HS\sqrt{AT})$ & weakly comm. & modified EVI           \\
OSP \cite{ortner2020regret}         & $\tilde{\landauO}(\sqrt{t_{mix}SAT})$ & ergodic           & Intractable   \\ 
UCRL2B \cite{fruit2020improved}      & $\tilde{\landauO}(\sqrt{\Gamma DSAT})$ & comm.      & EVI                           \\
%KL-UCRL     & $\tilde{O}$(\sqrt{S\sum_{x,a}\sV^*_{x,a}T}) & ergodic                     & modified EVI                  \\ \hline
EBF \cite{zhang2019regret}         & $\tilde{\landauO}(\sqrt{HSAT})$ & weakly comm.     & Intractable   \\
UCRL-V \cite{tossou2019near}         & $\tilde{\landauO}(\sqrt{DSAT})$ & comm.     & modified EVI   \\ \hline
Lower bound & $\landauOmega(\sqrt{DSAT})$ \cite{jaksch2010near} &                              &                               \\ \hline
\end{tabular}
\caption{The quest to minimax regret lower bound for model-based RL algorithms in infinite horizon average reward model with $S$ states, $A$ actions, and $T$ steps.
$D$ is the diameter of the MDP, $H\ge sp(\vh^*)$ is the upper bound on the span of the optimal bias, $t_{mix}$ is the mixing time of the MDP and $\Gamma$ is the upper bound on the number of next possible states.
EVI stands for Extended value iteration \cite{jaksch2010near}, VI for value iteration, assump. for assumption, comm. for communicating, and comp. for computation.
Intractable here means that there is no efficient implementations.
}
\label{ch:restless:tab:infinite}
\end{table}

%A few existing works have tried to remove the exponentiality in the state size of restless bandit such as \cite{jung2019regret, ortner2012regret, wang2020restless} for partially observed setting and \cite{akbarzadeh2022learning} for fully observed setting.
%However, removing the exponentiality of the number of arms in the $D, t_{mix}$, or $H$ remains unclear.
%Moreover, these quantities are defined only if the MDP has a specific structure.
%For instance, the mixing time is defined only if the MDP is ergodic.
%Such an assumption is costly to verified in restless bandit due the curse of dimensionality in the state size of the global MDP.

\section{Undiscounted restless Markovian bandit}
\label{ch:restless:sec:restless}

We consider a restless Markovian bandit having $n$ arms.
Each arm $\langle\gS_i, \{0,1\}, r_i, p_i\rangle$ is a MDP with finite state space $\gS_i$ of size $S_i$ and binary action space $\{0,1\}$ where $0$ denotes the action ``rest'' and $1$ denotes the action ``activate''.
If arm $i$ is in state $s_i$ and the decision maker executes $a_i\in\{0,1\}$, the arm incurs a random reward with expected value $r_i(s_i,a_i)$ and transitions to state $s'_i\in\gS_i$ with probability $p_i(s'_i\mid s_i,a_i)$.

The sequential decision problem is presented as the following.
At time step $1$, the state of all arms denoted by $\vs_1:=(s_{1,1},\dots,s_{1,n})$ is sampled according to some initial distribution $\rho$ over the state space $\gX:=\gS_1\times\dots\times\gS_n$.
At time step $t\ge1$, the decision maker observes the current state of all arms denoted by $\vs_t:=(s_{t,1},\dots,s_{t,n})$ and activates exactly $m$ arms encoded by action $\va_t:=(a_{t,1},\dots,a_{t,n})$ such that $\va_t\in\{0,1\}^n$ and $\sum_{i=1}^{n} a_{t,i}=m$ where $m\in[n]$ is constant over time.
%The decision maker then receives a random reward $r_t=\sum_{i=1}^{n}r_{t,i}$ where $r_{t,i}$ is a random reward from arm $i$.
Each arm $i$ incurs then a random reward $r_{t,i}$ and makes
a transition to new state $s_{t+1,i}$ in function of $s_{t,i}$ and $a_{t,i}$ but independently from the other arms.
So, the restless Markovian bandit is a specific MDP -- that we denote by $M$ -- whose state space is $\gX$ and action space is $\gA(m):=\{\va\in\{0,1\}^n : \sum_{i=1}^{n}a_i =m\}$.
We say that $M$ is the \emph{global} MDP and its arm is the \emph{local} MDP.

The decision maker wants to compute a policy $\pi:\gX\mapsto\gA(m)$ that maximizes the gain as defined in Section~\ref{ch:mdp:sec:gain}: for any $\vs\in\gX$,
\begin{equation}
    \label{ch:restless:eq:obj}
    g^\pi(\vs):=\lim_{T\to+\infty}\frac1T \E^\pi\left[ \sum_{t=1}^{T} \sum_{i=1}^nr_{t,i} \mid \vs_1=\vs\right].
\end{equation}

As presented in Chapter~\ref{ch:mdp}, if the MDP $M$ has finite state and action spaces, then an optimal policy $\pi^*$ such that for all $\vs\in\gX, g^{\pi^*}(\vs)=g^*(\vs):=\max_{\pi}g^{\pi}(\vs)$ exists and is deterministic.
However, it is shown in \cite[Theorem~4]{papadimitriou1994complexity} that computing an optimal policy in undiscounted restless bandit $M$ is PSPACE-hard.

%In about 1980, Peter Whittle proposed a heuristic in the form of largest index rule, later known as \textbf{Whittle index policy}, for undiscounted restless bandit problem.
%The computational complexity of this heuristic is linear in the number of arms which makes it scalable for problems with large number of arms.  
%We discuss more about this index policy in the following section.
%Similar to what is done in Section~\ref{ssec:rested_formul}, we assume that the state space of the arms are pairwise distinct: $\gS_i\cap\gS_j=\emptyset$ for any $i\neq j$.
%So, we will drop the index $i$ from expected reward and transition if no confusion is possible: we denote them by $r(s_i,a_i)$ instead of $r_i(s_i,a_i)$ and by $p(s'_i\mid s_i,a_i)$ instead of $p_i(s'_i\mid s_i,a_i)$.
%In consequence, for any global state-action pair $(\vs,\va)$ with $\vs\in\gX$ and $\va\in\gA$, the expected reward from the MDP $M$ is given by $r(\vs,\va)=\sum_{i=1}^{n}r(s_i,a_i)$.
%Moreover, $M$ transitions to next state $\vs'\in\gX$ with probability $p(\vs'\mid \vs,\va)=\prod_{i=1}^n p(s'_i\mid s_i,a_i)$.

%The activated arm incurs a random reward discounted like $\gamma^{t-1}r_t$ where $\gamma\in(0,1)$ is the discount factor.

\subsection{Structural properties of global MDP}
\label{ssec:mdp_params}

We presented the classification of MDPs in Definition~\ref{ch:mdp:defn:mdp_class} of Chapter~\ref{ch:mdp}.
We also divided the MDP space as presented in \figurename~\ref{ch:mdp:fig:mdp_class}.
In addition, we say that a MDP is \emph{recurrent} if for all policy $\pi$, the matrix $\mP^\pi$ defines a Markov chain where all states are recurrent (equivalently, for all states $s$ and $s'$, a Markov chain starting in $s$ will visit $s'$ with probability $1$).
A recurrent or unichain MDP is called \emph{aperiodic} if all matrices $\mP^\pi$ are aperiodic.
While the definition of ergodic MDP is given in Definition~\ref{ch:mdp:defn:mdp_class}, we also say that a MDP is \emph{ergodic} if it is recurrent and aperiodic.
Following \cite[Definition 5.1]{wei2020model},
\begin{defn}
    \label{ch:restless:defn:mixing_time}
    The mixing time of an ergodic MDP is defined as
    \begin{align*}
        t_{mix} := \max_{\pi}\min\left\{ t\ge1 \mid \lVert (\mP^\pi)^t(x,\cdot) - \mu^\pi\rVert_1 \le \frac14, \forall x\right\}.
    \end{align*}
\end{defn}
By Definition~\ref{ch:restless:defn:mixing_time}, the mixing time is the maximum time required for any policy starting at any initial state to make the state distribution $\frac14$-close (in $\ell_1$ norm) to the stationary distribution, the state distribution in steady regime.

We recall that the diameter of a MDP (see Definition~\ref{ch:rl:defn:diameter}) is finite if and only if the MDP is communicating.
It is shown in \cite[Appendix A]{jaksch2010near} that for any MDP with state space $\gX$ and action space $\gA$, the diameter is lower bounded by $\log_{|\gA|}|\gX|-3$.
This implies that the diameter of a restless bandit with $n$ arms, each arm has the same state size $S$ and each time exactly $m$ arms are activated, is lower bounded by
\begin{equation*}
    \log_{|\gA(m)|}|\gX| -3 = \sum_{i=1}^{n}\log_{|\gA(m)|}|\gS_i| -3 =n\log_{{n \choose m}}S -3.
\end{equation*}
This means that the diameter of any restless bandit is at least linear in the number of arms.

%\subsection{Span}
%
%Following \cite[Chapter~8]{puterman2014markov}, in finite-state Markov reward process $(\vr, \mP)$, we define $\bar{\mP}=\displaystyle\lim_{N\to+\infty}\frac1N\sum_{t=0}^{N-1}\mP^{t}$ as the Cesaro limit of the sequence $\{\mP^t\}_{t\ge0}$.
%This limit exists for finite-state process (see Section~A.4 of Appendix~A of \cite{puterman2014markov}).
%If $\bar{\mP}$ is stochastic, then the gain is $\vg=\bar{\mP}\vr$.
%The bias is defined according to the structure of Markov chain:
%\begin{enumerate}
%    \item if the Markov chain is \emph{aperiodic}, then the bias is $\vh=\displaystyle\sum_{t=0}^\infty(\mP^t-\bar{\mP})\vr$
%    \item if the Markov chain is \emph{periodic}, then the bias is $\vh=\displaystyle\lim_{N\to\infty}\frac1N\sum_{k=1}^N\sum_{t=0}^{k-1}(\mP^t-\bar{\mP})\vr$
%\end{enumerate}
%
%Let $\vh\in\sR^S$. The span of $\vh$ is given by $sp(\vh)=\max_xh(x) - \min_xh(x)$.

In the following, we study how the structure of local arms translate in the structure of the global MDPs.
To do so, we will provide a few simple examples and counter-examples.
In those examples, the bandit always has two arms and exactly one arm is activated at each decision time.
To ease the exposition, Arm $1$ is drawn in green color and Arm $2$ in dark purple.
In each arm, the transitions of action activate is drawn in back color and those of action rest is drawn in dashed red color.
In the global MDP, the state transitions when activating Arm $1$ is drawn in green arrows and those when activating Arm $2$ is drawn in dark purple.
The transitions under optimal action are drawn in double-head arrows.
The expected reward and probability of transition are noted along the transition arrows.
However, if the transition is deterministic, \ie, the probability is $1$, we only note the expected reward along the transition arrows.

\begin{figure}[htbp]
    \centering
    \begin{tabular}{cc}
    \begin{tikzpicture}[on grid, state/.style={circle,draw}, >= stealth', auto, prob/.style = {inner sep=1pt,font=\scriptsize}]
        \node[state, black!45!green]  (A) {$1$};
        \node[state, black!45!green]  (B) [below = 2cm of A]   {$2$};
        \node[state, black!45!green]  (C) [below left = 1cm and 2cm of A]   {$3$};
        \path[->]
        (A) edge[bend left=75]     node{$0$}	(B)
        (A) edge[bend left, dashed, red]     node{$0$}	(B)
	    (B) edge[bend left=75]     node{$1$}	(A)
	    (C) edge[bend left]     node{$0$}	(A)
        (C) edge[bend right, dashed, red]     node[below]{$0$}	(B)
        (B) edge[bend left, dashed, red]     node{$0$}	(A);
    \end{tikzpicture}
    &
    \begin{tikzpicture}[on grid, state/.style={circle,draw}, >= stealth', auto, prob/.style = {inner sep=1pt,font=\scriptsize}]
        \node[state, black!30!purple]  (A) {$1$};
        \node[state, black!30!purple]  (B) [below = 2cm of A]   {$2$};
        \path[->]
        (A) edge[bend left=75]     node{$0$}	(B)
        (A) edge[bend left, dashed, red]     node{$0$}	(B)
	    (B) edge[bend left=75]     node{$1$}	(A)
	    (B) edge[bend left, dashed, red]     node{$0$}	(A);
    \end{tikzpicture}
    \\
        Arm $1$ & Arm $2$ 
    \end{tabular}
    \caption{
        Restless Markovian arms. Arm 1 has 3 states and Arm 2 has 2 states.
        The black arrows show state transition of action activate and the red dashed one for action rest.
        The numbers along the arrows show the expected reward when executing the actions.
        The corresponding global MDP is given in \figurename~\ref{fig:hard_global}
    }
    \label{fig:hard_local}
\end{figure}


\begin{figure}[ht]
    \centering
    \begin{tabular}{c}
    \begin{tikzpicture}[on grid, state/.style={circle,draw}, >= stealth', auto, prob/.style = {inner sep=1pt,font=\scriptsize}]
        \node[state]  (A) {$1,1$};
        \node[state]  (B) [above = 2cm of A]   {$2,2$};
        \node[state]  (C) [right = 4cm of B]   {$1,2$};
        \node[state]  (D) [below = 2cm of C]   {$2,1$};
        \node[state]  (E) [right = 2cm of B]   {$3,1$};
        \node[state]  (F) [below = 2cm of E]   {$3,2$};
        \path[->]
        (A) edge[bend left, black!45!green]     node{$0$}	(B)
        (A) edge[bend left=75, black!30!purple]     node{$0$}	(B)
	    (C) edge[bend left, black!45!green]     node{$0$}	(D)
        (D) edge[bend left=75, black!30!purple] node{$0$} (C)
        (E) edge[ black!30!purple] node[above]{$0$} (B)
        (F) edge[black!45!green] node[below]{$0$} (A);
        \draw[-{Stealth}{Stealth}]
	    (B) edge[bend left=75, black!30!purple]     node{$1$}	(A)
	    (B) edge[bend left, black!45!green]     node{$1$}	(A)
        (D) edge[bend left, black!45!green] node{$1$} (C)
	    (C) edge[bend left=75, black!30!purple]     node{$1$}	(D)
        (E) edge[black!45!green] node[above]{$0$} (C)
        (F) edge[ black!30!purple] node[below]{$1$} (D);
    \end{tikzpicture}
    \\
        Global MDP is multichain and not weakly communicating.
    \end{tabular}
    \caption{
        A restless bandit with two arms given in \figurename~\ref{fig:hard_local}.
        Each time, exactly one arm is activated.
        The global state is denoted by $(s_1,s_2)$ where $s_1$ is the state of Arm $1$ and $s_2$ of Arm $2$.
        The green arrows show the state transition when activating Arm $1$.
        The purple arrows show the state transiiton when activating Arm $2$.
        The numbers along the arrows show the expected reward when executing the actions.
        The double head arrows show the state transition of optimal actions.
        The global MDP is multichain and not weakly communicating because state $(3,1)$ and $(3,2)$ are transient states and there are two recurrent classes: $\gX^1:=\{(1,1), (2,2)\}$ and $\gX^2:=\{(1,2),(2,1)\}$.
        The optimal gain in class $\gX^1$ is $0.5$ and in class $\gX^2$ is $1$.
        This shows that the actions in states $(3,1)$ and $(3,2)$ are decisive.
    }
    \label{fig:hard_global}
\end{figure}

%The following theorem shows that a restless bandit whose arms are all unichain is not necessarily unichain. This is true under the additional condition that all arms are aperiodic.
\subsection{Negative Results}
The following theorem shows that a restless bandit whose arms are all ergodic is not necessarily ergodic.
\begin{thm}
    \begin{enumerate}[label=(\roman*)]
        \item \label{thm:not_ergodic} There exists a RMAB whose arms are all recurrent that is noncommunicating. % the term noncommunicating is taken from [Puterman, 1994, page 353]. 
        %\item \label{thm:aperiodic_RB_ergo} There exists a Restless bandit whose arms are all ergodic that is not recurrent. 
        \item \label{thm:ergodic_arms_multichain_RB} There exists a RMAB whose arms are all ergodic that is multichain.
    \end{enumerate}
\end{thm}

\begin{proof}
    \textbf{Proof of \ref{thm:not_ergodic}} -- It is given by the counter-example in which the RMAB has two arms as given by \figurename~\ref{fig:recur_non_communicate}.
    For each arm, both "activate" and "rest" induce the same transition.
    In such example, the RMAB is a noncommunicating multichain MDP because if we start at state $1a$ or $2b$, then, there are two recurrent classes: one composed of $\{1a, 2b\}$ and the other one is $\{1b, 2a\}$.
    \begin{figure}[ht]
        \centering
        \begin{tabular}{ccc}
        \begin{tikzpicture}[on grid, state/.style={circle,draw}, >= stealth', auto, prob/.style = {inner sep=1pt,font=\scriptsize}]
            \node[state]  (A) {$1$};
            \node[state]  (B) [below = 2cm of A]   {$2$};
            \path[->]
            (A) edge[bend left]     node{}	(B)
    	    (B) edge[bend left]     node{}	(A);
        \end{tikzpicture}
        &
        \begin{tikzpicture}[on grid, state/.style={circle,draw}, >= stealth', auto, prob/.style = {inner sep=1pt,font=\scriptsize}]
            \node[state]  (A) {$a$};
            \node[state]  (B) [below = 2cm of A]   {$b$};
            \path[->]
            (A) edge[bend left]     node{}	(B)
    	    (B) edge[bend left]     node{}	(A);
        \end{tikzpicture}
        &
        \begin{tikzpicture}[on grid, state/.style={circle,draw}, >= stealth', auto, prob/.style = {inner sep=1pt,font=\scriptsize}]
            \node[state]  (A) {$1a$};
            \node[state]  (B) [right = 1.5cm of A]   {$1b$};
            \node[state]  (C) [right = 1.5cm of B]   {$2a$};
            \node[state]  (D) [right = 1.5cm of C]   {$2b$};
            \path[->]
            (A) edge[bend left=50]     node{}	(D)
    	    (B) edge[bend left]     node{}	(C)
    	    (C) edge[bend left]     node{}	(B)
            (D) edge[bend left=50] node{} (A);
        \end{tikzpicture}
        \\
            Arm $1$ & Arm $2$ & Global MDP
        \end{tabular}
        \caption{A RMAB with two 2-state arms that are recurrent.}
        \label{fig:recur_non_communicate}
    \end{figure}
    \medskip \\
    \textbf{Proof of \ref{thm:ergodic_arms_multichain_RB}} -- It is given by another counter-example in \figurename~\ref{fig:local_ergodic_multichain_RB}.
    \begin{figure}
        \centering
        \begin{tabular}{c}
        \begin{tikzpicture}[on grid, state/.style={ellipse,draw}, >= stealth', auto, prob/.style = {inner sep=1pt,font=\scriptsize}]
            \node[state]  (A) {$8$};
            \node[state]  (B) [right = 1.6cm of A]    {$1$};
            \node[state]  (C) [right = 1.6cm of B]    {$2$};
            \node[state]  (D) [below = 1.6cm of C]    {$3$};
            \node[state]  (E) [below = 1.6cm of D]    {$4$};
            \node[state]  (F) [left = 1.6cm of E]    {$5$};
            \node[state]  (G) [left = 1.6cm of F]    {$6$};
            \node[state]  (H) [below = 1.6cm of A]    {$7$};
            \node[text width=5cm] (I) [left = 1.4cm of H] {Arm's structure};
            \path[->]
                (A) edge[red, dashed, bend right=30] node{} (B)
                edge[red, dashed, loop above] node{} (A)
                (B) edge[red, dashed, bend right=30] node{} (C)
                edge[red, dashed, bend right=30] node{} (D)
                (C) edge[red, dashed, bend right=30] node{} (D)
                edge[red, dashed, loop above] node{} (C)
                (D) edge[red, dashed, bend right=30] node{} (E)
                edge[red, dashed, bend right=30] node{} (F)
                (E) edge[red, dashed, bend right=30] node{} (F)
                edge[red, dashed, loop below] node{} (E)
                (F) edge[red, dashed, bend right=30] node{} (G)
                edge[red, dashed, bend right=30] node{} (H)
                (G) edge[red, dashed, bend right=30] node{} (H)
                edge[red, dashed, loop below] node{} (G)
                (H) edge[red, dashed, bend right=30] node{} (A)
                edge[red, dashed, bend right=30] node{} (B)

                (A) edge[bend left=30]     node{}	(B)
                edge[bend left=50]     node{}	(C)
                (B) edge[loop above] node{} (B)
                edge[bend left=30]     node{}	(C)
                (C) edge[bend left=30]     node{}	(D)
                edge[bend left=50]     node{}	(E)
                (D) edge[loop right] node{} (D)
                edge[bend left=30]     node{}	(E)
                (E) edge[bend left=30]     node{}	(F)
                edge[bend left=50]     node{}	(G)
                (F) edge[loop below] node{} (F)
                edge[bend left=30]     node{}	(G)
                (G) edge[bend left=30]     node{}	(H)
                edge[bend left=50]     node{}	(A)
                (H) edge[loop left] node{} (H)
                edge[bend left=30]     node{}	(A);
        \end{tikzpicture} \\
        \begin{tikzpicture}[on grid, state/.style={ellipse,draw}, >= stealth', auto, prob/.style = {inner sep=1pt,font=\scriptsize}]
            \node[state]  (A) {$\begin{tabular}{c}15\\24\\25\end{tabular}$};
            \node[state]  (B) [right = 2cm of A]    {$\begin{tabular}{c}26\\35\\36\end{tabular}$};
            \node[state]  (C) [right = 2cm of B]    {$\begin{tabular}{c}37\\46\\47\end{tabular}$};
            \node[state]  (D) [right = 2cm of C]    {$\begin{tabular}{c}48\\57\\58\end{tabular}$};
            \node[state]  (E) [below = 3cm of D]    {$\begin{tabular}{c}51\\68\\61\end{tabular}$};
            \node[state]  (F) [left = 2cm of E]    {$\begin{tabular}{c}62\\71\\72\end{tabular}$};
            \node[state]  (G) [left = 2cm of F]    {$\begin{tabular}{c}73\\82\\83\end{tabular}$};
            \node[state]  (H) [left = 2cm of G]    {$\begin{tabular}{c}84\\13\\14\end{tabular}$};
            \node[text width=2cm] (J) [left = 1.5cm of A] {Recurrent\\Class $\bar{\gS}^{(1)}$};
            \path[->]
            (A) edge     node{}	(B)
            (B) edge     node{}	(C)
            (C) edge     node{}	(D)
            (D) edge     node{}	(E)
    	    (E) edge     node{}	(F)
    	    (F) edge     node{}	(G)
            (G) edge     node{}	(H)
            (H) edge     node{}	(A);
        \end{tikzpicture} \\ \hfill\\
        \begin{tikzpicture}[on grid, state/.style={ellipse,draw}, >= stealth', auto, prob/.style = {inner sep=1pt,font=\scriptsize}]
            \node[state]  (A) {$\begin{tabular}{c}11\\28\\21\end{tabular}$};
            \node[state]  (B) [right = 2cm of A]    {$\begin{tabular}{c}22\\31\\32\end{tabular}$};
            \node[state]  (C) [right = 2cm of B]    {$\begin{tabular}{c}33\\42\\43\end{tabular}$};
            \node[state]  (D) [right = 2cm of C]    {$\begin{tabular}{c}44\\53\\54\end{tabular}$};
            \node[state]  (E) [below = 3cm of D]    {$\begin{tabular}{c}55\\64\\63\end{tabular}$};
            \node[state]  (F) [left = 2cm of E]    {$\begin{tabular}{c}66\\75\\74\end{tabular}$};
            \node[state]  (G) [left = 2cm of F]    {$\begin{tabular}{c}77\\86\\84\end{tabular}$};
            \node[state]  (H) [left = 2cm of G]    {$\begin{tabular}{c}88\\17\\15\end{tabular}$};
            \node[text width=2cm] (J) [left = 1.5cm of A] {Recurrent\\Class $\bar{\gS}^{(2)}$};
            \path[->]
            (A) edge     node{}	(B)
            (B) edge     node{}	(C)
            (C) edge     node{}	(D)
            (D) edge     node{}	(E)
    	    (E) edge     node{}	(F)
    	    (F) edge     node{}	(G)
            (G) edge     node{}	(H)
            (H) edge     node{}	(A);
        \end{tikzpicture}
        \end{tabular}
        \caption{We consider a RMAB with 2 arms, each is an 8-state ergodic MDP where the arm goes from states $1$ and $2$ to state $2$ or $3$, states $3$ and $4$ to state $4$ or $5$, etc, under passive action, and goes from states $1$ and $8$ to state $1$ or $2$, etc, under active action. Both arms have the same structure.
        The first row shows the state transition of the arm: the black arrows represent active transition and the red ones represent passive transition.
        The second and third rows show the state transition of two recurrent classes under any policies that activate Arm 2 for any impair $x\in[8]$ and Arm 1 for any pair $x\in[8]$ when RMAB is in states $(x,x),(x,x+4),(x+1,x),(x+1,x+3),(x+1,x+4),(x+1,x+7)$.
        The first recurrent class is $\bar{\gS}^{(1)} =\{(x,x+4),(x+1,x+3),(x+1,x+4) : x\in[8]\}$ and the second one is $\bar{\gS}^{(2)} =\{(x,x),(x+1,x),(x+1,x+7) : x\in[8]\}$. The rest of the states $\bar{\gS}\setminus(\bar{\gS}^{(1)}\cup\bar{\gS}^{(2)})$ are transient regardless of whether Arm $1$ or Arm $2$ is pulled.
    }
        \label{fig:local_ergodic_multichain_RB}
    \end{figure}
\end{proof}

\subsubsection{Diameter, mixing time, and span}

\begin{thm}
    \begin{enumerate}[label=(\roman*)]
        \item \label{thm:diam} There exists an ergodic RMAB, whose arms all have a bounded diameter, that has a diameter exponential in the number of arms. % the term noncommunicating is taken from [Puterman, 1994, page 353]. 
        %\item \label{thm:aperiodic_RB_ergo} There exists a Restless bandit whose arms are all ergodic that is not recurrent. 
        \item \label{thm:mixing} There exists an ergodic RMAB, whose arms are all ergodic, that has an unbounded mixing time.
        \item \label{thm:span} There exists a communicating RMAB, whose arms all have a bounded span, that has an unbounded span.
    \end{enumerate}
\end{thm}
\begin{proof}
    \textbf{Proof of \ref{thm:diam}} -- consider an example of $n$ arms, each arm is a Markov chain with binary state space and $0.5$ probability of changing state under both actions.
The probability that RMAB goes from state $(1,\dots,1)$ to state $(0,\dots,0)$ is $(1/2)^n$.
Hence, the diameter is exponential in the number of arms.
\medskip \\
\textbf{Proof of \ref{thm:mixing}} -- Let $\mP^m$ be the transition matrix of the arm given in \figurename~\ref{fig:local_ergodic_multichain_RB}.
Let $\mU$ be the transition matrix such that the state transition is uniform in any state.
Any arm having $\mU$ as the transition matrix is ergodic and the RMAB that is composed of such arms is also ergodic.
Now, consider the arms having the transition matrix $\mP=(1-\varepsilon)\mP^m +\varepsilon \mU$ for $\varepsilon\in(0,1)$.
Such arms are ergodic and the RMAB having such arms is also ergodic.
However, the mixing time of such RMAB tends to infinity when $\varepsilon$ tends to $0$ because when $\varepsilon=0$ the RMAB is multichain as shown in Theorem~\ref{thm:ergodic_arms_multichain_RB}.
%Hence, the hypothesis of ergodicity on RMAB problem is still not enough for mixing time to be defined.
\medskip \\
\textbf{Proof of \ref{thm:span}} -- First of all, consider the RMAB in Figure~\ref{fig:recur_non_communicate}.
The reward in local state $1$ and $a$ is $1$ and in local state $2$ and $b$ is $0$ independently of the action taken.
Thus, the upper bound on the local span of each arm is $1$ but the RMAB is multichain whose upper bound on the global span is infinite.
Using the same technique above, let $\mP^m$ be the transition matrix of one arm in Figure~\ref{fig:recur_non_communicate} and $\mU$ be the transition matrix such that the state transition is uniform in any state.
The arms having the transition matrix $\mP=(1-\varepsilon)\mP^m +\varepsilon \mU$ for $\varepsilon\in(0,1)$ are ergodic with finite upper bound on the local span. 
The RMABs having such arms are communicating.
However, the upper bound on the global span tends to infinity when $\varepsilon$ tends to $0$.
\end{proof}

\subsection{Minimax lower bound on the expected regret}

\begin{thm}[Hard to learn]
    For any learning algorithm, there exists a communicating global MDP, where arms, TOCONTINUE
\end{thm}
\begin{proof}
    We construct a restless bandit having two arms: Arm $1$ is given in \figurename~\ref{fig:hard_local2} and Arm $2$ is the Arm $2$ of \figurename~\ref{fig:hard_local}.
    We give the corresponding bandit problem in \figurename~\ref{fig:hard_local2}.
    All actions at state $(2,2)$ are optimal.
    An optimal policy is the following: $\pi^*(1,1)=1, \pi^*(1,2)=2, \pi^*(2,1)=1, \pi^*(2,2)=2$.
    %That is, it is optimal to activate the arm in state $2$.
    $\pi^*$ induces a unichain Markov chain with the single recurrent class $\gX^1:=\{(1,2),(2,1)\}$.
    The optimal gain is $g^*=1$ and the span of the optimal bias is $sp(\vh^*)=\frac1\varepsilon$.

    Suboptimal policies activate Arm $2$ when the global MDP is in state $(1,1)$ and have a recurrent class $\gX^2:=\{(1,1),(2,2)\}$.
    The gain of all states in $\gX^2$ under those suboptimal policies is $1/2$.
    The regret gap of suboptimal action in $(1,1)$ is then $g^*-1/2=1/2$.

    At time $1$, the MDP is in state $(1,1)$.
    According to the structure MDP, the expected number of Arm $1$ activations to get from $(1,1)$ to $(1,2)$ is $\tau_1:=1/\varepsilon$.
    Consider a learner $\tilde{\gL}$ that acts uniformly randomly at state $(1,1)$ and optimally in all other states. 
    %The expected number of activations on Arm $1$ to get $(1,1)$ transitions to state $(1,2)$ is $1/\delta$.
    Since $\tilde{\gL}$ acts uniformly randomly at $(1,1)$, the expected number of visits to state $(1,1)$ is $2\tau_1$ time steps.
    This means that $\tilde{\gL}$ spends, in expectation, $\landauOmega(4\tau_1)$ time steps in $\gX^2$ before leaving to $\gX^1$ and get the maximum gain.
    Since $\tilde{\gL}$ acts suboptimally in state $(1,1)$ at least $\tau_1$ time steps (\ie, $\tilde{\gL}$ activates Arm $2$ in $(1,1)$), the regret of $\tilde{\gL}$ is at least $\landauOmega(\tau_1)$ ($\tau_1$ time steps in $(1,1)$ means $2\tau_1$ time steps in $\gX^2$).

    Consider now a learner $\bar{\gL}$ that knows everything except the transition of state $1$ of both arms (expected reward of all states and transition of state $2$).
    So, $\bar{\gL}$ acts optimally in state $(1,2), (2,1)$ and $(2,2)$ but needs to learn how to get from $(1,1)$ to $(1,2)$.
    Starting in $(1,1)$, the expected number of visits to $(1,1)$ of $\bar{\gL}$ is at least $\tau_1$.
    Following \cite{osband2016lower}, when both $\bar{\gL}$ and $\tilde{\gL}$ acts in $(1,1)$ for $N$ times, the KL-divergence of the distribution of $\bar{\gL}$'s actions and the distribution of $\tilde{\gL}$'s actions, denoted by $d_{KL}$, is upper bounded by $\displaystyle\frac{N}{2}\log_2\Bigl(\frac1{1-\varepsilon}\Bigr)$ \cite[Lemma~2]{osband2016lower}.
    Moreover, by Pinsker's inequality (also followed from \cite{osband2016lower}), the expected number of times that $\bar{\gL}$ activates Arm $1$ in $(1,1)$ (\ie acts optimally) over $N$ times is upper bounded by
    \begin{align*}
        N(\sqrt{\frac12d_{KL}}+\frac12)
        &\le N\left(\sqrt{\frac{N}4\log_2\Bigl(\frac1{1-\varepsilon}\Bigr)}+\frac12\right) \\
        &\le N\left(\sqrt{\frac{N\varepsilon}{4\ln(2)(1-\varepsilon)}}+\frac12\right)
    \end{align*}
    %(divided by $2$ because there are $2$ actions).
    This means that, over $N$ times in $(1,1)$, $\bar{\gL}$ acts suboptimally at least
    \begin{align*}
        N\left(1-\sqrt{\frac{N\varepsilon}{4\ln(2)(1-\varepsilon)}}-\frac12\right)
        &= N\left(\frac12 -\sqrt{\frac{N\varepsilon}{4\ln(2)(1-\varepsilon)}}\right) \\
        &= \frac{N}2\left(1 -\sqrt{\frac{N\varepsilon}{\ln(2)(1-\varepsilon)}}\right)
        %&\ge \frac{N}2 \left(1-\sqrt{\frac{N\varepsilon}{\ln2}}\right)%= \frac{\tau_1}2 \left(1-\sqrt{\frac{1}{\ln2}}\right).
    \end{align*}
    The regret of $\bar{\gL}$ is then at least $\frac{N}2 \left(1-\sqrt{\frac{N\varepsilon}{\ln(2)(1-\varepsilon)}}\right)$. %(we say ``at least'' because $\tau_1$ is the expected number of Arm $1$ activations to leave $(1,1)$ to $(1,2)$).
    %Any learners without any initial knowledge about both states of both arms suffer a regret at least the regret of $\bar{\gL}$.
    %Taking $\tau_1=1/\varepsilon=2T$ to conclude the proof.

    \begin{figure}[htbp]
        \centering
        \begin{tabular}{cc}
        \begin{tikzpicture}[on grid, state/.style={circle,draw}, >= stealth', auto, prob/.style = {inner sep=1pt,font=\scriptsize}]
            \node[state, black!45!green]  (A) {$1$};
            \node[state, black!45!green]  (B) [below = 2cm of A]   {$2$};
            \path[->]
            (A) edge[bend left=75]     node{$0,p{=}1{-}\varepsilon$}	(B)
            (A) edge[loop above]     node[above]{$0,p{=}\varepsilon$}	(A)
            (A) edge[bend left, dashed, red]     node{$0$}	(B)
    	    (B) edge[bend left=75]     node{$1$}	(A)
            (B) edge[bend left, dashed, red]     node{$0$}	(A);
        \end{tikzpicture}
        &
        \begin{tikzpicture}[on grid, state/.style={circle,draw}, >= stealth', auto, prob/.style = {inner sep=1pt,font=\scriptsize}]
                \node[state]  (A) {$1,1$};
                \node[state]  (B) [right = 2.5cm of A]   {$1,2$};
                \node[state]  (C) [right = 2.5cm of B]   {$2,1$};
                \node[state]  (D) [left = 2.5cm of A]   {$2,2$};
                \path[->]
                (A) edge[bend right=80, black!30!purple]     node[above]{$0,p{=}1$}	(D)
                (B) edge[bend left, black!45!green]     node{$0,p{=}\varepsilon$}	(A)
                (B) edge[bend right, black!45!green]     node[below]{$0,p{=}1{-}\varepsilon$}	(C)
                (C) edge[bend right=80, black!30!purple]     node[above]{$0,p{=}1$}	(B);
                % transition of optimal actions
                \draw[-{Stealth}{Stealth}]
                (A) edge[bend right, black!45!green]     node[above]{$0,p{=}1{-}\varepsilon$}	(D)
                (A) edge[bend left, black!45!green]     node{$0,p{=}\varepsilon$}	(B)
                (B) edge[bend right=80, black!30!purple]     node[below]{$1,p{=}1$}	(C)
                (C) edge[bend right, black!45!green]     node[above]{$1,p{=}1$}	(B)
                (D) edge[bend right, black!45!green] node[below]{$1,p{=}1$} (A)
                (D) edge[bend right=80, black!30!purple] node[below]{$1,p{=}1$} (A);
        \end{tikzpicture}
        \\
            Arm $1$ is ergodic & Global MDP is communicating
        \end{tabular}
        \caption{
            2-state restless Markovian arms.
            The black arrows show state transition of action activate and the red dashed one for action rest.
            The numbers along the arrows show the expected reward when executing the actions.
            The double head arrows show the state transition of optimal actions.
        }
        \label{fig:hard_local2}
    \end{figure}
\end{proof}





\subsection{Positive Results}

\begin{thm}
    \label{thm:aperiodic_RB_comm} A RMAB whose arms are all ergodic is communicating. 
\end{thm}
\begin{proof}
    We prove the theorem by its contraposition. Assume that a given RMAB is noncommunicating.
    In this RMAB, there are global states that are not reachable from each other.
    This implies to two possibilities: 1. for some arms, some local states are not recurrent, 2. all arms are periodic.
    Each possibility implies that there exists at least one arm that is not ergodic.
\end{proof}

\begin{thm}
    \label{thm:unichain} If all arms are unichain MDP with at least one state reachable in one step from any other states under both passive and active actions, then the corresponding RMAB is also a unichain MDP with at least one state reachable in one step from any other global states under any policies.
\end{thm}
\begin{proof}
    For each arm $i\in[n]$, let $z_i$ be the state that is reachable in one step from any other states under both passive and active actions.
    We have that $p(z_i \mid x_i, a)>0$ for all $i\in[n], x_i\in\gS_i,a\in\{0,1\}$.
    %For simplification, we write $P^a(x_i,z_i) =P^a_i(x_i, z_i)$.
    For any action $\va\in\bar{\gA}$, any state $\vx,\vy\in\bar{\gS},$ the RMAB's state transition is given by ${\vp(\vy \mid \vx, \va) =\prod_{i=1}^n p(y_i \mid x_i, a_i)}$.
    Then, for any action $\va\in\bar{\gA}$, any state $\vx\in\bar{\gS}$, we have
    \begin{align*}
        \vp(\vz \mid \vx, \va) =\prod_{i=1}^n p(z_i \mid x_i,a_i) > 0,
    \end{align*}
    because $n$ is finite and for any $i$, $p(z_i \mid x_i, a_i)>0$.
    Hence, $\vz$ is reachable from any other global states in one step under any policies.
    Consequently, no policies induce the global Markov chain that has multiple closed irreducible recurrent classes.
    %there cannot be multiple recurrent classes that are closed under any policies.
    That concludes the proof.
\end{proof}

\begin{thm}
    \label{thm:ergodicity_coeff} For each arm $i\in[n]$, let $\beta_i$ be defined by
    \begin{align*}
        \beta_i = 1-\min_{\substack{x_i,y_i\in\gS_i \\a,a'\in\{0,1\}}} \sum_{z_i\in\gS_i} \min\{p(z_i \mid x_i, a), p(z_i \mid y_i, a')\}.
    \end{align*}
    Similarly, for a RMAB, let $\vbeta$ be defined by
    \begin{align*}
        \vbeta = 1-\min_{\substack{\vx,\vy\in\bar{\gS} \\\va,\va'\in\bar{\gA}}} \sum_{\vz\in\bar{\gS}} \min\{\vp(\vz \mid \vx, \va), \vp(\vz \mid \vy, \va')\}.
    \end{align*}
    If $\beta_i<1$ for any arm $i$, then $\vbeta<1$.
    Moreover, if there exists $\varepsilon>0$ such that for any arm $i\in[n]$,
    \begin{align*}
        \min_{\substack{x_i,y_i\in\gS_i \\a,a'\in\{0,1\}}} \sum_{z_i\in\gS_i} \min\{p(z_i \mid x_i, a), p(z_i \mid y_i, a')\} \ge \varepsilon,
    \end{align*}
    then, $\vbeta\le 1-\varepsilon^n$.
\end{thm}
\begin{proof}
    For any pair $(\vx,\vy)\in\bar{\gS}^2$ and any pair $(\va,\va')\in\bar{\gA}^2$,
    \begin{align*}
        \sum_{\vz\in\bar{\gS}} \min\{\vp(\vz \mid \vx, \va), \vp(\vz \mid \vy, \va')\}
        &= \sum_{\vz\in\bar{\gS}} \min\left\{\prod_{i=1}^np(z_i \mid x_i, a), \prod_{i=1}^np(z_i \mid y_i, a')\right\} \\
        &\ge \sum_{\vz\in\bar{\gS}}\prod_{i=1}^n \min\{p(z_i \mid x_i, a), p(z_i \mid y_i, a')\} \\
        &= \prod_{i=1}^n \left(\sum_{z_i\in\gS_i} \min\{p(z_i \mid x_i, a), p(z_i \mid y_i, a')\}\right)
    \end{align*}
    %\KK{Add more detail about the second equality}
    Since $\beta_i<1$, we have that $\sum_{z_i\in\gS_i} \min\{P^{a_i}(x_i,z_i), P^{a_i'}(y_i,z_i)\}>0$.
    In consequences, for any pair $(\vx,\vy)\in\bar{\gS}^2$ and any pair $(\va,\va')\in\bar{\gA}^2,$ we have ${\sum_{\vz\in\bar{\gS}} \min\{\vp(\vz \mid \vx, \va), \vp(\vz \mid \vy, \va')\}{>}0}$.
    We conclude the proof using the definition of $\vbeta$.
\end{proof}


\section{Learning RMAB}

We consider learning a RMAB problem whose arms' rewards $\{r_i\}_{i\in[n]}$ and transition $\{P_i\}_{i\in[n]}$ are unknown and need to be learnt via observations.

\subsection{Regret definition and required assumptions}

Classically, the regret of the learner after $T$ time steps is given by
\begin{align}
    \Reg(M,T) := Tg^* - \sum_{t=1}^{T} \vr(\bX_{t},\bA_{t}). \label{eq:regret}
\end{align}
%This definition implicitly requires that the unknown MDP $M$ is weakly communicating and the reward is bounded.
This definition implicitly requires that the reward is bounded and the optimal gain $g^*$ is state-independent.

For Bayesian approach, the prior distribution $\phi$ of the unknown MDP is given.
We define the Bayesian regret by 
\begin{align}
    \BayReg(\phi, T) := \ex{\Reg(M, T)} \label{eq:bayreg}
\end{align}
where the expectation over all the randomness ($\phi$, agent's trajectory...).

In the following, We present a methodology to bound the regret.
Then, we go through the assumptions required for such methodology.

\subsubsection{UCRL2 and TSDE framework}

The algorithm updates its policy in episodic manner using doubling trick.
Let $K_T$ be the total number of episodes (or policy updates) up to time $T$.
Let $t^k$ be the time step when episode $k$ with the convention that $t^1:=1$.
Let $M^k$ be the imagined MDP of the algorithm for episode $k$ and $\pi^k$ be the policy used in episode $k$.
We require $\pi^k$ to induce state-independent gain in order to have the Bellman evaluation equations
\begin{align*}
    g^k +h^{\pi^k}(\vx) = \vr^k(\vx, \pi^k(\vx)) +\sum_{\vy}\vp^k(\vy \mid \vx, \pi^k(\vx))h^{\pi^k}(\vy).
\end{align*}
Then,
\begin{align*}
    \Reg(M, T)&=\sum_{k=1}^{K_T}\sum_{t=t^k}^{t^{k+1}-1} \underbrace{(g^*{-}g^k)}_{\Delta_1} {+}\underbrace{(\vr^k{-}\vr)(\mX_t,\mA_t)}_{\Delta_2} {+}\underbrace{\sum_{\vy}(\vp^k-\vp)(\vy \mid \mX_t, \mA_t)h^{\pi^k}(\vy)}_{\Delta_3}\\
              &\quad  \underbrace{h^{\pi^k}(\mX_{t+1}){-}h^{\pi^k}(\mX_{t})}_{\Delta_4} {+}\underbrace{\sum_{\vy}\vp(\vy \mid \mX_t, \mA_t)h^{\pi^k}(\vy)-h^{\pi^k}(\mX_{t+1})}_{\Delta_5}
\end{align*}

\begin{itemize}
    \item The second term $\Delta_2$ is bounded by sub-Gaussian inequality (or simply zero if $\{r_i\}_{i\in[n]}$ is known);
    \item The first term $\Delta_1$ and the last term $\Delta_5$ are bounded based on the approach used: for optimism, $\Delta_1$ is non-positive due to optimism and the sum of $\Delta_5$ is bounded by Azuma-Hoeffding inequality for martingale difference sequence. For Bayesian, $\Delta_1$ is bounded by $K_T$ and $\Delta_5$ has zero expectation.
\end{itemize}
The terms $\Delta_3$ and $\Delta_4$ require additional assumption on $h^{\pi^k}$
\begin{align*}
    sp(\vh^{\pi^k})\le H^k \text{ where } H^k<+\infty.
\end{align*}
%Let $\Pi^k$ be the set of \emph{gain optimal} policies in MDP $M^k$.
%The assumption needed is that 
%\begin{align*}
%    \Pi^k\neq \emptyset \text{ and }\sup_{\pi^k\in\Pi^k}sp(\vh^{\pi^k})\le H^k \text{ where } H^k<+\infty.
%\end{align*}
Under this assumption
\begin{itemize}
    \item the sum of $\Delta_4$ is the \emph{telescopic sum} which is bounded by $K_TH$ where $H:=\max_{k\le K_T}H^k$. Depending on the doubling trick used, $K_T$ is bounded \emph{a priori} by $O\big(\sqrt{SA\ln(T)}\big)$;
    \item $\Delta_3$ is bounded by $H\vert\vp^k-\vp\vert_{1}$ and $\vert\vp^k-\vp\vert_{1}$ is bounded by Weissman's inequality.
\end{itemize}

\paragraph{UCRL2 assumption}

The assumption used in UCRL2 is that the unknown MDP $M$ is \emph{communicating} (otherwise, the diameter is infinite) and has bounded non-negative reward.
This assumption implies
\begin{itemize}
    \item $g^*$ is state-independent and the regret is well defined;
    \item for each $k\ge1$, $M^k$ is communicating and $g^k$ is state-independent;
    \item for each $k\ge1$, $sp(\vh^{\pi^k})$ is bounded by the diameter of $M$.
\end{itemize}
The second item is true because the set of plausible MDPs $\gM^k$ can be considered as a MDP with finitely many actions.
If $M$ belongs to $\gM^k$, then $\gM^k$ is also \emph{communicating}.
$g^k$ can be considered as the optimal gain of this finitely many actions MDP $\gM^k$.
So, $g^k$ is state-independent.
The third item is true because $M$ belongs to $\gM^k$ together with $\gM^k$ is a MDP with finitely many actions imply that the diameter of $\gM^k$ is upper bounded by the diameter of $M$.
Under the assumption, \cite[Theorem~4]{bartlett2012regal} implies that the span of the optimal bias (the bias that verifies Bellman optimality equation) is bounded by the diameter of the MDP times the optimal gain.

%\KK{I cannot find any assumption about $sp(\vh^{\pi^k})\le H^k$ in UCRL2 paper}

\paragraph{TSDE assumption}

Let $\mathrm{supp}(\phi)$ denote the support of the distribution $\phi$.
The assumption used for TSDE is that $\mathrm{supp}(\phi)$ is a set of \emph{weakly communicating} MDPs and there exists $H<+\infty$ such that in any MDP drawn from the support, if $\vh\in\sR^S$ verifies Bellman optimality equation, then $sp(\vh)\le H$.
This assumption implies that
\begin{itemize}
    \item $g^*$ is state-independent and the regret is well defined;
    \item for each $k\ge1$, $M^k$ is weakly communicating and $g^k$ is state-independent;
    \item for each $k\ge1$, $sp(\vh^{\pi^k})$ is bounded by $H$.
\end{itemize}

\KK{What follows is just a draft. The notations are not rigorous at all.}

\subsubsection{Our assumptions}

For each arm $i\in[n]$, the unknown parameter $P_i$ belongs to a compact set $\Theta_i$.
Using Whittle index policy as a baseline, we need the two following assumptions:
\begin{asmp}
    \label{asmp:indexable}
    For any $i\in[n], P\in\Theta_i$, the arm $\langle\gS_i,\{0,1\},P,r_i\rangle$ is indexable.
\end{asmp}
\begin{asmp}
    \label{asmp:unichain}
    For any $i\in[n], P\in\Theta_i$, the arm $\langle\gS_i,\{0,1\},P,r_i\rangle$ has at least one state that is reachable in one step from any other states under any actions:
    \begin{align*}
        \beta_i = 1-\min_{\substack{x_i,y_i\in\gS_i \\a,a'\in\{0,1\}}} \sum_{z_i\in\gS_i} \min\{P^{a}(x_i,z_i), P^{a'}(y_i,z_i)\}<1.
    \end{align*}
\end{asmp}

Assumption~\ref{asmp:indexable} allows Whittle index policy to be meaningful.
By Theorem~\ref{thm:unichain}, Assumption~\ref{asmp:unichain} implies that the unknown RMAB is unichain. Consequently, the average gain of Whittle index policy is constant over all states of RMAB.
%Let $(\mP^W, \vr^W)$ be the controlled parameters of the corresponding RMAB under Whittle index policy $\vpi^W$ and $(g^W, h^W)$ be the average gain and bias function under $\vpi^W$ and satisfying
Let $(g^W, h^W)$ be the average gain and bias function under $\vpi^W$ and satisfying
\begin{align}
    g^W + h^W(\vx) = \vr(\vx,\va) +\sum_{\vz\in\bar{\gS}} \mP^{\va}(\vx, \vz)h^W(\vz),\hspace{1cm} \forall \vx\in\bar{\gS}, \va=\vpi^W(\vx). \label{eq:bellman}
\end{align}

\subsection{Learning Algorithm}
\label{ssec:learn_algo}

Our algorithm starts by choosing a prior $\phi_i$ over the parameter $P_i$ of arm $i$ for each $i\in[n]$.
The support of $\phi_i$ is the compact set $\Theta_i$. The algorithm updates its policy episodically: let $\gO_{k-1}$ be the observations made prior to the beginning of episode $k\ge1$ with the convention $\gO_{0}=\emptyset$.
Let $t_k$ be the time instant where episode $k$ begins and $t_1=1$.
The algorithm samples the parameters $P_{k,i}$ from the posterior distribution $\phi_i(\cdot \mid \gO_{k-1})$, compute Whittle indices of each arm $\langle\gS_i,\{0,1\},P_{k,i},r_i\rangle$ for $i\in[n]$ and applies the Whittle index policy $\vpi_k^W$ during episode $k$.
For any $i\in[n]$ and any state-action pair $(x_i,a_i)\in\gS_i\times\{0,1\}$, let $N_{k-1}(x_i,a_i):=\sum_{t'=1}^{t_k-1}\sI{(X_{t',i},A_{t',i})=(x_i,a_i)}$ be the number of visits to the pair $(x_i,a_i)$ up to the start of episode $k$ and $\nu_{k,t}(x_i,a_i):=\sum_{t'=t_k}^{t}\sI{(X_{t',i},A_{t',i})=(x_i,a_i)}$ be the number of times that arm $i$ visits the pair $(x_i,a_i)$ from time $t_k$ to $t$. Let $N_{k-1}^+(x_i,a_i):=\max\{1,N_{k-1}(x_i,a_i)\}$. Also, let $T_k:=t_{k+1}-t_{k}$ be the length of episode $k$. At time instant $t>t_k$, the algorithm starts episode $k+1$ if
\begin{align}
    t-t_k > T_{k-1} \text{ or } \sum_{x_i,a_i} \frac{\nu_{k,t}(x_i,a_i)}{N_{k-1}^+(x_i,a_i)}>1 \text{ for some } i\in[n]. \label{eq:epi_cond}
\end{align}
The first condition of \eqref{eq:epi_cond} ensures that the length of each episode is not too long and grows linearly with the number of episodes.
The second condition is adapted from \emph{extended doubling trick} which is proposed in \cite{tossou2019near}.
This condition ensures that if either a new state-action pair $(x_i,a_i)$ is visited or the number of visits to at least one of the visited state-action pairs is doubled or the number of visits to visited state-action pairs is double in average for some arm $i$, then, start a new episode.
We show in Lemma~\ref{} that if $T$ is the total number of time steps, then, the total number of episodes is bounded as a logarithmic of $T$.

Note that, by Assumptions~\ref{asmp:indexable} and \ref{asmp:unichain}, the sampled RMAB\footnote{Precisely, we sample the parameter of each arm} is unichain and indexable.
Hence, the Whittle index policy is meaningful and induces the average gain that is constant over all states in $\bar{\gS}$.

\subsection{Regret Bound}
\label{ssec:regret_bound}

Let $K_T$ be the total number of episodes over $T$ time steps.

\subsubsection{High Probability Event}
\label{sssec:high_event}

For any $i\in[n]$, any $(x_i,a_i,y_i)\in\gS_i\times\{0,1\}\times\gS_i$, let $${\Phat_{k-1}^{a_i}(x_i,y_i):=\frac1{N^+_{k-1}(x_i,a_i)}\sum_{t'=1}^{t_k-1}\sI{(X_{t',i},A_{t',i},X_{t'+1,i})=(x_i,a_i,y_i)}}$$ be the empirical estimator of $P^{a_i}(x_i,y_i)$.% and $\sigHat_{k-1}:=\Phat_{k-1}(1-\Phat_{k-1})$ be its population variance.
%The rigorous notation is $\Phat_{k-1}=\Phat_{N^+_{k-1}}$ and $\sigHat_{k-1}=\sigHat_{N^+_{k-1}}$.
If $T$ is the total number of time steps, consider the following event
\begin{align}
    \gE_1(T):=\left\{\forall i,x_i,a,y_i,k\ge0: |\Phat_k^a(x_i,y_i) {-}P^a(x_i,y_i)|\le \sqrt{\frac{2\Phat_k(1-\Phat_k)ln(20nS^2N_k^2T)}{N_k(x_i,a_i)}} +\frac{3ln(20nS^2N_k^2T)}{N_k(x_i,a_i)}\right\}. \label{eq:event_1}
\end{align}

\begin{lem}
    \label{lem:high_event1}
    For total number of time steps $T$, we have $\Proba{\gE_1(T)}\ge 1-\frac1{2T}$.
\end{lem}
\begin{proof}
The rigorous notation is $\Phat_{k-1}=\Phat_{N^+_{k-1}}$.
We slightly change notation $N^+_{k-1}$ to $\ell$ so that $\Phat_{N^+_{k-1}}=\Phat_\ell$.
From \cite[Theorem~1]{audibert2009exploration}, for any $\ell\ge1$ and $\varepsilon>0$,
\begin{align}
    \Proba{|\Phat_\ell-P|\le\sqrt{\frac{2\Phat_\ell(1-\Phat_\ell)\varepsilon}{\ell}} +\frac{3\varepsilon}{\ell}} \ge 1-3e^{-\varepsilon}. \label{eq:bern_concentration}
\end{align}
Now, consider the following event
\begin{align*}
    \gE'_1(T):=\left\{\exists i,x_i,a,y_i,\ell\ge1: |\Phat_\ell^a(x_i,y_i) -P^a(x_i,y_i)| > \sqrt{\frac{2\Phat_\ell(1-\Phat_\ell)ln(20nS^2\ell^2T)}\ell} +\frac{3ln(20nS^2\ell^2T)}{\ell}\right\}.
\end{align*}
We have that the complement of $\gE_1(T)$ is smaller then or equal to event $\gE'_1(T)$, $\bar{\gE}_1(T)\subseteq\gE'_1(T)$. To show that $\gE_1(T)$ occurs with high probability, we work on its complement: by union bound, we have
\begin{align*}
    \Proba{\bar{\gE}_1(T)}
    &\le \Proba{\gE'_1(T)} \\
    &\le \sum_{i=1}^{n} \sum_{x_i,a,y_i} \sum_{l=1}^{\infty} \Proba{|\Phat_\ell^a(x_i,y_i) -P^a(x_i,y_i)| > \sqrt{\frac{2\Phat_\ell(1-\Phat_\ell)ln(20nS^2\ell^2T)}\ell} +\frac{3ln(20nS^2\ell^2T)}{\ell}} \\
    &\le \sum_{i=1}^{n} \sum_{x_i,a,y_i} \sum_{l=1}^{\infty} \Proba{|\Phat_\ell^a(x_i,y_i) -P^a(x_i,y_i)| > \sqrt{\frac{2\Phat_\ell(1-\Phat_\ell)ln(2\pi^2nS^2\ell^2T)}\ell} +\frac{3ln(2\pi^2nS^2\ell^2T)}{\ell}} \\
    &\le \sum_{i=1}^{n} \sum_{x_i,a,y_i} \sum_{l=1}^{\infty} 3e^{-ln(2\pi^2nS^2\ell^2T)}
    =2nS^2\sum_{l=1}^{\infty}\frac{3}{2\pi^2nS^2\ell^2T} = \frac1{2T}
\end{align*}
\end{proof}

At the start of episode $k\ge1$, the estimator $\Phat_{k-1}$ is made prior and the learner samples $P_k$ from the posterior distribution.
Now, consider the following event
\begin{align}
    \gE_2(T):=\left\{\forall i,x_i,a,y_i,k\ge0: |\Phat_k^a(x_i,y_i) {-}P_{k+1}^a(x_i,y_i)|\le \sqrt{\frac{2\Phat_k(1-\Phat_k)ln(20nS^2N_k^2T)}{N_k(x_i,a_i)}} +\frac{3ln(20nS^2N_k^2T)}{N_k(x_i,a_i)}\right\}. \label{eq:event_2}
\end{align}
\begin{lem}
    \label{lem:high_event2}
    $\Proba{\gE_2(T)}\ge 1-\frac1{2T}$.
\end{lem}
\begin{proof}
For every episode $k\ge1$, $P_k$ and $P$ are identically distributed.
With a slightly abuse of notation, we have the following inequality that is similar to \eqref{eq:bern_concentration}
\begin{align*}
    \Proba{|\Phat_\ell-P_{\ell+1}|\le\sqrt{\frac{2\Phat_\ell(1-\Phat_\ell)\varepsilon}{\ell}} +\frac{3\varepsilon}{\ell}} \ge 1-3e^{-\varepsilon}.
\end{align*}
We follow the same process in the proof of Lemma~\ref{lem:high_event1}.
\end{proof}

Finally, we consider the following event
\begin{align}
    \gE(T){:=}\left\{\forall i,x_i,a,y_i,k\ge0: |P_{k+1}^a(x_i,y_i) {-}P^a(x_i,y_i)|\le \sqrt{\frac{2\Phat_{k}(1-\Phat_k)ln(20nS^2N_k^2T)}{N_k(x_i,a_i)}} +\frac{3ln(20nS^2N_k^2T)}{N_k(x_i,a_i)}\right\}. \label{eq:event}
\end{align}

\begin{lem}
    \label{lem:high_event}
    $\Proba{\gE(T)}\ge 1-\frac1{T}$.
\end{lem}
\begin{proof}
The complement of $\gE$ is smaller than the union of $\bar{\gE}_1$ and $\bar{\gE}_2$.
Then, by union bound, $\Proba{\bar{\gE}}\le \Proba{\bar{\gE}_1} +\Proba{\bar{\gE}_2}$.
We use Lemmas~\ref{lem:high_event1} and \ref{lem:high_event2} to conclude the proof.
\end{proof}

Note that this high probability does not depends on the episode $k$.

\begin{lem}
    \label{lem:global_conc}
    For any episode $k\ge1$, $\vx\in\bar{\gS}$, $\va\in\bar{\gA}$, we have
    \begin{align}
        \sum_{\vy\in\bar{\gS}}|\mP_k^{\va}(\vx,\vy) -\mP^{\va}(\vx,\vy)| 
        \le \sum_{i=1}^{n} \sum_{y_i\in\gS_i} |P_k^{a_i}(x_i,y_i) -P^{a_i}(x_i,y_i)|.
    \end{align}
\end{lem}
\begin{proof}
    To proof...
\end{proof}

\begin{lem}
    \label{lem:nb_episodes}
    The number of episodes $K_T$ is bounded as follows:
    \begin{align}
        K_T\le2\sqrt{nST\ln(T)}.
    \end{align}
\end{lem}
\begin{proof}
    To proof
\end{proof}


\section{Open questions}

\begin{itemize}
    \item Does the above properties have implications on regret? 
    \item What about Lazy MDPs? \emph{i.e.} chains such that $P(i | i,a)>\varepsilon$ for all states and actions.
    \item If RMAB problem is in stationary regime, what is the regime of each arm?
\end{itemize}




\begin{thm}[Hard to learn restless bandit (incorrect)]
    For any learning algorithm, and any total time steps $T$, there exists a communicating global MDP, whose arms are all ergodic, that make the algorithm suffers a regret linear in $T$.
\end{thm}

\begin{proof}
    We construct a restless bandit having two arms as given in \figurename~\ref{fig:hard_local5} and global problem in \figurename~\ref{fig:hard_global5}.
    All actions at state $(2,2)$ are optimal.
    An optimal policy is the following: $\pi^*(1,1)=1, \pi^*(1,2)=2, \pi^*(2,1)=1, \pi^*(2,2)=2$.
    %That is, it is optimal to activate the arm in state $2$.
    $\pi^*$ induces a unichain Markov chain with the single recurrent class $\gX^1:=\{(1,2),(2,1)\}$.
    The optimal gain is $g^*=1$ and the span of the optimal bias is $sp(\vh^*)=\frac1\varepsilon$.
    Suboptimal policies activate Arm $1$ when the global MDP is in state $(1,1)$ and have a recurrent class $\gX^2:=\{(1,1),(2,2)\}$.
    The gain of all states in $\gX^2$ under those suboptimal policies is $1-\varepsilon/2$.
    The regret gap of suboptimal action in $(1,1)$ is then $g^*-1+\varepsilon/2=\varepsilon/2$.
    The tricky state is $(1,1)$ in which activate Arm $2$ gives a reward $1-\varepsilon$ but induces suboptimal recurrent class $\gX^2$ while activate Arm $1$ give no rewards but induces optimal recurrent class $\gX^1$.

    At time $1$, the MDP is in state $(1,1)$.
    According to the structure MDP, the expected number of Arm $1$ activations to get from $(1,1)$ to $(1,2)$ is $\tau_1:=1/\delta$.
    Consider a learner $\tilde{\gL}$ that acts uniformly randomly at state $(1,1)$ and optimally in all other states. 
    %The expected number of activations on Arm $1$ to get $(1,1)$ transitions to state $(1,2)$ is $1/\delta$.
    Since $\tilde{\gL}$ acts uniformly randomly at $(1,1)$, the expected number of visits to state $(1,1)$ is $2\tau_1$ time steps.
    This means that $\tilde{\gL}$ spends, in expectation, $\landauOmega(4\tau_1)$ time steps in $\gX^2$ before leaving to $\gX^1$ and get the maximum gain.
    Since $\tilde{\gL}$ acts suboptimally in state $(1,1)$ at least $\tau_1$ time steps (\ie, $\tilde{\gL}$ activates Arm $2$ in $(1,1)$), the regret of $\tilde{\gL}$ is at least $\landauOmega(\tau_1\varepsilon)$ ($\tau_1$ time steps in $(1,1)$ means $2\tau_1$ time steps in $\gX^2$).
    %Now, consider a learner $\bar{\gL}$ that knows everything except the transition of state $1$ of both arms (expected reward of all states and transition of state $2$).
    %So, $\bar{\gL}$ acts optimally in state $(1,2), (2,1)$ and $(2,2)$ but needs to learn how to get from $(1,1)$ to $(1,2)$.
    Now, consider a learner $\bar{\gL}$ that acts optimally in state $(1,2), (2,1)$ and $(2,2)$ but needs to learn how to get from $(1,1)$ to $(1,2)$.
    Starting in $(1,1)$, the expected number of visits to $(1,1)$ of $\bar{\gL}$ is at least $\tau_1$.
    Following \cite{osband2016lower}, when both $\bar{\gL}$ and $\tilde{\gL}$ acts in $(1,1)$ for $N$ times, the KL-divergence of the distribution of $\bar{\gL}$'s actions and the distribution of $\tilde{\gL}$'s actions, denoted by $d_{KL}$, is upper bounded by $\displaystyle\frac{N}{2}\left(\log_2\Bigl(\frac{1}{1-\delta}\Bigr)\right)$ \cite[Lemma~2]{osband2016lower}.
    Moreover, by Pinsker's inequality (also followed from \cite{osband2016lower}), the expected number of times that $\bar{\gL}$ activates Arm $1$ in $(1,1)$ (\ie acts optimally) over $N$ times is upper bounded by
    \begin{align*}
        N(\sqrt{\frac12d_{KL}}+\frac12)
        &\le N\left(\sqrt{\frac{N}{4}\left(\log_2\Bigl(\frac{1}{1-\delta}\Bigr)\right)}+\frac12\right) \\
        &\le N\left(\sqrt{\frac{N\delta}{4\ln(2)(1-\delta)}}+\frac12\right)
    \end{align*}
    This means that, over $N$ times in $(1,1)$, $\bar{\gL}$ acts suboptimally at least
    \begin{align*}
        N -N\left(\sqrt{\frac{N\delta}{4\ln(2)(1-\delta)}}+\frac12\right)
        &= N\left(\frac12-\sqrt{\frac{N\delta}{4\ln(2)(1-\delta)}}\right) \\
        &= \frac{N}2 \left(1-\sqrt{\frac{N\delta}{\ln(2)(1-\delta)}}\right)%\le \frac{N}2 \left(1-\sqrt{\frac{\varepsilon}{\ln(2)\delta}}\right)%= \frac{\tau_1}2 \left(1-\sqrt{\frac{1}{\ln2}}\right).
    \end{align*}
    %where the last inequality is true because $N\ge \tau_1$ and $\tau_1=1/\varepsilon$.
    The regret of $\bar{\gL}$ is then at least $\frac{N\varepsilon}2 \left(1-\sqrt{\frac{N\delta}{\ln(2)(1-\delta)}}\right)$. %(we say ``at least'' because $\tau_1$ is the expected number of Arm $1$ activations to leave $(1,1)$ to $(1,2)$).
    Any learners without any initial knowledge about both states of both arms suffer a regret at least the regret of $\bar{\gL}$.
    %Taking $\varepsilon=\frac{\delta}{T}$ and $\delta=0.25$, to conclude the proof.

    \begin{figure}[htbp]
        \centering
        \begin{tabular}{cc}
        \begin{tikzpicture}[on grid, state/.style={circle,draw}, >= stealth', auto, prob/.style = {inner sep=1pt,font=\scriptsize}]
            \node[state, black!45!green]  (A) {$1$};
            \node[state, black!45!green]  (B) [below = 2cm of A]   {$2$};
            \path[->]
            (A) edge[bend left=90]     node[rotate=90,anchor=north]{$0,1{-}\delta$}	(B)
            (A) edge[loop above]     node[above]{$0,\delta$}	(A)
            (A) edge[bend left, dashed, red]     node[rotate=90,anchor=north]{$0,1$}	(B)
            (B) edge[bend left=90]     node[rotate=90,anchor=north]{$1,1$}	(A)
            (B) edge[bend left, dashed, red]     node[rotate=90,anchor=north]{$0,1$}	(A);
        \end{tikzpicture}
        &
        \begin{tikzpicture}[on grid, state/.style={circle,draw}, >= stealth', auto, prob/.style = {inner sep=1pt,font=\scriptsize}]
            \node[state, black!45!green]  (A) {$1$};
            \node[state, black!45!green]  (B) [below = 2cm of A]   {$2$};
            \path[->]
            %(A) edge[bend left=75]     node{$0,p{=}1{-}\delta{-}\varepsilon$}	(B)
            (A) edge[bend left=75]     node{$1{-}\varepsilon$}	(B)
            %(A) edge[loop above]     node[above]{$0,p{=}\delta{+}\varepsilon$}	(A)
            (A) edge[bend left, dashed, red]     node{$0$}	(B)
    	    (B) edge[bend left=75]     node{$1$}	(A)
            (B) edge[bend left, dashed, red]     node{$0$}	(A);
        \end{tikzpicture}
        \\
            Arm $1$ is ergodic & Arm $2$
        \end{tabular}
        \caption{
            2-state restless Markovian arms.
            The black arrows show state transition of action activate and the red dashed one for action rest.
            The numbers along the arrows show the expected reward when executing the actions.
            The double head arrows show the state transition of optimal actions.
        }
        \label{fig:hard_local5}
    \end{figure}

    \begin{figure}[htbp]
        \centering
        \begin{tikzpicture}[on grid, state/.style={circle,draw}, >= stealth', auto, prob/.style = {inner sep=1pt,font=\scriptsize}]
                \node[state]  (A) {$1,1$};
                \node[state]  (B) [right = 2.5cm of A]   {$1,2$};
                \node[state]  (C) [right = 2.5cm of B]   {$2,1$};
                \node[state]  (D) [left = 2.5cm of A]   {$2,2$};
                \path[->]
                (B) edge[bend left, black!45!green]     node{$0,\delta$}	(A)
                (B) edge[bend right, black!45!green]     node[below]{$0,1{-}\delta$}	(C)
                (A) edge[bend right=80, black!30!purple]     node[above]{$1{-}\varepsilon,1$}	(D)
                (C) edge[bend right=80, black!30!purple]     node[above]{$1{-}\varepsilon,1$}	(B);
                % transition of optimal actions
                \draw[-{Stealth}{Stealth}]
                (A) edge[bend right, black!45!green]     node[above]{$0,1{-}\delta$}	(D)
                (A) edge[bend left, black!45!green]     node{$0,\delta$}	(B)
                (B) edge[bend right=80, black!30!purple]     node[below]{$1,1$}	(C)
                (C) edge[bend right, black!45!green]     node[above]{$1,1$}	(B)
                (D) edge[bend right, black!45!green] node[below]{$1,1$} (A)
                (D) edge[bend right=80, black!30!purple] node[below]{$1,1$} (A);
        \end{tikzpicture}
        \caption{
            A restless bandit with two arms given in \figurename~\ref{fig:hard_local5}.
            This global MDP is communicating.
            Each time, exactly one arm is activated.
            The global state is denoted by $(s_1,s_2)$ where $s_1$ is the state of Arm $1$ and $s_2$ of Arm $2$.
            The green arrows show the state transition when activating Arm $1$.
            The purple arrows show the state transition when activating Arm $2$.
            The numbers along the arrows show the expected reward and transition probability (if the transition is probabilistic) when executing the actions.
            The double head arrows show the state transition of optimal actions.
            The suboptimal policies induce two recurrent classes: $\gX^1:=\{(1,2), (2,1)\}$ and $\gX^2:=\{(1,1),(2,2)\}$.
            The optimal gain in class $\gX^1$ is $1$.
            The suboptimal gain in class $\gX^2$ is $1-\varepsilon/2$.
        }
        \label{fig:hard_global5}
    \end{figure}
\end{proof}



    \begin{figure}[htbp]
        \centering
        \begin{tabular}{cc}
        \begin{tikzpicture}[on grid, state/.style={circle,draw}, >= stealth', auto, prob/.style = {inner sep=1pt,font=\scriptsize}]
            \node[state, black!45!green]  (A) {$1$};
            \node[state, black!45!green]  (B) [below = 2cm of A]   {$2$};
            \path[->]
            (A) edge[bend left=75]     node{$0,p{=}1{-}\delta$}	(B)
            (A) edge[loop above]     node[above]{$0,p{=}\delta$}	(A)
            (A) edge[bend left, dashed, red]     node{$0$}	(B)
    	    (B) edge[bend left=75]     node{$1$}	(A)
            (B) edge[bend left, dashed, red]     node{$0$}	(A);
        \end{tikzpicture}
        &
        \begin{tikzpicture}[on grid, state/.style={circle,draw}, >= stealth', auto, prob/.style = {inner sep=1pt,font=\scriptsize}]
            \node[state, black!45!green]  (A) {$1$};
            \node[state, black!45!green]  (B) [below = 2cm of A]   {$2$};
            \path[->]
            (A) edge[bend left=75]     node{$0,p{=}1{-}\delta{-}\varepsilon$}	(B)
            (A) edge[loop above]     node[above]{$0,p{=}\delta{+}\varepsilon$}	(A)
            (A) edge[bend left, dashed, red]     node{$0$}	(B)
    	    (B) edge[bend left=75]     node{$1$}	(A)
            (B) edge[bend left, dashed, red]     node{$0$}	(A);
        \end{tikzpicture}
        \\
            Arm $1$ is ergodic & Arm $2$ is ergodic
        \end{tabular}
        \caption{
            2-state restless Markovian arms.
            The black arrows show state transition of action activate and the red dashed one for action rest.
            The numbers along the arrows show the expected reward when executing the actions.
            The double head arrows show the state transition of optimal actions.
        }
        \label{fig:hard_local4}
    \end{figure}

    \begin{figure}[htbp]
        \centering
        \begin{tikzpicture}[on grid, state/.style={circle,draw}, >= stealth', auto, prob/.style = {inner sep=1pt,font=\scriptsize}]
                \node[state]  (A) {$1,1$};
                \node[state]  (B) [right = 2.5cm of A]   {$1,2$};
                \node[state]  (C) [right = 2.5cm of B]   {$2,1$};
                \node[state]  (D) [left = 2.5cm of A]   {$2,2$};
                \path[->]
                (A) edge[bend right, black!45!green]     node[above]{$0,p{=}1{-}\delta$}	(D)
                (A) edge[bend left, black!45!green]     node{$0,p{=}\delta$}	(B)
                (B) edge[bend left, black!45!green]     node{$0,p{=}\delta$}	(A)
                (B) edge[bend right, black!45!green]     node[below]{$0,p{=}1{-}\delta$}	(C)
                (C) edge[bend right=80, black!30!purple]     node[above]{$0,p{=}1{-}\delta{-}\varepsilon$}	(B)
                (C) edge[bend left=90, black!30!purple]     node[above]{$0,p{=}\delta{+}\varepsilon$}	(A);
                % transition of optimal actions
                \draw[-{Stealth}{Stealth}]
                (A) edge[bend right=80, black!30!purple]     node[above]{$0,p{=}1{-}\delta{-}\varepsilon$}	(D)
                (A) edge[bend left=90, black!30!purple]     node[above]{$0,p{=}\delta{+}\varepsilon$}	(C)
                (B) edge[bend right=80, black!30!purple]     node[below]{$1,p{=}1$}	(C)
                (C) edge[bend right, black!45!green]     node[above]{$1,p{=}1$}	(B)
                (D) edge[bend right, black!45!green] node[below]{$1,p{=}1$} (A)
                (D) edge[bend right=80, black!30!purple] node[below]{$1,p{=}1$} (A);
        \end{tikzpicture}
        \caption{
            A restless bandit with two arms given in \figurename~\ref{fig:hard_local4}.
            This global MDP is communicating.
            Each time, exactly one arm is activated.
            The global state is denoted by $(s_1,s_2)$ where $s_1$ is the state of Arm $1$ and $s_2$ of Arm $2$.
            The green arrows show the state transition when activating Arm $1$.
            The purple arrows show the state transiiton when activating Arm $2$.
            The numbers along the arrows show the expected reward and transition probability (if the transition is probabilistic) when executing the actions.
            The double head arrows show the state transition of optimal actions.
            The suboptimal policies induce two recurrent classes: $\gX^1:=\{(1,2), (2,1)\}$ and $\gX^2:=\{(1,1),(2,2)\}$.
            The optimal gain in class $\gX^1$ is $1$.
            The suboptimal gain in class $\gX^2$ is $0.5$.
        }
        \label{fig:hard_global4}
    \end{figure}


\endgroup
