\begingroup
\let\clearpage\relax

%\dominitoc
\chapter{Learning in Average Reward Restless Markovian Bandits}
\label{ch:learning_restless}

In the previous chapter, we adapt a few learning algorithms to discounted rested Markovian bandits and showed that their regrets, which are upper bounded sublinearly in the number of arms, match the minimax Bayesian regret that we derived in the chapter.
We also showed that any optimistic index policy that computes optimistic index on each arm independently from the other arms does not guarantee the optimism in face of uncertainty (OFU) principle.
In this chapter, we consider the learning problem in restless Markovian bandit with average reward criterion.
Such a bandit suffers from the curse of dimensionality and general-purpose learning algorithm are not efficient when applied to restless bandit. Recently, a few learning algorithms have been specifically designed to work for restless bandits. Yet, they most often work only for very particular subclasses of restless bandits, or they hold under conditions that are very hard to verify.
Hence, in this chapter, we provide some explanations why the performance of a learning algorithm for a restless bandit depends, in general, on some structural conditions (like diameter or bias) of the global MDP. We show that the properties of the local arms (like ergodic or small diameter) do not, in general, imply similar properties for the global MDPs. This implies that defining a class of restless bandit that has desirable properties (like small diameter) is difficult. 

In Section~\ref{}, we...

\section{Introduction}

Restless Markovian bandits are the specific MDPs that manifest the curse of dimensionality.
The obvious effect of the curse is the bandit state size that is exponential in the number of arms.
%Moreover, not only the state size that is cursed, but also the MDP parameters such as diameter.
%However, to the best of our knowledge, there is yet no clear dependency between the diameter or mixing time of the global MDP and the number of arms in general restless Markovian bandit.

For learning generic MDPs with average reward criterion, the current best algorithms have a regret over $T$ time steps bounded by $\landauO(\sqrt{DSAT})$ in the unknown MDP with state size $S$, action size $A$, and diameter $D$ (see Table~\ref{ch:rl:tab:infinite} for more algorithms with regret guarantee).
This regret is bound valid for communicating MDPs.
To what we have observed, the parameters of the MDP that appear in regret upper bound are:
\begin{itemize}
    \item diameter $D$, defined by Definition~\ref{ch:rl:defn:diameter}, in \eg, \cite{jaksch2010near, fruit2020improved, tossou2019near}
    \item span of optimal bias $H\ge sp(\vh^*)$ in \eg, \cite{bartlett2012regal, ouyang2017learning, fruit2018efficient, zhang2019regret}
    \item mixing time $t_{mix}$, defined by Definition~\ref{ch:restless:defn:mixing_time}, in \eg, \cite{ortner2020regret}.
\end{itemize}
Applying these algorithms to learn restless Markovian bandits with average reward criterion provokes a few critical issues:
\begin{itemize}
    %\item verifying the structure of restless bandit is computationally ``expensive'' 
    \item the state size of restless bandit is exponential in the number of arms which makes the regret not scalable with the number of arms
    \item the diameter $D$ (if defined), mixing time $t_{mix}$ (if defined), or span of the optimal bias $H$ may also be exponential in the number of arms
    \item compute an optimal policy in restless bandit is PSPACE-hard \cite{papadimitriou1994complexity}, let alone the computation of optimistic policy
\end{itemize}
%These parameters are finite for MDPs with specific structure.
%That is, the mixing time is defined for ergodic MDPs and the diameter of a MDP is finite if and only if the MDP is communicating.
%These algorithms are then applicable to restless Markovian bandits only if the latter have the specified structure.

%It is then reasonable to assume some structure properties of the bandit.
For partially observed restless bandit, the work of \cite{ortner2012regret} derives colored-UCRL algorithm, a modified version of UCRL2, that achieves a regret sublinear in time.
Colored-UCRL successfully removes the exponentiality in the number of arms from the state size of the bandit in its regret.
However, the mixing time parameter yet appears in the regret bound and the dependency between the mixing time and the number of arms is unclear.
The same discussion goes to the work of \cite{jung2019thompson} that adapts TSDE to the same setting of restless bandit in \cite{ortner2012regret}.
To the best of our knowledge, Restless-UCB of \cite{wang2020restless} is the first algorithm that has a regret explicitly bounded linearly in the number of arms for partially observed restless bandit.
Yet, we must mention that this result is valid for a very restrictive class of restless bandit in which each arm is the birth-death Markov reward process.
Also, Restless-UCB requires a generative model to be able to uniformly explore the dynamic of each arm before commiting on its optimistic planning.
All of these works also assume an oracle that knows how to compute an optimal policy given a restless bandit.

The work of \cite{akbarzadeh2022learning} adapts TSDE to the fully observed restless bandit problem and successfully removes the exponentiality in the number of arms from the state size of the bandit in the Bayesian regret of the modified version, namely RB-TSDE.
However, the ergodicity coefficient of the global MDP appears in the regret bound and the dependency between such a coefficient and the number of arms is unclear.
In general, the ergodicity coefficient provides an upper bound on the mixing time of the MDP.

Finally, due to the curse of dimensionality, verifying the structure of the bandit may be computationally expensive, let alone the learning aspect.
This inspires us to study the implication of structure of arms in the structure of the bandit because verifying the structure of all arms is done linearly in the number of arms.

%That is, in average reward criterion, the structure of the MDP plays an important role in the performance of the learning algorithms.


%Reinforcement learning algorithms work depending on the assumption of MDP structure such as ergodic, communicating, and weakly communicating.

%(here should motivate the problem + add related work about learning and why it is related to diameters and other properties)

%(We should define local properties and global properties).

We show that:
\begin{itemize}
    \item Local ergodic does not imply global weakly communicating because of periodicity 
    \item Local ergodic (= recurrent + aperiodic) does not imply globally ergodic (for now, our only example is unichain. Can we do more?)
    \item For globally ergodic, diameter can be exponential in the number of arms
\end{itemize}

%We recall Table~\ref{ch:rl:tab:infinite} below
%
%\begin{table}[ht]
%\begin{tabular}{|l|l|l|l|}
%\hline
%Algorithm & Regret & Assump. on $M$ & Policy comp.            \\ \hline
%UCRL2 \cite{jaksch2010near}       & $\tilde{\landauO}(DS\sqrt{AT})$ & comm.         & EVI \\ 
%REGAL \cite{bartlett2012regal}      & $\tilde{\landauO}(HS\sqrt{AT})$  & weakly comm. & Intractable   \\ 
%TSDE \cite{ouyang2017learning}        & $\tilde{\landauO}(HS\sqrt{AT})$ & weakly comm. & VI       \\ 
%SCAL \cite{fruit2018efficient}        & $\tilde{\landauO}(HS\sqrt{AT})$ & weakly comm. & modified EVI           \\
%OSP \cite{ortner2020regret}         & $\tilde{\landauO}(\sqrt{t_{mix}SAT})$ & ergodic           & Intractable   \\ 
%UCRL2B \cite{fruit2020improved}      & $\tilde{\landauO}(\sqrt{\Gamma DSAT})$ & comm.      & EVI                           \\
%%KL-UCRL     & $\tilde{O}$(\sqrt{S\sum_{x,a}\sV^*_{x,a}T}) & ergodic                     & modified EVI                  \\ \hline
%EBF \cite{zhang2019regret}         & $\tilde{\landauO}(\sqrt{HSAT})$ & weakly comm.     & Intractable   \\
%UCRL-V \cite{tossou2019near}         & $\tilde{\landauO}(\sqrt{DSAT})$ & comm.     & modified EVI   \\ \hline
%Lower bound & $\landauOmega(\sqrt{DSAT})$ \cite{jaksch2010near} &                              &                               \\ \hline
%\end{tabular}
%\caption{The quest to minimax regret lower bound for model-based RL algorithms in infinite horizon average reward model with $S$ states, $A$ actions, and $T$ steps.
%$D$ is the diameter of the MDP, $H\ge sp(\vh^*)$ is the upper bound on the span of the optimal bias, $t_{mix}$ is the mixing time of the MDP and $\Gamma$ is the upper bound on the number of next possible states.
%EVI stands for Extended value iteration \cite{jaksch2010near}, VI for value iteration, assump. for assumption, comm. for communicating, and comp. for computation.
%Intractable here means that there is no efficient implementations.
%}
%\label{ch:restless:tab:infinite}
%\end{table}

%A few existing works have tried to remove the exponentiality in the state size of restless bandit such as \cite{jung2019regret, ortner2012regret, wang2020restless} for partially observed setting and \cite{akbarzadeh2022learning} for fully observed setting.
%However, removing the exponentiality of the number of arms in the $D, t_{mix}$, or $H$ remains unclear.
%Moreover, these quantities are defined only if the MDP has a specific structure.
%For instance, the mixing time is defined only if the MDP is ergodic.
%Such an assumption is costly to verified in restless bandit due the curse of dimensionality in the state size of the global MDP.

\section{Undiscounted restless Markovian bandit}
\label{ch:restless:sec:restless}

We consider a restless Markovian bandit having $n$ arms.
Each arm $\langle\gS_i, \{0,1\}, r_i, p_i\rangle$ is a MDP with finite state space $\gS_i$ of size $S_i$ and binary action space $\{0,1\}$ where $0$ denotes the action ``rest'' and $1$ denotes the action ``activate''.
If arm $i$ is in state $s_i$ and the decision maker executes $a_i\in\{0,1\}$, the arm incurs a random reward with expected value $r_i(s_i,a_i)$ and transitions to state $s'_i\in\gS_i$ with probability $p_i(s'_i\mid s_i,a_i)$.
Similarly to what is done in Chapter~\ref{ch:rested}, we assume that the state space of the arms are pairwise distinct: $\gS_i\cap\gS_j=\emptyset$ for any $i\neq j$.
So, we will drop the index $i$ from expected reward and transition if no confusion is possible: we denote them by $r(s_i,a_i)$ instead of $r_i(s_i,a_i)$ and by $p(s'_i\mid s_i,a_i)$ instead of $p_i(s'_i\mid s_i,a_i)$.

The sequential decision problem is presented as the following.
At time step $1$, the state of all arms denoted by $\vs_1:=(s_{1,1},\dots,s_{1,n})$ is sampled according to some initial distribution $\rho$ over the state space $\gX:=\gS_1\times\dots\times\gS_n$.
At time step $t\ge1$, the decision maker observes the current state of all arms denoted by $\vs_t:=(s_{t,1},\dots,s_{t,n})$ and activates exactly $m$ arms encoded by action $\va_t:=(a_{t,1},\dots,a_{t,n})$ such that $\va_t\in\{0,1\}^n$ and $\sum_{i=1}^{n} a_{t,i}=m$ where $m\in[n]$ is constant over time.
%The decision maker then receives a random reward $r_t=\sum_{i=1}^{n}r_{t,i}$ where $r_{t,i}$ is a random reward from arm $i$.
Each arm $i$ incurs then a random reward $r_{t,i}$ and makes
a transition to new state $s_{t+1,i}$ in function of $s_{t,i}$ and $a_{t,i}$ but independently from the other arms.
So, the restless Markovian bandit is a specific MDP -- that we denote by $M$ -- whose state space is $\gX$ and action space is $\gA(m):=\{\va\in\{0,1\}^n : \sum_{i=1}^{n}a_i =m\}$.
We say that $M$ is the \emph{global} MDP and its arm is the \emph{local} MDP.

The decision maker wants to compute a policy $\pi:\gX\mapsto\gA(m)$ that maximizes the gain as defined in Section~\ref{ch:mdp:sec:gain}: for any $\vs\in\gX$,
\begin{equation}
    \label{ch:restless:eq:obj}
    g^\pi(\vs):=\lim_{T\to+\infty}\frac1T \E^\pi\left[ \sum_{t=1}^{T} \sum_{i=1}^nr_{t,i} \mid \vs_1=\vs\right].
\end{equation}

As presented in Chapter~\ref{ch:mdp}, if the MDP $M$ has finite state and action spaces, then an optimal policy $\pi^*$ such that for all $\vs\in\gX, g^{\pi^*}(\vs)=g^*(\vs):=\max_{\pi}g^{\pi}(\vs)$ exists and is deterministic.
However, it is shown in \cite[Theorem~4]{papadimitriou1994complexity} that computing an optimal policy in undiscounted restless bandit $M$ is PSPACE-hard.

%In about 1980, Peter Whittle proposed a heuristic in the form of largest index rule, later known as \textbf{Whittle index policy}, for undiscounted restless bandit problem.
%The computational complexity of this heuristic is linear in the number of arms which makes it scalable for problems with large number of arms.  
%We discuss more about this index policy in the following section.
%Similar to what is done in Section~\ref{ssec:rested_formul}, 
%In consequence, for any global state-action pair $(\vs,\va)$ with $\vs\in\gX$ and $\va\in\gA$, the expected reward from the MDP $M$ is given by $r(\vs,\va)=\sum_{i=1}^{n}r(s_i,a_i)$.
%Moreover, $M$ transitions to next state $\vs'\in\gX$ with probability $p(\vs'\mid \vs,\va)=\prod_{i=1}^n p(s'_i\mid s_i,a_i)$.

%The activated arm incurs a random reward discounted like $\gamma^{t-1}r_t$ where $\gamma\in(0,1)$ is the discount factor.

\subsection{Structural properties of global MDP}
\label{ssec:mdp_params}

We presented the classification of MDPs in Definition~\ref{ch:mdp:defn:mdp_class} of Chapter~\ref{ch:mdp}.
We also divided the MDP space as presented in \figurename~\ref{ch:mdp:fig:mdp_class}.
In addition, we say that a MDP is \emph{recurrent} if for all policy $\pi$, the matrix $\mP^\pi$ defines a Markov chain where all states are recurrent (equivalently, for all states $s$ and $s'$, a Markov chain starting in $s$ will visit $s'$ with probability $1$).
A recurrent or unichain MDP is called \emph{aperiodic} if all matrices $\mP^\pi$ are aperiodic.
While the definition of ergodic MDP is given in Definition~\ref{ch:mdp:defn:mdp_class}, we also say that a MDP is \emph{ergodic} if it is recurrent and aperiodic.
Following \cite[Definition 5.1]{wei2020model},
\begin{defn}
    \label{ch:restless:defn:mixing_time}
    The mixing time of an ergodic MDP is defined as
    \begin{align*}
        t_{mix} := \max_{\pi}\min\left\{ t\ge1 : \lVert (\mP^\pi)^t(x,\cdot) - \mu^\pi\rVert_1 \le \frac14, \forall x\right\}.
    \end{align*}
\end{defn}
By Definition~\ref{ch:restless:defn:mixing_time}, the mixing time is the maximum time required for any policy starting at any initial state to make the state distribution $\frac14$-close (in $\ell_1$ norm) to the stationary distribution, the state distribution in steady regime.

We recall that the diameter of a MDP (see Definition~\ref{ch:rl:defn:diameter}) is finite if and only if the MDP is communicating.
It is shown in \cite[Appendix A]{jaksch2010near} that for any MDP with state space $\gX$ and action space $\gA$, the diameter is lower bounded by $\log_{|\gA|}|\gX|-3$.
This implies that the diameter of a restless bandit with $n$ arms, each arm has the same state size $S$ and each time exactly $m$ arms are activated, is lower bounded by
\begin{equation*}
    \log_{|\gA(m)|}|\gX| -3 = \sum_{i=1}^{n}\log_{|\gA(m)|}|\gS_i| -3 =n\log_{{n \choose m}}S -3.
\end{equation*}
This means that the diameter of any restless bandit is at least linear in the number of arms.

%\subsection{Span}
%
%Following \cite[Chapter~8]{puterman2014markov}, in finite-state Markov reward process $(\vr, \mP)$, we define $\bar{\mP}=\displaystyle\lim_{N\to+\infty}\frac1N\sum_{t=0}^{N-1}\mP^{t}$ as the Cesaro limit of the sequence $\{\mP^t\}_{t\ge0}$.
%This limit exists for finite-state process (see Section~A.4 of Appendix~A of \cite{puterman2014markov}).
%If $\bar{\mP}$ is stochastic, then the gain is $\vg=\bar{\mP}\vr$.
%The bias is defined according to the structure of Markov chain:
%\begin{enumerate}
%    \item if the Markov chain is \emph{aperiodic}, then the bias is $\vh=\displaystyle\sum_{t=0}^\infty(\mP^t-\bar{\mP})\vr$
%    \item if the Markov chain is \emph{periodic}, then the bias is $\vh=\displaystyle\lim_{N\to\infty}\frac1N\sum_{k=1}^N\sum_{t=0}^{k-1}(\mP^t-\bar{\mP})\vr$
%\end{enumerate}
%
%Let $\vh\in\sR^S$. The span of $\vh$ is given by $sp(\vh)=\max_xh(x) - \min_xh(x)$.

In the following, we study the structure of the global MDPs when the local arms all are well structured.
To do so, we will provide a few simple examples and counter-examples.
In those examples, the bandit always has two arms and exactly one arm is activated at each decision time.
To ease the exposition, Arm $1$ is drawn in green color and Arm $2$ in dark purple.
In each arm, the transitions of action activate is drawn in back color and those of action rest is drawn in dashed red color.
In the global MDP, the state transitions when activating Arm $1$ is drawn in green arrows and those when activating Arm $2$ is drawn in dark purple.
The transitions under optimal action are drawn in double-head arrows.
The expected reward and probability of transition are noted along the transition arrows.
However, if the transition is deterministic, \ie, the probability is $1$, we only note the expected reward along the transition arrows.


%The following theorem shows that a restless bandit whose arms are all unichain is not necessarily unichain. This is true under the additional condition that all arms are aperiodic.
\subsection{Negative results}

In general, the expected regret is frequently bounded based on the concentration inequalities. 
These inequalities encode how well an unknown parameter is estimated.
So, it should be safe to say that ``the regret is bounded if the unknown parameters of the MDP are well estimated''.
Meanwhile, for a restless Markovian arm having $S$ states, there are at most $2S+2S^2$ parameters.
This means that a restless bandit having $n$ arms, each with $S$ states, is a specific MDP in which there are $n(2S+2S^2)$ parameters instead of $\landauO(S^{n})$.
Intuitively, if each arm visits all of its states frequently, then the $2S+2S^2$ unknown parameters should be well estimated as well as the $n(2S+2S^2)$ unknown parameters of the global MDP.
Then, the expected regret in learning such a restless bandit should be bounded linearly in $n$.
Unfortunately, we show in this section that this reasoning is not always applicable. 

\subsubsection{Non learnable restless bandit}

We show in the following theorem that the assumption that each arm is unichain and has bounded span of optimal bias is not enough to make a restless bandit learnable.

\begin{thm}[Non learnable restless bandit]
    \label{thm:non_learnable}
    For any learning algorithm $\gL$, there is a restless bandit having all unichain arms, each has a local span of optimal bias bounded by $0.5$, such that for any total number of time steps $T\ge2$, the expected regret of $\gL$ at least $\frac{T}4$.
\end{thm}
\begin{proof}
    Consider two restless bandit problems given in \figurename~\ref{fig:hard_global}.
    The arms of both bandits are given in \figurename~\ref{fig:hard_local}.
    All arms are unichain and have a local span of optimal bias bounded by $0.5$.
    Both bandits $M^a$ and $M^b$ have two recurrent classes $\gX^1$ whose optimal gain is $0.5$ and $\gX^2$ whose optimal gain is $1$.
    So, the regret gap when ending up in $\gX^1$ instead of $\gX^2$ is $\frac12$.
    At time step $1$, both $M^a$ and $M^b$ are in state $(3,1)$ with probability $0.5$ and state $(3,2)$ with probability $0.5$.
    Consider any learning algorithm $\gL$ that knows the parameters of both $M^a$ and $M^b$ but does not know with which one it is interacting.
    In states $(3,1)$, $\gL$ activates Arm $1$ with probability $\mu_1$ and in $(3,2)$, $\gL$  rests Arm $1$ with probability $\mu_2$ where $\mu_1$ and $\mu_2$ are our degree of freedoms to minimize the expected regret.

    Facing $M^a$, $\gL$ ends up in $\gX^1$ with probability $(1-\mu_1)$ when starting in $(3,1)$ and with probability $(1-\mu_2)$ when starting in $(3,2)$.
    Then, its expected regret is
    \begin{align*}
        \ex{\Reg(\gL,M^a,T)}
        &=0.5\Bigl((1-\mu_1)\frac{T}2 +\mu_1\times 0\Bigr) +0.5\Bigl((1-\mu_2)\frac{T}2 +\mu_2\times 0\Bigr)\\
        &=\frac{T}2\left(1-\frac{\mu_1+\mu_2}2\right).
    \end{align*}
    Facing $M^b$, $\gL$ ends up in $\gX^1$ with probability $\mu_1$ when starting in $(3,1)$ and with probability $\mu_2$ when starting in $(3,2)$ and its expected regret is
    \begin{align*}
        \ex{\Reg(\gL,M^b,T)}
        &=0.5\Bigl((1-\mu_1)\times 0 +\mu_1\frac{T}2\Bigr) +0.5\Bigl((1-\mu_2)\times 0 +\mu_2\frac{T}2\Bigr)\\
        &=\frac{T}2\times\frac{\mu_1+\mu_2}2.
    \end{align*}
    Then $\gL$ wants to adjust $\mu_1$ and $\mu_2$ such that
    \begin{equation}
        \label{eq:rg_linear}
        \min_{\mu_1,\mu_2\in[0,1]}\left(\max\biggl\{\frac{T}2\left(1-\frac{\mu_1+\mu_2}2\right), \frac{T}2\times\frac{\mu_1+\mu_2}2\biggr\}\right).
    \end{equation}
    For the best possible choice of $(\mu_1,\mu_2)$, the minimum value of \eqref{eq:rg_linear} is $\displaystyle\frac{T}4$.
    This means that no learning algorithms can achieve an expected regret smaller than $\frac{T}4$ on both models at the same time.
    That concludes the proof.

    \begin{figure}[htbp]
        \centering
        \begin{tabular}{ccc}
        \begin{tikzpicture}[on grid, state/.style={circle,draw}, >= stealth', auto, prob/.style = {inner sep=1pt,font=\scriptsize}]
            \node[state, black!45!green,line width=0.4mm]  (A) {$1$};
            \node[state, black!45!green,line width=0.4mm]  (B) [below = 2cm of A]   {$2$};
            \node[state, black!45!green,line width=0.4mm]  (C) [below left = 1cm and 2cm of A]   {$3$};
            \path[->]
            (A) edge[bend left=75]     node{$0$}	(B)
            (A) edge[bend left, dashed, red]     node{$0$}	(B)
    	    (B) edge[bend left=75]     node{$1$}	(A)
    	    (C) edge[bend left]     node{$0$}	(A)
            (C) edge[bend right, dashed, red]     node[below]{$0$}	(B)
            (B) edge[bend left, dashed, red]     node{$0$}	(A);
        \end{tikzpicture}
        &
        \begin{tikzpicture}[on grid, state/.style={circle,draw}, >= stealth', auto, prob/.style = {inner sep=1pt,font=\scriptsize}]
            \node[state, black!45!green,line width=0.4mm]  (A) {$1$};
            \node[state, black!45!green,line width=0.4mm]  (B) [below = 2cm of A]   {$2$};
            \node[state, black!45!green,line width=0.4mm]  (C) [below left = 1cm and 2cm of A]   {$3$};
            \path[->]
            (A) edge[bend left=75]     node{$0$}	(B)
            (A) edge[bend left, dashed, red]     node{$0$}	(B)
    	    (B) edge[bend left=75]     node{$1$}	(A)
    	    (C) edge[bend left, dashed, red]     node{$0$}	(A)
            (C) edge[bend right]     node[below]{$0$}	(B)
            (B) edge[bend left, dashed, red]     node{$0$}	(A);
        \end{tikzpicture}
        &
        \begin{tikzpicture}[on grid, state/.style={circle,draw}, >= stealth', auto, prob/.style = {inner sep=1pt,font=\scriptsize}]
            \node[state, RoyalBlue,line width=0.4mm]  (A) {$1$};
            \node[state, RoyalBlue,line width=0.4mm]  (B) [below = 2cm of A]   {$2$};
            \path[->]
            (A) edge[bend left=75]     node{$0$}	(B)
            (A) edge[bend left, dashed, red]     node{$0$}	(B)
    	    (B) edge[bend left=75]     node{$1$}	(A)
    	    (B) edge[bend left, dashed, red]     node{$0$}	(A);
        \end{tikzpicture}
        \\
            Arm $1^a$ & Arm $1^b$ & Arm $2$ 
        \end{tabular}
        \caption{
            Restless Markovian arms. Arm $1^a$ and Arm $1^b$ has 3 states and Arm $2$ has 2 states.
            The black arrows show state transition of action activate and the dashed red one for action rest.
            The numbers along the arrows show the expected reward when executing the actions.
            The span of the optimal bias in any arms is $0.5$.
            The two corresponding global MDPs is given in \figurename~\ref{fig:hard_global}
        }
        \label{fig:hard_local}
    \end{figure}
    
    \begin{figure}[htbp]
        \centering
        \begin{tabular}{cc}
        \begin{tikzpicture}[on grid, state/.style={circle,draw}, >= stealth', auto, prob/.style = {inner sep=1pt,font=\scriptsize}]
            \node[state]  (A) {$1,1$};
            \node[state]  (B) [above = 2cm of A]   {$2,2$};
            \node[state]  (C) [right = 4cm of B]   {$1,2$};
            \node[state]  (D) [below = 2cm of C]   {$2,1$};
            \node[state]  (E) [right = 2cm of B]   {$3,1$};
            \node[state]  (F) [below = 2cm of E]   {$3,2$};
            \path[->]
            (A) edge[line width=0.4mm, bend left, black!45!green]     node{$0$}	(B)
    	    (C) edge[line width=0.4mm, bend left, black!45!green]     node{$0$}	(D)
            (D) edge[line width=0.4mm, bend left=75, RoyalBlue] node{$0$} (C)
            (A) edge[line width=0.4mm, bend left=75, RoyalBlue]     node{$0$}	(B)
            (E) edge[line width=0.4mm, RoyalBlue] node[above]{$0$} (B)
            (F) edge[line width=0.4mm, black!45!green] node[below]{$0$} (A)
            %\draw[-{Stealth}{Stealth}]
    	    (B) edge[line width=0.4mm, bend left, black!45!green]     node{$1$}	(A)
            (D) edge[line width=0.4mm, bend left, black!45!green] node{$1$} (C)
    	    (B) edge[line width=0.4mm, bend left=75, RoyalBlue]     node{$1$}	(A)
    	    (C) edge[line width=0.4mm, bend left=75, RoyalBlue]     node{$1$}	(D)
            (E) edge[line width=0.4mm, black!45!green] node[above]{$0$} (C)
            (F) edge[line width=0.4mm, RoyalBlue] node[below]{$1$} (D);
        \end{tikzpicture}
        &
        \begin{tikzpicture}[on grid, state/.style={circle,draw}, >= stealth', auto, prob/.style = {inner sep=1pt,font=\scriptsize}]
            \node[state]  (A) {$1,1$};
            \node[state]  (B) [above = 2cm of A]   {$2,2$};
            \node[state]  (C) [right = 4cm of B]   {$1,2$};
            \node[state]  (D) [below = 2cm of C]   {$2,1$};
            \node[state]  (E) [right = 2cm of B]   {$3,1$};
            \node[state]  (F) [below = 2cm of E]   {$3,2$};
            \path[->]
            (A) edge[line width=0.4mm, bend left, black!45!green]     node{$0$}	(B)
    	    (C) edge[line width=0.4mm, bend left, black!45!green]     node{$0$}	(D)
            (D) edge[line width=0.4mm, bend left=75, RoyalBlue] node{$0$} (C)
            (A) edge[line width=0.4mm, bend left=75, RoyalBlue]     node{$0$}	(B)
            (E) edge[line width=0.4mm, black!45!green] node[above]{$0$} (B)
            (F) edge[line width=0.4mm, RoyalBlue] node[below]{$1$} (A)
            %\draw[-{Stealth}{Stealth}]
    	    (B) edge[line width=0.4mm, bend left, black!45!green]     node{$1$}	(A)
            (D) edge[line width=0.4mm, bend left, black!45!green] node{$1$} (C)
    	    (B) edge[line width=0.4mm, bend left=75, RoyalBlue]     node{$1$}	(A)
    	    (C) edge[line width=0.4mm, bend left=75, RoyalBlue]     node{$1$}	(D)
            (E) edge[line width=0.4mm, RoyalBlue] node[above]{$0$} (C)
            (F) edge[line width=0.4mm, black!45!green] node[below]{$0$} (D);
        \end{tikzpicture}
        \\
            $M^a:=\{\text{Arm }1^a, \text{Arm }2\}$ & $M^b:=\{\text{Arm }1^b, \text{Arm }2\}$
        \end{tabular}
        \caption{
            Two restless bandits, each with two arms given in \figurename~\ref{fig:hard_local}.
            Exactly one arm is activated at each decision time and the initial global state is either $(3,1)$ or $(3,2)$.
            The global state is denoted by $(s_1,s_2)$ where $s_1$ is the state of Arm $1$ and $s_2$ of Arm $2$.
            The green arrows show the state transition when activating Arm $1$.
            The purple arrows show the state transition when activating Arm $2$.
            The numbers along the arrows show the expected reward when executing the actions.
            The global MDP $M^a$ is formed by Arm $1^a$ and Arm $2$ and $M^b$ by Arm $1^b$ and Arm $2$.
            Both MDPs are multichain and not weakly communicating because state $(3,1)$ and $(3,2)$ are transient states and there are two recurrent classes: $\gX^1:=\{(1,1), (2,2)\}$ and $\gX^2:=\{(1,2),(2,1)\}$.
            The optimal gain in class $\gX^1$ is $0.5$ and in class $\gX^2$ is $1$.
            This shows that the actions in states $(3,1)$ and $(3,2)$ are decisive.
            In $M^a$, the optimal action in state $(3,1)$ is to rest Arm $2$ and in state $(3,2)$ is to activate Arm $2$.
            In contrast, in $M^b$, the optimal action in state $(3,1)$ is to activate Arm $2$ and in state $(3,2)$ is to rest Arm $2$.
            Starting by either $(3,1)$ or $(3,2)$, no learning algorithms have a sublinear expected regret in both examples.
        }
        \label{fig:hard_global}
    \end{figure}
\end{proof}

Theorem~\ref{thm:non_learnable} inspires use to define a more restrictive assumption on local arm.
We should mention that the global MDPs used in the proof of Theorem~\ref{thm:non_learnable} are not weakly communicating. So, their diameter is infinite.
%We will see in the following that there exists restless bandits whose diameter is exponential in the number of arms.

\subsubsection{Hard to learn restless bandits whose arms all are ergodic}

The following theorem shows that a restless bandit whose arms are all ergodic or all recurrent is not necessarily unichain.

\begin{thm}[Multichain restless bandit]
    \label{thm:multichain}
    For restless bandits with finite number of arms and each arm has finite number of states,
    \begin{enumerate}[label=(\roman*)]
        \item \label{thm:not_ergodic} there exists a restless bandit whose arms are all recurrent that is not weakly communicating. % the term noncommunicating is taken from [Puterman, 1994, page 353]. 
        \item \label{thm:ergodic_arms_multichain_RB} there exists a restless bandit whose arms are all ergodic that is multichain.
    \end{enumerate}
\end{thm}

\begin{proof}
    \textbf{Proof of \ref{thm:not_ergodic}} -- It is given by the counter-example in which the restless bandit has two arms as given by \figurename~\ref{fig:recur_non_communicate} and exactly one arm is activated at each decision time.
    For each arm, both ``activate'' and ``rest'' actions induce the same state transition.
    In such example, the restless bandit is a multichain and not weakly communicating MDP because if we start at state $(1,1)$ or $(2,2)$, then there are two recurrent classes: one composed of $\{(1,1), (2,2)\}$ and the other one is $\{(1,2), (2,1)\}$.
    \begin{figure}[ht]
        \centering
        \begin{tabular}{ccc}
        \begin{tikzpicture}[on grid, state/.style={circle,draw}, >= stealth', auto, prob/.style = {inner sep=1pt,font=\scriptsize}]
            \node[state, black!45!green,line width=0.4mm]  (A) {$1$};
            \node[state, black!45!green,line width=0.4mm]  (B) [below = 2cm of A]   {$2$};
            \path[->]
            (A) edge[bend left=75]     node{$0$}	(B)
            (A) edge[bend left, dashed, red]     node{$0$}	(B)
    	    (B) edge[bend left=75]     node{$1$}	(A)
            (B) edge[bend left, dashed, red]     node{$0$}	(A);
        \end{tikzpicture}
        &
        \begin{tikzpicture}[on grid, state/.style={circle,draw}, >= stealth', auto, prob/.style = {inner sep=1pt,font=\scriptsize}]
            \node[state, RoyalBlue,line width=0.4mm]  (A) {$1$};
            \node[state, RoyalBlue,line width=0.4mm]  (B) [below = 2cm of A]   {$2$};
            \path[->]
            (A) edge[bend left=75]     node{$0$}	(B)
            (A) edge[bend left, dashed, red]     node{$0$}	(B)
    	    (B) edge[bend left=75]     node{$1$}	(A)
    	    (B) edge[bend left, dashed, red]     node{$0$}	(A);
        \end{tikzpicture}
        &
        \begin{tikzpicture}[on grid, state/.style={circle,draw}, >= stealth', auto, prob/.style = {inner sep=1pt,font=\scriptsize}]
                \node[state]  (A) {$1,1$};
                \node[state]  (B) [right = 1.5cm of A]   {$1,2$};
                \node[state]  (C) [right = 2cm of B]   {$2,1$};
                \node[state]  (D) [left = 2cm of A]   {$2,2$};
                \path[->]
                (A) edge[bend right, black!45!green, line width=0.4mm]     node[above]{$0$}	(D)
                (A) edge[bend right=80, RoyalBlue, line width=0.4mm]     node[above]{$0$}	(D)
                (B) edge[bend right, black!45!green, line width=0.4mm]     node[below]{$0$}	(C)
                (B) edge[bend right=80, RoyalBlue, line width=0.4mm]     node[below]{$0$}	(C)
                (C) edge[bend right, black!45!green, line width=0.4mm]     node[above]{$0$}	(B)
                (C) edge[bend right=80, RoyalBlue, line width=0.4mm]     node[above]{$0$}	(B)
                % transition of optimal actions
                (D) edge[bend right, black!45!green, line width=0.4mm] node[below]{$1$} (A)
                (D) edge[bend right=80, RoyalBlue, line width=0.4mm] node[below]{$1$} (A);
        \end{tikzpicture}
        \\
            Arm $1$ & Arm $2$ & Global MDP having four states.
        \end{tabular}
        \caption{
            A restless bandit with two arms and exactly one arm is activated at each decision time.
            %Arm $1$ is given and Arm $2$ is identical to Arm $1$ and given in \figurename~\ref{fig:hard_local}.
            The global state is denoted by $(s_1,s_2)$ where $s_1$ is the state of Arm $1$ and $s_2$ of Arm $2$.
            The green arrows show the state transition when activating Arm $1$.
            The blue arrows show the state transiiton when activating Arm $2$.
            The numbers along the green and blue arrows show the expected reward when executing the actions.\\
            For both arms, the black arrows show state transition of action activate and the dashed red one for action rest.
            The numbers along the black and dashed red arrows show the expected reward when executing the actions.
        }
        \label{fig:recur_non_communicate}
    \end{figure}
    \medskip \\
    \textbf{Proof of \ref{thm:ergodic_arms_multichain_RB}} -- It is given by the following counter-example.
    Consider a restless bandit with two 8-state arms presented in \figurename~\ref{fig:ergodic_arm}.
    Both arms are identical in term of state transition that is summarized in Table~\ref{tab:ergodic_arm}.
    We observe that when rest, state $1$ and $2$ have the same possible next states $2$ and $3$.
    Similarly, the pair $\{3,4\}$ has $\{4,5\}$, $\{5,6\}$ has $\{6,7\}$, and $\{7,8\}$ has $\{8,1\}$.
    Similar pattern is observed under action activate: $\{2,3\}$ has $\{3,4\}$, $\{4,5\}$ has $\{5,6\}$, $\{6,7\}$ has $\{7,8\}$, and $\{8,1\}$ has $\{1,2\}$.
    This means that we can construct a cycle of transition by switching between rest (R) and activate (A):
    \begin{align*}
        \{1,2\} \overset{R}{\to} \{2,3\} \overset{A}{\to} \{3,4\} \overset{R}{\to} \{4,5\} \overset{A}{\to} \{5,6\} \overset{R}{\to} \{6,7\} \overset{A}{\to} \{7,8\} \overset{R}{\to} \{8,1\} \overset{A}{\to} \{1,2\}.
    \end{align*}
    With a proper synchronization between the states of Arm $1$ and Arm $2$, we can construct policies that induce two recurrent classes as presented in \figurename~\ref{fig:local_ergodic_multichain_RB}.
    Indeed, consider the following arrangement.
    \begin{align}
        &\text{Arm 1} : \{1,2\} \overset{R}{\to} \{2,3\} \overset{A}{\to} \{3,4\} \overset{R}{\to} \dots \overset{A}{\to} \{1,2\} \nonumber \\
        &\text{Arm 2} : \{4,5\} \overset{A}{\to} \{5,6\} \overset{R}{\to} \{6,7\} \overset{A}{\to} \dots \overset{R}{\to} \{4,5\} \nonumber \\
        &\text{MDP} : \begin{pmatrix}1,5\\2,4\\2,5\end{pmatrix} \overset{2}{\to} \begin{pmatrix}2,6\\3,5\\3,6\end{pmatrix} \overset{1}{\to} \begin{pmatrix}3,7\\4,6\\4,7\end{pmatrix} \overset{2}{\to} \dots \overset{1}{\to} \begin{pmatrix}1,5\\2,4\\2,5\end{pmatrix} \label{eq:mdp_transition}
    \end{align}
    where the symbol $\overset{i}{\to}$ in \Eqref{eq:mdp_transition} means the transition when Arm $i$ is activated.
    It means that any policies that activate Arm $2$ for any odd $s\in[8]$ and rest Arm $2$ for any even $s\in[8]$ when the global MDP is in state $(s,s+4), (s+1,s+3)$, and $(s+1,s+4)$ induce a recurrent class $\gX^1$ as given in \figurename~\ref{fig:local_ergodic_multichain_RB} (note that for $s$ big enough, state $9$ is state $1$, $10$ is $2$, etc).
    The recurrent class $\gX^2$ in \figurename~\ref{fig:local_ergodic_multichain_RB} is constructed by simply changing the sequence of Arm 2's state in the arrangement above and adapting the state of global MDP accordingly:
    \begin{align*}
        &\text{Arm 2} : \{8,1\} \overset{A}{\to} \{1,2\} \overset{R}{\to} \{2,3\} \overset{A}{\to} \dots \overset{R}{\to} \{8,1\} \\
        &\text{MDP} : \begin{pmatrix}1,1\\2,8\\2,1\end{pmatrix} \overset{2}{\to} \begin{pmatrix}2,2\\3,1\\3,2\end{pmatrix} \overset{1}{\to} \begin{pmatrix}3,3\\4,2\\4,3\end{pmatrix} \overset{2}{\to} \dots \overset{1}{\to} \begin{pmatrix}1,1\\2,8\\2,1\end{pmatrix}
    \end{align*}
    So, any policies that activate Arm $2$ for any odd $s\in[8]$ and rest Arm $2$ for any even $s\in[8]$ when the global MDP is in state $(s,s), (s+1,s+7)$, and $(s+1,s)$ induce the class $\gX^2$.
    All in all, any policies that activate Arm $2$ for any odd $s\in[8]$ and rest Arm $2$ for any even $s\in[8]$ when the global MDP is in state $(s,s), (s+1,s+7), (s+1,s), (s,s+4), (s+1,s+3)$ and $(s+1,s+4)$ induce two recurrent classes $\gX^1$ and $\gX^2$.
    That concludes the proof.
    \begin{table}[ht]
        \centering
        \begin{tabular}{|c|c|c|}
            \hline
            Current state & Next state when rest & Next state when activate \\\hline 
            $1$   & $2$ or $3$  &  $1$ or $2$ \\
            $2$   & $2$ or $3$  &  $3$ or $4$ \\
            $3$   & $4$ or $5$  &  $3$ or $4$ \\
            $4$   & $4$ or $5$  &  $5$ or $6$ \\
            $5$   & $6$ or $7$  &  $5$ or $6$ \\
            $6$   & $6$ or $7$  &  $7$ or $8$ \\
            $7$   & $8$ or $1$  &  $7$ or $8$ \\
            $8$   & $8$ or $1$  &  $1$ or $2$ \\ \hline
        \end{tabular}
        \caption{State transition of arms in \figurename~\ref{fig:ergodic_arm}.}
        \label{tab:ergodic_arm}
    \end{table}
    %in \figurename~\ref{fig:local_ergodic_multichain_RB}.
    \begin{figure}
        \centering
        \begin{tabular}{cc}
        \begin{tikzpicture}[on grid, state/.style={ellipse,draw}, >= stealth', auto, prob/.style = {inner sep=1pt,font=\scriptsize}]
            \node[state, black!45!green, line width=0.4mm]  (A) {$8$};
            \node[state, black!45!green, line width=0.4mm]  (B) [right = 1.6cm of A]    {$1$};
            \node[state, black!45!green, line width=0.4mm]  (C) [right = 1.6cm of B]    {$2$};
            \node[state, black!45!green, line width=0.4mm]  (D) [below = 1.6cm of C]    {$3$};
            \node[state, black!45!green, line width=0.4mm]  (E) [below = 1.6cm of D]    {$4$};
            \node[state, black!45!green, line width=0.4mm]  (F) [left = 1.6cm of E]    {$5$};
            \node[state, black!45!green, line width=0.4mm]  (G) [left = 1.6cm of F]    {$6$};
            \node[state, black!45!green, line width=0.4mm]  (H) [below = 1.6cm of A]    {$7$};
            \path[->]
                (A) edge[red, dashed, bend right=30] node{} (B)
                edge[red, dashed, loop above] node{} (A)
                (B) edge[red, dashed, bend right=30] node{} (C)
                edge[red, dashed, bend right=30] node{} (D)
                (C) edge[red, dashed, bend right=30] node{} (D)
                edge[red, dashed, loop above] node{} (C)
                (D) edge[red, dashed, bend right=30] node{} (E)
                edge[red, dashed, bend right=30] node{} (F)
                (E) edge[red, dashed, bend right=30] node{} (F)
                edge[red, dashed, loop below] node{} (E)
                (F) edge[red, dashed, bend right=30] node{} (G)
                edge[red, dashed, bend right=30] node{} (H)
                (G) edge[red, dashed, bend right=30] node{} (H)
                edge[red, dashed, loop below] node{} (G)
                (H) edge[red, dashed, bend right=30] node{} (A)
                edge[red, dashed, bend right=30] node{} (B)

                (A) edge[bend left=30]     node{}	(B)
                edge[bend left=50]     node{}	(C)
                (B) edge[loop above] node{} (B)
                edge[bend left=30]     node{}	(C)
                (C) edge[bend left=30]     node{}	(D)
                edge[bend left=50]     node{}	(E)
                (D) edge[loop right] node{} (D)
                edge[bend left=30]     node{}	(E)
                (E) edge[bend left=30]     node{}	(F)
                edge[bend left=50]     node{}	(G)
                (F) edge[loop below] node{} (F)
                edge[bend left=30]     node{}	(G)
                (G) edge[bend left=30]     node{}	(H)
                edge[bend left=50]     node{}	(A)
                (H) edge[loop left] node{} (H)
                edge[bend left=30]     node{}	(A);
        \end{tikzpicture} 
        &
        \begin{tikzpicture}[on grid, state/.style={ellipse,draw}, >= stealth', auto, prob/.style = {inner sep=1pt,font=\scriptsize}]
            \node[state, RoyalBlue,line width=0.4mm]  (A) {$8$};
            \node[state, RoyalBlue,line width=0.4mm]  (B) [right = 1.6cm of A]    {$1$};
            \node[state, RoyalBlue,line width=0.4mm]  (C) [right = 1.6cm of B]    {$2$};
            \node[state, RoyalBlue,line width=0.4mm]  (D) [below = 1.6cm of C]    {$3$};
            \node[state, RoyalBlue,line width=0.4mm]  (E) [below = 1.6cm of D]    {$4$};
            \node[state, RoyalBlue,line width=0.4mm]  (F) [left = 1.6cm of E]    {$5$};
            \node[state, RoyalBlue,line width=0.4mm]  (G) [left = 1.6cm of F]    {$6$};
            \node[state, RoyalBlue,line width=0.4mm]  (H) [below = 1.6cm of A]    {$7$};
            \path[->]
                (A) edge[red, dashed, bend right=30] node{} (B)
                edge[red, dashed, loop above] node{} (A)
                (B) edge[red, dashed, bend right=30] node{} (C)
                edge[red, dashed, bend right=30] node{} (D)
                (C) edge[red, dashed, bend right=30] node{} (D)
                edge[red, dashed, loop above] node{} (C)
                (D) edge[red, dashed, bend right=30] node{} (E)
                edge[red, dashed, bend right=30] node{} (F)
                (E) edge[red, dashed, bend right=30] node{} (F)
                edge[red, dashed, loop below] node{} (E)
                (F) edge[red, dashed, bend right=30] node{} (G)
                edge[red, dashed, bend right=30] node{} (H)
                (G) edge[red, dashed, bend right=30] node{} (H)
                edge[red, dashed, loop below] node{} (G)
                (H) edge[red, dashed, bend right=30] node{} (A)
                edge[red, dashed, bend right=30] node{} (B)

                (A) edge[bend left=30]     node{}	(B)
                edge[bend left=50]     node{}	(C)
                (B) edge[loop above] node{} (B)
                edge[bend left=30]     node{}	(C)
                (C) edge[bend left=30]     node{}	(D)
                edge[bend left=50]     node{}	(E)
                (D) edge[loop right] node{} (D)
                edge[bend left=30]     node{}	(E)
                (E) edge[bend left=30]     node{}	(F)
                edge[bend left=50]     node{}	(G)
                (F) edge[loop below] node{} (F)
                edge[bend left=30]     node{}	(G)
                (G) edge[bend left=30]     node{}	(H)
                edge[bend left=50]     node{}	(A)
                (H) edge[loop left] node{} (H)
                edge[bend left=30]     node{}	(A);
            \end{tikzpicture} \\
            Arm $1$ & Arm $2$
        \end{tabular}
        \caption{
            Two restless Markovian arms, each having $8$ states.
            Both arms are identical in term of transition structure.
            The black arrows show state transition of action activate and the dashed red one for action rest.
            To ease the exposition, the expected reward is not shown.
            Also, each state transition happens with probability $0.5$.
            That is, each arm goes from states $1$ and $2$ to state $2$ or $3$, states $3$ and $4$ to state $4$ or $5$, etc, under action rest, and goes from states $1$ and $8$ to state $1$ or $2$, etc, under action activate.
            We provide a list of transition in Table~\ref{tab:ergodic_arm}.
            The restless bandit having these two arms and exactly one arm is activated at each decision time is multichain because some policies induce two classes of recurrent states as shown in \figurename~\ref{fig:local_ergodic_multichain_RB}.
        }
        \label{fig:ergodic_arm}
    \end{figure}
    \begin{figure}
        \centering
        \begin{tabular}{c}
        \begin{tikzpicture}[on grid, state/.style={ellipse,draw}, >= stealth', auto, prob/.style = {inner sep=1pt,font=\scriptsize}]
            \node[state]  (A) {$\begin{tabular}{c}1,5\\2,4\\2,5\end{tabular}$};
            \node[state]  (B) [right = 2.4cm of A]    {$\begin{tabular}{c}2,6\\3,5\\3,6\end{tabular}$};
            \node[state]  (C) [right = 2.4cm of B]    {$\begin{tabular}{c}3,7\\4,6\\4,7\end{tabular}$};
            \node[state]  (D) [right = 2.4cm of C]    {$\begin{tabular}{c}4,8\\5,7\\5,8\end{tabular}$};
            \node[state]  (E) [below = 3cm of D]    {$\begin{tabular}{c}5,1\\6,8\\6,1\end{tabular}$};
            \node[state]  (F) [left = 2.4cm of E]    {$\begin{tabular}{c}6,2\\7,1\\7,2\end{tabular}$};
            \node[state]  (G) [left = 2.4cm of F]    {$\begin{tabular}{c}7,3\\8,2\\8,3\end{tabular}$};
            \node[state]  (H) [left = 2.4cm of G]    {$\begin{tabular}{c}8,4\\1,3\\1,4\end{tabular}$};
            \node[text width=2cm] (J) [left = 1.5cm of A] {Recur.\\Class $\gX^1$};
            \path[->]
            (A) edge[RoyalBlue,line width=0.4mm]     node{}	(B)
            (B) edge[black!30!green,line width=0.4mm]     node{}	(C)
            (C) edge[RoyalBlue,line width=0.4mm]     node{}	(D)
            (D) edge[black!30!green,line width=0.4mm]     node{}	(E)
    	    (E) edge[RoyalBlue,line width=0.4mm]     node{}	(F)
    	    (F) edge[black!30!green,line width=0.4mm]     node{}	(G)
            (G) edge[RoyalBlue,line width=0.4mm]     node{}	(H)
            (H) edge[black!30!green,line width=0.4mm]     node{}	(A);
        \end{tikzpicture} \\ \hfill\\
        \begin{tikzpicture}[on grid, state/.style={ellipse,draw}, >= stealth', auto, prob/.style = {inner sep=1pt,font=\scriptsize}]
            \node[state]  (A) {$\begin{tabular}{c}1,1\\2,8\\2,1\end{tabular}$};
            \node[state]  (B) [right = 2.4cm of A]    {$\begin{tabular}{c}2,2\\3,1\\3,2\end{tabular}$};
            \node[state]  (C) [right = 2.4cm of B]    {$\begin{tabular}{c}3,3\\4,2\\4,3\end{tabular}$};
            \node[state]  (D) [right = 2.4cm of C]    {$\begin{tabular}{c}4,4\\5,3\\5,4\end{tabular}$};
            \node[state]  (E) [below = 3cm of D]    {$\begin{tabular}{c}5,5\\6,4\\6,3\end{tabular}$};
            \node[state]  (F) [left = 2.4cm of E]    {$\begin{tabular}{c}6,6\\7,5\\7,4\end{tabular}$};
            \node[state]  (G) [left = 2.4cm of F]    {$\begin{tabular}{c}7,7\\8,6\\8,4\end{tabular}$};
            \node[state]  (H) [left = 2.4cm of G]    {$\begin{tabular}{c}8,8\\1,7\\1,5\end{tabular}$};
            \node[text width=2cm] (J) [left = 1.5cm of A] {Recur.\\Class $\gX^2$};
            \path[->]
            (A) edge[RoyalBlue,line width=0.4mm]     node{}	(B)
            (C) edge[RoyalBlue,line width=0.4mm]     node{}	(D)
    	    (E) edge[RoyalBlue,line width=0.4mm]     node{}	(F)
            (G) edge[RoyalBlue,line width=0.4mm]     node{}	(H)
            (B) edge[black!30!green,line width=0.4mm]     node{}	(C)
            (D) edge[black!30!green,line width=0.4mm]     node{}	(E)
    	    (F) edge[black!30!green,line width=0.4mm]     node{}	(G)
            (H) edge[black!30!green,line width=0.4mm]     node{}	(A);
        \end{tikzpicture}
        \end{tabular}
        \caption{
            State transition within two recurrent classes of a restless with 2 arms presented in \figurename~\ref{fig:ergodic_arm}.
            The first recurrent class is $\gX^1 =\{(s,s+4),(s+1,s+3),(s+1,s+4): s\in[8]\}$ and the second one is $\gX^2 =\{(s,s),(s+1,s),(s+1,s+7): s\in[8]\}$.
            These are two recurrent classes under any policies that activate Arm 2 for any odd $s\in[8]$ and Arm 1 for any even $s\in[8]$ when the bandit is in states $(s,s),(s,s+4),(s+1,s),(s+1,s+3),(s+1,s+4),(s+1,s+7)$.
            The blue arrows show the state transition when Arm $2$ is activated and the green ones show the one when Arm $1$ is activated.
            Each ellipse regroups the states having the same state transition under the same action.
            Note that each ellipse can transition to itself like in $\gX^1$ state $(2,4)$ can transition to state $(2,5)$ but it not shown.
            The rest of the states $\gX\setminus(\gX^1\cup\gX^2)$ are transient.
        }
        \label{fig:local_ergodic_multichain_RB}
    \end{figure}
\end{proof}

Theorem~\ref{thm:multichain} means that even though all arms are ergodic, the diameter or the mixing time of the global MDP are infinite or undefined.
%That is, the global MDP is multichain and weakly communicating.
%The following theorem shows that having the global MDP that is ergodic 

\begin{thm}
    For restless bandit with finite number of arms, each arm has a finite number of states,
    \begin{enumerate}[label=(\roman*)]
        \item \label{thm:diam} there exists a bandit having $n$ 2-state arms, each arm has the diameter bounded by $2$, that is ergodic and has a diameter bounded by $2^n$. % the term noncommunicating is taken from [Puterman, 1994, page 353]. 
        %\item \label{thm:aperiodic_RB_ergo} There exists a Restless bandit whose arms are all ergodic that is not recurrent. 
        \item \label{thm:mixing} there exists a bandit, whose arms are all ergodic with bounded mixing time, that is ergodic and has a mixing time in the same order of $\landauO(1/\varepsilon)$ for any $\varepsilon\in(0,1)$.
        \item \label{thm:span} there exists a bandit, whose arms all have a bounded local span of optimal bias, that is communicating and has a global span of optimal bias in the same order of $\landauO(1/\varepsilon)$ for any $\varepsilon\in(0,1)$.
    \end{enumerate}
\end{thm}
\begin{proof}
    \textbf{Proof of \ref{thm:diam}} -- consider an example of $n$ arms, each arm is a MDP with state space $\{1,2\}$ and has $0.5$ probability of changing state under both actions.
    It should be clear the global MDP is ergodic.
    The probability that the global MDP goes from state $(1,\dots,1)$ to state $(2,\dots,2)$ is $(1/2)^n$.
    Hence, the diameter of the global MDP is $2^n$.
    \medskip \\

    \textbf{Proof of \ref{thm:mixing}} -- Let $p_1$ and $p_2$ be the state transition functions of Arm $1$ and Arm $2$ given in \figurename~\ref{fig:ergodic_arm} respectively.
    $p_1$ and $p_2$ are both ergodic and induce the same state transition that is given in Table~\ref{tab:ergodic_arm}.
    Suppose that the mixing time of $p_1$ and $p_2$ is $t_{mix}>1$.
    Note that we can specify $p_1$ and $p_2$ such that $t_{mix}$ is small.
    Let's denote the bandit having two arms $p_1$ and $p_2$ by $M$.
    We have seen that $M$ is multichain.
    So the mixing time of $M$ is infinite.

    Now, consider two 8-state restless arms having transition function $u_1$ and $u_2$ where $u_1$ and $u_2$ induce the same state transition: any state $s\in[8]$ transitions to any other state $s'\in[8]$ with probability $\frac18$ under either action rest or activate.
    It is then clear that $u_1$ and $u_2$ are both ergodic and have the mixing time equals $1$.
    Any bandit having two arms $u_1$ and $u_2$ is an ergodic global MDP.

    For any $\varepsilon\in(0,1)$, consider two 8-state arms having transition function $p'_1=(1-\varepsilon)p_1+\varepsilon u_1$ and $p'_2=(1-\varepsilon)p_2+\varepsilon u_2$.
    That is, at each state transition, we toss a two-face coin whose head probability is $\varepsilon$.
    If the coin heads up, the arm evolves like $u$.
    If the coin tails up, the arm evoles like $p$.
    So, $p'_1$ and $p'_2$ are both ergodic.
    We know that if $\varepsilon=0$, the mixing time of $p'_1$ and $p'_2$ is $t_{mix}$ and if $\varepsilon=1$, the mixing time is $1$.
    Then, there exists $\varepsilon_0>0$ such that the mixing time of $p'_1$ and $p'_2$ is $t_{mix}/2$.
    For any $\varepsilon\in(0,\varepsilon_0]$, any bandit having two arms $p'_1$ and $p'_2$ is an ergodic global MDP denoted by $M'(\varepsilon)$.
    So, the mixing time of $M'(\varepsilon)$ is defined.
    However, when $\varepsilon\to0$, $M'(\varepsilon)\to M$.
    This is equivalent to say that when $\varepsilon\to0$, the mixing time of $M'(\varepsilon)$ tends to infinity.
    \medskip \\

    \textbf{Proof of \ref{thm:span}} -- We use the same technique in the proof of \ref{thm:mixing}.
    First of all, consider the bandit having $4$ global states as in Figure~\ref{fig:recur_non_communicate}.
    We denote this bandit by $M$ and the arms' transition functions by $p_1$ and $p_2$ and reward functions by $r_1$ and $r_2$.
    As we mentioned above, the local span of optimal bias of each arm is $0.5$ but $M$ is multichain.
    So, the global span of optimal bias is infinite.

    Consider now two two-state arms with reward functions $r_1$ and $r_2$, and transition functions $u_1$ and $u_2$ that induce the same state transition: any state $s\in[2]$ transitions to any other state $s'\in[2]$ with probability $\frac12$.
    So, $u_1$ and $u_2$ are both ergodic and the local span of optimal bias is simply $1$.

    With the same technique above, for any $\varepsilon\in(0,1)$, consider two two-state arms having reward functions $r_1$ and $r_2$, and transition functions $p'_1=(1-\varepsilon)p_1+\varepsilon u_1$ and $p'_2=(1-\varepsilon)p_2+\varepsilon u_2$.
    So, for any $\varepsilon\in(0,1)$, $p'_1$ and $p'_2$ are both ergodic and have the local span of optimal bias equals $\displaystyle\frac{1}{2-\varepsilon}$.
    We denote any bandit having two arms $(r_1,p'_1)$ and $(r_2,p'_2)$ by $M'(\varepsilon)$.
    We will see in Theorem~\ref{thm:aperiodic_RB_comm} that for any $\varepsilon\in(0,1)$, $M'(\varepsilon)$ is a communicating MDP.
    However, when $\varepsilon\to0$, $M'(\varepsilon)\to M$.
    This is equivalent to say that when $\varepsilon\to0$, the global span of optimal bias of $M'(\varepsilon)$ tends to infinity.
\end{proof}

\subsection{Positive results}

In this section, we show a few results in which the assumption on local arms implies ``positive'' properties on the global MDP.
The term ``positive'' refers to the fact that the properties frequently required by the learning algorithms for generic MDPs are satisfied.
This term does not imply neither the learning algorithm achieves sublinear expected regret nor their regret bounded is explicitly linear in the number of arms.

\begin{thm}
    \label{thm:aperiodic_RB_comm} A restless bandit whose arms are all ergodic is a communicating MDP.
\end{thm}
\begin{proof}
    We prove the theorem by its contraposition. Assume that a given global MDP is not communicating (either weakly communicating or not weakly communicating).
    In this MDP, there are global states that are not reachable from each other.
    This implies to two possibilities: (1) for some arms, some local states are not recurrent, (2) all arms are recurrent but periodic.
    Each possibility implies that there exists at least one arm that is not ergodic.
\end{proof}
This theorem means that if all arms are ergodic, then the global MDP has a finite diameter.
The following theorem also provides a result on the structure of global MDP.

\begin{thm}
    \label{thm:unichain}
    For restless bandit with finite number of arms, if all arms are unichain with at least one state reachable in one time step from any other states under both rest and activate actions, then the corresponding global MDP is also unichain with at least one global state reachable in one time step from any other global states under any policies.
\end{thm}
\begin{proof}
    Suppose that the bandit have $n$ arms and exactly $m$ arms are activated at each time step.
    For each arm $i\in[n]$, let $z_i$ be the state that is reachable in one time step from any other states under both rest and activate actions.
    We have that $p(z_i \mid s_i, a_i)>0$ for all $i\in[n], x_i\in\gS_i,a_i\in\{0,1\}$.
    For any action $\va\in\gA(m)$, any state $\vs,\vs'\in\gX,$ the global MDP $M$'s transition is given by ${p(\vs' \mid \vs, \va) =\prod_{i=1}^n p(s'_i \mid s_i, a_i)}$.
    Then, for any action $\va\in\gA(m)$, any state $\vs\in\gX$, we have
    \begin{align*}
        p(\vz \mid \vs, \va) =\prod_{i=1}^n p(z_i \mid s_i,a_i) > 0,
    \end{align*}
    because $n$ is finite and for any $i$, $p(z_i \mid s_i, a_i)>0$.
    Hence, the global state $\vz$ is reachable from any other global states in one step under any policies.
    In consequence, no policies induce a global Markov chain that has multiple closed irreducible recurrent classes.
    %there cannot be multiple recurrent classes that are closed under any policies.
    That concludes the proof.
\end{proof}

Last but not least, we show how the assumption on the ergodicity coefficient of local arms affects the ergodicity coefficient of the global MDP.
\begin{thm}
    \label{thm:ergodicity_coeff}
    For each arm $i\in[n]$, the ergodicity coefficient $\gamma_i$ of the arm is defined by
    \begin{align*}
        \gamma_i = 1-\min_{\substack{s_i,s'_i\in\gS_i \\a,a'\in\{0,1\}}} \sum_{z_i\in\gS_i} \min\{p(z_i \mid x_i, a), p(z_i \mid y_i, a')\}.
    \end{align*}
    Similarly, for any bandit with $n$ arms and any $m\in[n]$, the ergodicity coefficient $\Gamma$ of the bandit is defined by
    \begin{align*}
        \Gamma = 1-\min_{\substack{\vs,\vs'\in\gX \\\va,\va'\in\gA(m)}} \sum_{\vz\in\gX} \min\{p(\vz \mid \vs, \va), p(\vz \mid \vs', \va')\}.
    \end{align*}
    If $\gamma_i<1$ for any arm $i$, then $\Gamma<1$.

    Moreover, if there exists $\varepsilon>0$ such that for any arm $i\in[n]$,
    \begin{equation}
        \label{eq:gamma_ep}
        %\min_{\substack{s_i,s'_i\in\gS_i \\a,a'\in\{0,1\}}} \sum_{z_i\in\gS_i} \min\{p(z_i \mid s_i, a), p(z_i \mid s'_i, a')\} \ge \varepsilon,
        \gamma_i \le 1-\varepsilon,
    \end{equation}
    then $\Gamma \le 1-\varepsilon^n$.
\end{thm}
Hence, for a restless bandit with $n$ arms, if \eqref{eq:gamma_ep} holds for all $i\in[n]$, then the global span of optimal bias is bounded like 
\begin{align*}
 sp(\vh^*)\le \landauOmega(1/\varepsilon^n)
\end{align*}
where $\vh^*$ is the optimal bias of the global MDP.
Indeed, if \eqref{eq:gamma_ep} holds for all $i\in[n]$, then $\Gamma \le 1-\varepsilon^n$ and the global MDP is unichain.
Let $\pi^*$ be an optimal policy and $\vr^{\pi^*}$ and $\mP^{\pi^*}$ be the expected reward vector and transition matrix under policy $\pi^*$.
Suppose that for any global state $\vs$, $r^{\pi^*}(\vs)\in[0,r_{max}]$.
Then, the optimal gain $g^*$ is bounded in $[0, r_{max}]$.
We recall that the Bellman evaluation equation for $\pi^*$ in vector form is $\vh^*=\vr^{\pi^*} - g^*\vone +\mP^{\pi^*}\vh^*$.
Then, $sp(\vh^*)\le r_{max} +sp(\mP^{\pi^*}\vh^*)\le r_{max} +\Gamma sp(\vh^*)$ because $sp(\mP^{\pi^*}\vh^*)\le \Gamma sp(\vh^*)$.
So, $sp(\vh^*)\le r_{max}/\varepsilon^n$.

\begin{proof}
    For any two global states $\vs,\vs'\in\gX$ and any two actions $\va,\va'\in\gA(m)$, we have
    \begin{align*}
        \sum_{\vz\in\gX} \min\{p(\vz \mid \vs, \va), p(\vz \mid \vs', \va')\}
        &= \sum_{\vz\in\gX} \min\left\{\prod_{i=1}^np(z_i \mid s_i, a_i), \prod_{i=1}^np(z_i \mid s'_i, a'_i)\right\} \\
        &\ge \sum_{\vz\in\gX}\prod_{i=1}^n \min\{p(z_i \mid s_i, a_i), p(z_i \mid s'_i, a'_i)\} \\
        &= \prod_{i=1}^n \left(\sum_{z_i\in\gS_i} \min\{p(z_i \mid s_i, a_i), p(z_i \mid s'_i, a'_i)\}\right).
    \end{align*}
    %\KK{Add more detail about the second equality}
    Since for any $i\in[n]$, $\gamma_i<1$, we have that
    \begin{align*}
        \sum_{z_i\in\gS_i} \min\{p(z_i \mid s_i, a_i), p(z_i \mid s'_i, a'_i)\}
        &\ge \min_{\substack{x_i,y_i\in\gS_i \\a,a'\in\{0,1\}}} \sum_{z_i\in\gS_i} \min\{p(z_i \mid x_i, a), p(z_i \mid y_i, a')\} \\
        &= 1-\gamma_i >0.
    \end{align*}
    In consequences, ${\sum_{\vz\in\gX} \min\{p(\vz \mid \vs, \va), p(\vz \mid \vs', \va')\}{>}0}$.
    We conclude that $\Gamma{<}1$.\\
    Moreover, if there exists $\varepsilon>0$ such that \eqref{eq:gamma_ep} holds for any $i\in[n]$, then it follows that
    \begin{align*}
        \prod_{i=1}^n \left(\sum_{z_i\in\gS_i} \min\{p(z_i \mid s_i, a_i), p(z_i \mid s'_i, a'_i)\}\right)
        &\ge \varepsilon^n.
    \end{align*}
    We can conclude that $\Gamma\le 1-\varepsilon^n$ using its definition.
\end{proof}

\section{Assumptions for learning restless bandit}
\KK{I have not found a better title yet.}

\subsection{Regret definition and required assumptions}

Classically, the regret of the learner after $T$ time steps is given by
\begin{align}
    \Reg(M,T) := Tg^* - \sum_{t=1}^{T} \vr(\bX_{t},\bA_{t}). \label{eq:regret}
\end{align}
%This definition implicitly requires that the unknown MDP $M$ is weakly communicating and the reward is bounded.
This definition implicitly requires that the reward is bounded and the optimal gain $g^*$ is state-independent.

For Bayesian approach, the prior distribution $\phi$ of the unknown MDP is given.
We define the Bayesian regret by 
\begin{align}
    \BayReg(\phi, T) := \ex{\Reg(M, T)} \label{eq:bayreg}
\end{align}
where the expectation over all the randomness ($\phi$, agent's trajectory...).

In the following, We present a methodology to bound the regret.
Then, we go through the assumptions required for such methodology.

\subsubsection{UCRL2 and TSDE framework}

The algorithm updates its policy in episodic manner using doubling trick.
Let $K_T$ be the total number of episodes (or policy updates) up to time $T$.
Let $t^k$ be the time step when episode $k$ with the convention that $t^1:=1$.
Let $M^k$ be the imagined MDP of the algorithm for episode $k$ and $\pi^k$ be the policy used in episode $k$.
We require $\pi^k$ to induce state-independent gain in order to have the Bellman evaluation equations
\begin{align*}
    g^k +h^{\pi^k}(\vx) = \vr^k(\vx, \pi^k(\vx)) +\sum_{\vy}\vp^k(\vy \mid \vx, \pi^k(\vx))h^{\pi^k}(\vy).
\end{align*}
Then,
\begin{align*}
    \Reg(M, T)&=\sum_{k=1}^{K_T}\sum_{t=t^k}^{t^{k+1}-1} \underbrace{(g^*{-}g^k)}_{\Delta_1} {+}\underbrace{(\vr^k{-}\vr)(\mX_t,\mA_t)}_{\Delta_2} {+}\underbrace{\sum_{\vy}(\vp^k-\vp)(\vy \mid \mX_t, \mA_t)h^{\pi^k}(\vy)}_{\Delta_3}\\
              &\quad  \underbrace{h^{\pi^k}(\mX_{t+1}){-}h^{\pi^k}(\mX_{t})}_{\Delta_4} {+}\underbrace{\sum_{\vy}\vp(\vy \mid \mX_t, \mA_t)h^{\pi^k}(\vy)-h^{\pi^k}(\mX_{t+1})}_{\Delta_5}
\end{align*}

\begin{itemize}
    \item The second term $\Delta_2$ is bounded by sub-Gaussian inequality (or simply zero if $\{r_i\}_{i\in[n]}$ is known);
    \item The first term $\Delta_1$ and the last term $\Delta_5$ are bounded based on the approach used: for optimism, $\Delta_1$ is non-positive due to optimism and the sum of $\Delta_5$ is bounded by Azuma-Hoeffding inequality for martingale difference sequence. For Bayesian, $\Delta_1$ is bounded by $K_T$ and $\Delta_5$ has zero expectation.
\end{itemize}
The terms $\Delta_3$ and $\Delta_4$ require additional assumption on $h^{\pi^k}$
\begin{align*}
    sp(\vh^{\pi^k})\le H^k \text{ where } H^k<+\infty.
\end{align*}
%Let $\Pi^k$ be the set of \emph{gain optimal} policies in MDP $M^k$.
%The assumption needed is that 
%\begin{align*}
%    \Pi^k\neq \emptyset \text{ and }\sup_{\pi^k\in\Pi^k}sp(\vh^{\pi^k})\le H^k \text{ where } H^k<+\infty.
%\end{align*}
Under this assumption
\begin{itemize}
    \item the sum of $\Delta_4$ is the \emph{telescopic sum} which is bounded by $K_TH$ where $H:=\max_{k\le K_T}H^k$. Depending on the doubling trick used, $K_T$ is bounded \emph{a priori} by $O\big(\sqrt{SA\ln(T)}\big)$;
    \item $\Delta_3$ is bounded by $H\vert\vp^k-\vp\vert_{1}$ and $\vert\vp^k-\vp\vert_{1}$ is bounded by Weissman's inequality.
\end{itemize}

\paragraph{UCRL2 assumption}

The assumption used in UCRL2 is that the unknown MDP $M$ is \emph{communicating} (otherwise, the diameter is infinite) and has bounded non-negative reward.
This assumption implies
\begin{itemize}
    \item $g^*$ is state-independent and the regret is well defined;
    \item for each $k\ge1$, $M^k$ is communicating and $g^k$ is state-independent;
    \item for each $k\ge1$, $sp(\vh^{\pi^k})$ is bounded by the diameter of $M$.
\end{itemize}
The second item is true because the set of plausible MDPs $\gM^k$ can be considered as a MDP with finitely many actions.
If $M$ belongs to $\gM^k$, then $\gM^k$ is also \emph{communicating}.
$g^k$ can be considered as the optimal gain of this finitely many actions MDP $\gM^k$.
So, $g^k$ is state-independent.
The third item is true because $M$ belongs to $\gM^k$ together with $\gM^k$ is a MDP with finitely many actions imply that the diameter of $\gM^k$ is upper bounded by the diameter of $M$.
Under the assumption, \cite[Theorem~4]{bartlett2012regal} implies that the span of the optimal bias (the bias that verifies Bellman optimality equation) is bounded by the diameter of the MDP times the optimal gain.

%\KK{I cannot find any assumption about $sp(\vh^{\pi^k})\le H^k$ in UCRL2 paper}

\paragraph{TSDE assumption}

Let $\mathrm{supp}(\phi)$ denote the support of the distribution $\phi$.
The assumption used for TSDE is that $\mathrm{supp}(\phi)$ is a set of \emph{weakly communicating} MDPs and there exists $H<+\infty$ such that in any MDP drawn from the support, if $\vh\in\sR^S$ verifies Bellman optimality equation, then $sp(\vh)\le H$.
This assumption implies that
\begin{itemize}
    \item $g^*$ is state-independent and the regret is well defined;
    \item for each $k\ge1$, $M^k$ is weakly communicating and $g^k$ is state-independent;
    \item for each $k\ge1$, $sp(\vh^{\pi^k})$ is bounded by $H$.
\end{itemize}

\KK{What follows is just a draft. The notations are not rigorous at all.}

\subsubsection{Our assumptions}

For each arm $i\in[n]$, the unknown parameter $P_i$ belongs to a compact set $\Theta_i$.
Using Whittle index policy as a baseline, we need the two following assumptions:
\begin{asmp}
    \label{asmp:indexable}
    For any $i\in[n], P\in\Theta_i$, the arm $\langle\gS_i,\{0,1\},P,r_i\rangle$ is indexable.
\end{asmp}
\begin{asmp}
    \label{asmp:unichain}
    For any $i\in[n], P\in\Theta_i$, the arm $\langle\gS_i,\{0,1\},P,r_i\rangle$ has at least one state that is reachable in one step from any other states under any actions:
    \begin{align*}
        \beta_i = 1-\min_{\substack{x_i,y_i\in\gS_i \\a,a'\in\{0,1\}}} \sum_{z_i\in\gS_i} \min\{P^{a}(x_i,z_i), P^{a'}(y_i,z_i)\}<1.
    \end{align*}
\end{asmp}

Assumption~\ref{asmp:indexable} allows Whittle index policy to be meaningful.
By Theorem~\ref{thm:unichain}, Assumption~\ref{asmp:unichain} implies that the unknown RMAB is unichain. Consequently, the average gain of Whittle index policy is constant over all states of RMAB.
%Let $(\mP^W, \vr^W)$ be the controlled parameters of the corresponding RMAB under Whittle index policy $\vpi^W$ and $(g^W, h^W)$ be the average gain and bias function under $\vpi^W$ and satisfying
Let $(g^W, h^W)$ be the average gain and bias function under $\vpi^W$ and satisfying
\begin{align}
    g^W + h^W(\vx) = \vr(\vx,\va) +\sum_{\vz\in\bar{\gS}} \mP^{\va}(\vx, \vz)h^W(\vz),\hspace{1cm} \forall \vx\in\bar{\gS}, \va=\vpi^W(\vx). \label{eq:bellman}
\end{align}

\section{Open questions}

\begin{itemize}
    \item Does the above properties have implications on regret? 
    \item What about Lazy MDPs? \emph{i.e.} chains such that $P(i | i,a)>\varepsilon$ for all states and actions.
    \item If RMAB problem is in stationary regime, what is the regime of each arm?
\end{itemize}

\endgroup
