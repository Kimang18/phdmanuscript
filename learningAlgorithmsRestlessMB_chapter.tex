\begingroup

\let\clearpage\relax

\chapter{Learning in Average Reward Restless Markovian Bandits}
\label{ch:learning_restless}

In the previous chapter, we adapted a few learning algorithms to discounted rested Markovian bandits and showed that their regrets, which were upper bounded sub-linearly in the number of arms, matched the minimax Bayesian regret that we derived in the chapter.
We also showed that there was no index definition such that, relying on the confidence bonuses on arms' transition, computing optimistic index on each arm independently of the other arms guaranteed the optimism in face of uncertainty (OFU) principle in general discounted rested bandits.
In this chapter, we consider the learning problem in restless Markovian bandit with average reward criterion.
Such a bandit is a Markov decision process (MDP) that suffers from the curse of dimensionality, and general-purpose learning algorithms are not efficient when directly applied. Recently, a few learning algorithms have been specifically designed to work for restless bandits. Yet, they most often work only for very particular subclasses of restless bandits, or they require conditions that are computationally hard to verify.
Hence, in this chapter, we provide some arguments to explain why those conditions are needed when learning in restless Markovian bandits. We show that the properties of the local arms (like ergodic or small diameter) do not, in general, imply similar properties for the bandit. This implies that defining a class of restless bandits that have desirable properties (like small diameter) by relying on the assumption on local arms is difficult. 
Finally, we discuss a few issues when learning in general class of restless bandits and present RB-TSDE \cite{akbarzadeh2022learning} along with its regret analysis under a few technical assumptions.

In Section~\ref{ch:restless:contribute}, we provide the context of learning in restless bandit problems and present our contributions.
We discuss several existing works that design reinforcement learning (RL) algorithms for restless bandit problems in Section~\ref{ch:restless:related}.
We then recall the notations and the problem formulation of the undiscounted restless bandit in Section~\ref{ch:restless:sec:restless}.
Section~\ref{ch:restless:sec:example} accumulates examples and counter-examples that respectively show ``desirable'' and ``non-desirable'' properties of restless bandit when its arms all have ``desirable'' properties.
In Section~\ref{ch:restless:sec:generic}, we discuss the requirements when using Whittle index policy as the baseline policy in regret definition.
We also present an overview of regret analysis of RB-TSDE \cite{akbarzadeh2022learning}, a modified version of TSDE \cite{ouyang2017learning} for learning in restless bandits.
Section~\ref{ch:restless:sec:conclude} concludes this chapter.

\section{Contributions}
\label{ch:restless:contribute}

Restless Markovian bandits are the structured MDPs that manifest the curse of dimensionality.
The obvious effect of the curse is that the bandit's state size is exponential in the number of arms.
%However, to the best of our knowledge, there is yet no clear dependency between the diameter or mixing time of the global MDP and the number of arms in general restless Markovian bandit.

For learning generic MDPs with average reward criterion, the current best algorithms have a regret over $T$ time steps bounded by $\tilde{\landauO}(\sqrt{HSAT})$ in the unknown weakly communicating MDP with state size $S$, action size $A$, and an upper bound on span of the optimal bias function $H$ (see Table~\ref{ch:rl:tab:infinite} for more algorithms with regret guarantee).
%This regret bound is valid for communicating MDPs.
To what we have observed, the MDP properties that appear in regret upper bound are:
\begin{itemize}
    \item the diameter $D$ (defined by Definition~\ref{ch:rl:defn:diameter}) used in \eg, \cite{jaksch2010near, fruit2020improved, tossou2019near}
    \item the upper bound on span of the optimal bias function $H\ge sp(\vh^*)$ used in \eg, \cite{bartlett2012regal, ouyang2017learning, fruit2018efficient, zhang2019regret}
    \item the mixing time $t_{mix}$ (defined by Definition~\ref{ch:restless:mixing}) used in \eg, \cite{ortner2020regret}.
\end{itemize}
Then, applying these algorithms to learn restless Markovian bandits with average reward criterion provokes a few critical issues:
\begin{itemize}
    %\item verifying the structure of restless bandit is computationally ``expensive'' 
    \item the state size of restless bandit is exponential in the number of arms
    \item the diameter $D$ (if defined), the mixing time $t_{mix}$ (if defined), or the upper bound on span of the optimal bias $H$ may also be exponential in the number of arms
    \item compute an optimal policy in restless bandit is PSPACE-hard \cite{papadimitriou1994complexity}, let alone the computation of optimistic policy
\end{itemize}
So, the regret bound of those general-purpose RL algorithms is not scalable with the number of arms.
A few algorithms that are specifically designed for restless bandits successfully eliminate the exponentiality in the number of arms from the state size of bandit in their regret bound (see \eg, \cite{ortner2012regret, jung2019thompson, akbarzadeh2022learning}).
Yet, no clear dependency between the diameter, the mixing time, or the upper bound on span of the optimal bias function that appears in their regret bound and the number of arms is given.
This calls for the definition of subclass of restless bandits whose MDP properties can be expressed polynomially in the number of arms.

In this chapter, we provide a few results that support this argument and study how the assumptions on arms' structure translates in the structure of the bandit.
We show that no matter how good a learning algorithm is, there is a restless bandit, all of whose arms are unichain, that makes the algorithm suffer an expected regret linear in the total steps regardless of how many total steps the algorithm wants to learn.
This implies that no RL algorithms can perform uniformly well over the general class of restless bandits all of whose arms are unichain. 

Furthermore, we study the restless bandits having richer arms' structure.
We give an example showing that a restless bandit can be multichain even though all of its arms are ergodic.
Moreover, we also show that:
\begin{itemize}
    \item for restless bandits all of whose arms are ergodic,
        \begin{itemize}
            \item there is an ergodic bandit whose diameter is exponential in the number of arms;
            \item there is an ergodic bandit whose mixing time can be as big as we want;
        \end{itemize}
    \item for restless bandits that are communicating, there is a bandit, all of whose arms have a bounded span of local bias function, that has a span of global bias function as big as we want.
\end{itemize}
We also provide a piece of positive result related to the ergodicity coefficient of the bandit.
Finally, we provide a discussion on model-based RL algorithms that use Whittle index policy as the baseline policy in regret definition and provide an overview of regret analysis of RB-TSDE \cite{akbarzadeh2022learning} when learning in restless bandits.
All of our results suggest that the MDP properties of restless bandit can be exponential in the number of arms in general class of restless bandits.
This calls for work to design subclasses of restless bandits whose MDP properties can be expressed polynomially in the number of arms.
Another direction of work is to design algorithms that directly learns the Whittle index of the unknown restless bandit in model-free style.

%These parameters are finite for MDPs with specific structure.
%That is, the mixing time is defined for ergodic MDPs and the diameter of a MDP is finite if and only if the MDP is communicating.
%These algorithms are then applicable to restless Markovian bandits only if the latter have the specified structure.

%It is then reasonable to assume some structure properties of the bandit.


\section{Related work}
\label{ch:restless:related}

We believe that there are at least two directions of research for learning in an unknown restless Markovian bandits: (1) model-free algorithms that directly estimate the Whittle index and (2) model-based algorithms that estimate the parameters of the unknown restless bandit.

Since the Whitle index policy is asymptotically optimal when the restless bandit satisfies some conditions \cite{weber1990index} and performs extremely well in practice (see \eg, \cite{glazebrook2002index, ansell2003whittle, glazebrook2006some}), 
the first direction consists in learning the Whittle index of the unknown fully observed restless bandit.
The celebrated Q-learning (QL) algorithm \cite{watkins1989learning} is one of the most popular approach.
For discrete state-space restless arm, the work of \cite{gibson2021novel} learns the Whittle index by maintaining two Q-functions, updating them using QL algorithm, and deducing the Whittle index from them when needed.
Meanwhile, the work of \cite{fu2019towards} learns the Whittle index by maintaining only one Q-function, updating it using QL algorithm, and deducing the Whittle index as the smallest critical penalty that equalizes the estimated Q-value of action activate and the estimated Q-value of action rest.
Yet both works \cite{gibson2021novel, fu2019towards} only provide the numerical proof of convergence to the correct Whittle index, and no theoretical proof is given.
Using the ideas from two-timescale stochastic approximation (see \eg, \cite{abounadi2001learning, schwartz1993reinforcement}), the work of \cite{avrachenkov2022whittle} proposes a two-timescale QL algorithm for learning the Whittle index of any unknown indexable restless arms. Their algorithm maintains a Q-function and a vector of penalty separately, updates Q-function using QL algorithm and the vector of penalty using stochastic iterative algorithm with a slower timescale update compared to the QL algorithm.
The theoretical and numerical proofs of convergence are provided in their work \cite{avrachenkov2022whittle}.
For continuous state-space restless arm, the work of \cite{nakhleh2021neurwin} uses deep reinforcement learning framework to estimate the Whittle indices of the arms with large state space or convoluted transition kernel, assuming a notion of strong indexability.

The second direction is to design a model-based learning algorithm with theoretical performance guarantee.
For partially observed restless bandit, the work of \cite{ortner2012regret} derives colored-UCRL algorithm, a modified version of UCRL2, that achieves a regret bounded proportionally to $\sqrt{T}$ where $T$ is the total steps.
In its regret bound, colored-UCRL successfully removes the exponentiality in the number of arms from the state size of the bandit.
However, the mixing time parameter appears in the bound, and the dependency between the mixing time and the number of arms is unclear.
The same discussion goes to the work of \cite{jung2019thompson} that adapts TSDE \cite{ouyang2017learning} to the learning setting of restless bandit and assumes a condition similar to \cite{ortner2012regret}.
To the best of our knowledge, Restless-UCB of \cite{wang2020restless} is the first algorithm that has a regret bounded explicitly linearly in the number of arms for partially observed restless bandit.
Yet, we must mention that their regret bound is proportional to $T^{2/3}$, and their result is valid for a very restrictive class of restless bandits in which each arm is a birth-death Markov reward process whose state transition satisfies a few technical constraints.
Also, Restless-UCB requires a generative model to be able to uniformly explore the dynamic of each arm before committing on its optimistic planning.
All of these works also assume an oracle that knows how to compute an optimal policy given a restless bandit.
The work of \cite{akbarzadeh2022learning} adapts TSDE \cite{ouyang2017learning} to the fully observed restless bandit problem and successfully removes the exponentiality in the number of arms from the state size of the bandit in the Bayesian regret bound of RB-TSDE \cite{akbarzadeh2022learning}, the modified version of TSDE \cite{ouyang2017learning}.
However, the ergodicity coefficient of the bandit appears in the regret bound, and the dependency between such a coefficient and the number of arms is unclear.
It may be useful to note that the ergodicity coefficient provides an upper bound on the mixing time and the span of bias function of the MDP.

%These important related works inspires...
%This inspires us to study the implication of structure of arms in the structure of the bandit because verifying the structure of all arms can be done linearly in the number of arms.
%\KK{To finish}

\section{Undiscounted restless bandit}
\label{ch:restless:sec:restless}

We recall from Chapter~\ref{ch:mb} that a restless Markovian bandit is multi-armed bandit having $n\in\N^+$ arms.
Each arm $\langle\gS_i, \{0,1\}, r_i, p_i\rangle$ is an MDP with finite state space $\gS_i$ of size $S$ and binary action space $\{0,1\}$ where $0$ denotes the action ``rest'', and $1$ denotes the action ``activate''.
If arm $i$ is in state $s_i$, and the decision maker executes $a_i\in\{0,1\}$, the arm incurs a random reward with expected value $r_i(s_i,a_i)$ and transitions to state $s'_i\in\gS_i$ with probability $p_i(s'_i\mid s_i,a_i)$.
Similarly to what is done in Chapter~\ref{ch:learning_rested}, we assume that the state space of the arms are pairwise distinct: $\gS_i\cap\gS_j=\emptyset$ for any $i\neq j$.
So, we will drop the index $i$ from the expected reward and transition if no confusion is possible: we denote them by $r(s_i,a_i)$ instead of $r_i(s_i,a_i)$ and by $p(s'_i\mid s_i,a_i)$ instead of $p_i(s'_i\mid s_i,a_i)$.

At time step $1$, the state of all arms denoted by $\vs_1:=(s_{1,1},\dots,s_{1,n})$ is sampled according to some initial distribution $\rho$ over the state space $\gX:=\gS_1\times\dots\times\gS_n$.
At time step $t\ge1$, the decision maker observes the current state of all arms denoted by $\vs_t:=(s_{t,1},\dots,s_{t,n})$ and activates exactly $m$ arms encoded by action $\va_t:=(a_{t,1},\dots,a_{t,n})$ such that $\va_t\in\{0,1\}^n$ and $\sum_{i=1}^{n} a_{t,i}=m$, and $m\in[n]$ is constant over time.
Each arm $i$ incurs then a random reward $r_{t,i}$ and makes
a transition to new state $s_{t+1,i}$ in function of $s_{t,i}$ and $a_{t,i}$ but independently of the other arms.
So, the restless Markovian bandit is a specific MDP -- that we denote by $M$ -- whose state space is $\gX$, and action space is $\gA(m):=\{\va\in\{0,1\}^n : \sum_{i=1}^{n}a_i =m\}$.
We will say \emph{global} to refer to the quantities related to the bandit $M$ and \emph{local} to refer to the quantities related to the arms.
Without loss of generality, we assume that for any global state $\vs\in\gX$ and global action $\va\in\gA(m)$, the expected reward is bounded like $\sum_{i=1}^{n}r(s_i,a_i)\in[0,r_{max}]$.

The decision maker wants to compute a policy $\pi:\gX\mapsto\gA(m)$ that maximizes the gain as defined in Section~\ref{ch:mdp:sec:gain}: for any global state $\vs\in\gX$,
\begin{equation}
    \label{ch:restless:eq:obj}
    g^\pi_M(\vs):=\lim_{T\to+\infty}\frac1T \E^\pi\left[ \sum_{t=1}^{T} \sum_{i=1}^nr_{t,i} \mid \vs_1=\vs\right].
\end{equation}

As presented in Chapter~\ref{ch:mdp}, if the MDP $M$ has finite state and action spaces, then an optimal policy $\pi^*$ such that for all $\vs\in\gX, g^{\pi^*}(\vs)=g^*(\vs):=\max_{\pi}g^{\pi}(\vs)$ exists and is deterministic.
However, it is shown in \cite[Theorem~4]{papadimitriou1994complexity} that computing an optimal policy in undiscounted restless bandit $M$ is PSPACE-hard.

We recall that the diameter of an MDP (see Definition~\ref{ch:rl:defn:diameter}) is finite if and only if the MDP is communicating.
By \cite[Appendix A]{jaksch2010near}, the diameter of bandit $M$ is lower bounded by
\begin{equation*}
    \log_{|\gA(m)|}|\gX| -3 = \sum_{i=1}^{n}\log_{|\gA(m)|}|\gS_i| -3 =n\log_{{n \choose m}}S -3.
\end{equation*}
This lower bound is not exponential in $n$, which inspires us to study the diameter of restless bandit, and which gives us hope to remove the exponentiality in $n$ from the regret bound of general-purpose algorithms.

\subsection{Additional structural properties of MDP}
\label{ssec:mdp_params}

We presented the classification of MDPs in Definition~\ref{ch:mdp:defn:mdp_class} of Chapter~\ref{ch:mdp}.
We also divided the MDP space as presented in \figurename~\ref{ch:mdp:fig:mdp_class}.
In addition, we say that an MDP is \emph{recurrent} if for all policy $\pi$, the transition matrix $\mP^\pi$ defines a Markov chain where all states are recurrent (equivalently, for all states $s$ and $s'$, a Markov chain starting in $s$ will visit $s'$ with probability $1$).
A recurrent or unichain MDP is called \emph{aperiodic} if all matrices $\mP^\pi$ are aperiodic.
An equivalence of the definition of ergodic MDP in Definition~\ref{ch:mdp:defn:mdp_class} is that an MDP is \emph{ergodic} if it is recurrent and aperiodic.

As mentioned in the beginning, there exist learning algorithms that rely on the mixing time of the MDPs.
So, we provide the definition of mixing time below.
Following \cite[Definition 5.1]{wei2020model},
\begin{defn}[The mixing time of ergodic MDP]
    The mixing time of an ergodic MDP with state space $\gS$ is defined as
    \begin{align*}
        t_{mix} := \max_{\pi}\min\left\{ t\ge1 : \norm{(\mP^\pi)^t(s,\cdot) - \mu^\pi}_{\ell_1} \le \frac14, \forall s\in\gS\right\}.
    \end{align*}
    \label{ch:restless:mixing}
\end{defn}
By Definition~\ref{ch:restless:mixing}, the mixing time is the maximum time required for any policy starting at any initial state to make the state distribution $\frac14$-close (in $\ell_1$ norm) to the stationary distribution, the state distribution in steady regime.

\section{Local and global structures in restless bandits}
\label{ch:restless:sec:example}

In the following, we study the structure of the bandit when all the local arms are well-structured.
To do so, we will provide a few simple examples and counter-examples.
In those examples, the bandit always has two arms, and exactly one arm is activated at each decision time.
To ease the exposition, Arm $1$ is drawn in green color and Arm $2$ in blue.
In each arm, the state transitions of action activate are drawn in black color and those of action rest are drawn in dashed red color.
In the global MDP, the state transitions when activating Arm $1$ are drawn in green arrows, and those when activating Arm $2$ are drawn in blue.
If applicable, the expected reward and probability of transition are noted along the transition arrows.
However, if the transition is deterministic, \ie, the probability of transition is $1$, we only note the expected reward along the transition arrows.

\subsection{Negative results}

In general, the expected regret is frequently bounded based on the concentration inequalities. 
These inequalities encode how well an unknown parameter is estimated.
So, it should be safe to say that ``if the unknown parameters of the MDP are well estimated, then the regret is bounded sub-linearly in the total steps $T$''.
Meanwhile, for a restless Markovian arm having $S$ states, there are at most $2S+2S^2$ parameters.
This means that a restless bandit $M$ described above is a specific MDP in which there are $n(2S+2S^2)$ parameters instead of $\landauO(S^{n})$.
Intuitively, if each arm visits all of its states frequently, then the $2S+2S^2$ unknown parameters should be well estimated as well as the $n(2S+2S^2)$ unknown parameters of the bandit.
Then, the expected regret in learning such a restless bandit should be bounded not only linearly in $n$ but also sub-linearly in $T$.
Unfortunately, we show in this section that this reasoning is not always applicable.

\subsubsection{Non-learnable restless bandit all of whose arms are unichain}

We show in the following theorem that the assumption that each arm is unichain and has a bounded span of local bias function under any policy is not enough to make a restless bandit learnable.

\begin{thm}[Non learnable restless bandit]
    \label{thm:non_learnable}
    \begin{enumerate}[label=(\roman*)]
        \item[]
        \item \label{it:non_learnable1} For any learning algorithm $\gL$, there is a restless bandit all of whose arms are unichain and have a bounded span of local bias function under any policy such that for any total steps $T\ge2$, the expected regret of $\gL$ at least $\frac{T}4$.
        \item \label{it:non_learnable2} For any learning algorithm $\gL$, any total steps $T\ge2$, there is a restless bandit, all of whose arms are unichain and have a bounded span of local bias function under any policy, that is weakly communicating and such that the expected regret of $\gL$ is at least $\frac{T}5$.
    \end{enumerate}
\end{thm}
\begin{proof}
    \textbf{Proof of \ref{it:non_learnable1}} -- consider two restless bandit problems given in \figurename~\ref{fig:hard_global}.
    The arms of both bandits are given in \figurename~\ref{fig:hard_local}.
    All arms are unichain and have a bounded span of local bias under any policy.
    Both bandits $M^a$ and $M^b$ have two recurrent classes $\gX^1$ whose optimal gain is $0.5$ and $\gX^2$ whose optimal gain is $1$.
    So, the regret gap when ending up in $\gX^1$ instead of $\gX^2$ is $\frac12$.
    At time step $1$, both $M^a$ and $M^b$ are in state $(3,1)$ with probability $0.5$ and state $(3,2)$ with probability $0.5$.
    Consider any learning algorithm $\gL$ that knows the parameters of both $M^a$ and $M^b$ but does not know with which bandit it is interacting.
    In states $(3,1)$, $\gL$ activates Arm $1$ with probability $\theta_1$ and in $(3,2)$, $\gL$  rests Arm $1$ with probability $\theta_2$ where $\theta_1$ and $\theta_2$ are its degree of freedoms to minimize the expected regret.

    Facing $M^a$, $\gL$ ends up in $\gX^1$ with probability $(1-\theta_1)$ when starting in $(3,1)$ and with probability $(1-\theta_2)$ when starting in $(3,2)$.
    Then, its expected regret is
    \begin{align*}
        \ex{\Reg(\gL,M^a,T)}
        &=0.5\Bigl((1-\theta_1)\frac{T}2 +\theta_1\times 0\Bigr) +0.5\Bigl((1-\theta_2)\frac{T}2 +\theta_2\times 0\Bigr)\\
        &=\frac{T}2\left(1-\frac{\theta_1+\theta_2}2\right).
    \end{align*}
    Facing $M^b$, $\gL$ ends up in $\gX^1$ with probability $\theta_1$ when starting in $(3,1)$ and with probability $\theta_2$ when starting in $(3,2)$ and its expected regret is
    \begin{align*}
        \ex{\Reg(\gL,M^b,T)}
        &=0.5\Bigl((1-\theta_1)\times 0 +\theta_1\frac{T}2\Bigr) +0.5\Bigl((1-\theta_2)\times 0 +\theta_2\frac{T}2\Bigr)\\
        &=\frac{T}2\times\frac{\theta_1+\theta_2}2.
    \end{align*}
    Then $\gL$ wants to adjust $\theta_1$ and $\theta_2$ such that
    \begin{equation}
        \label{eq:rg_linear}
        \min_{\theta_1,\theta_2\in[0,1]}\left(\max\biggl\{\frac{T}2\left(1-\frac{\theta_1+\theta_2}2\right), \frac{T}2\times\frac{\theta_1+\theta_2}2\biggr\}\right).
    \end{equation}
    For the best possible choice of $(\theta_1,\theta_2)$, the minimum value of \eqref{eq:rg_linear} is $\displaystyle\frac{T}4$.
    This means that no learning algorithms can achieve an expected regret smaller than $\frac{T}4$ on both models at the same time.
    That concludes the proof.
    \medskip \\


    \begin{figure}[hb]
        \centering
        \begin{tabular}{ccc}
        \begin{tikzpicture}[on grid, state/.style={circle,draw}, >= stealth', auto, prob/.style = {inner sep=1pt,font=\scriptsize}]
            \node[state, black!45!green,line width=0.4mm]  (A) {$1$};
            \node[state, black!45!green,line width=0.4mm]  (B) [below = 2cm of A]   {$2$};
            \node[state, black!45!green,line width=0.4mm]  (C) [below left = 1cm and 2cm of A]   {$3$};
            \path[->]
            (A) edge[bend left=75]     node{$0$}	(B)
            (A) edge[bend left, dashed, red]     node{$0$}	(B)
    	    (B) edge[bend left=75]     node{$1$}	(A)
    	    (C) edge[bend left]     node{$0$}	(A)
            (C) edge[bend right, dashed, red]     node[below]{$0$}	(B)
            (B) edge[bend left, dashed, red]     node{$0$}	(A);
        \end{tikzpicture}
        &
        \begin{tikzpicture}[on grid, state/.style={circle,draw}, >= stealth', auto, prob/.style = {inner sep=1pt,font=\scriptsize}]
            \node[state, black!45!green,line width=0.4mm]  (A) {$1$};
            \node[state, black!45!green,line width=0.4mm]  (B) [below = 2cm of A]   {$2$};
            \node[state, black!45!green,line width=0.4mm]  (C) [below left = 1cm and 2cm of A]   {$3$};
            \path[->]
            (A) edge[bend left=75]     node{$0$}	(B)
            (A) edge[bend left, dashed, red]     node{$0$}	(B)
    	    (B) edge[bend left=75]     node{$1$}	(A)
    	    (C) edge[bend left, dashed, red]     node{$0$}	(A)
            (C) edge[bend right]     node[below]{$0$}	(B)
            (B) edge[bend left, dashed, red]     node{$0$}	(A);
        \end{tikzpicture}
        &
        \begin{tikzpicture}[on grid, state/.style={circle,draw}, >= stealth', auto, prob/.style = {inner sep=1pt,font=\scriptsize}]
            \node[state, RoyalBlue,line width=0.4mm]  (A) {$1$};
            \node[state, RoyalBlue,line width=0.4mm]  (B) [below = 2cm of A]   {$2$};
            \path[->]
            (A) edge[bend left=75]     node{$0$}	(B)
            (A) edge[bend left, dashed, red]     node{$0$}	(B)
    	    (B) edge[bend left=75]     node{$1$}	(A)
    	    (B) edge[bend left, dashed, red]     node{$0$}	(A);
        \end{tikzpicture}
        \\
            Arm $1^a$ & Arm $1^b$ & Arm $2$  \\
            \multicolumn{3}{c}{State transition is deterministic.}
        \end{tabular}
        \caption{
            Restless Markovian arms. Arm $1^a$ and Arm $1^b$ has 3 states and Arm $2$ has 2 states.
            The black arrows show state transition of action activate and the dashed red one for action rest.
            The numbers along the arrows show the expected reward when executing the actions.
            The span of bias in both arms are bounded.
            The two corresponding global MDPs is given in \figurename~\ref{fig:hard_global}.
        }
        \label{fig:hard_local}
    \end{figure}
    
    \begin{figure}[hp]
        \centering
        \begin{tabular}{cc}
        \begin{tikzpicture}[on grid, state/.style={circle,draw}, >= stealth', auto, prob/.style = {inner sep=1pt,font=\scriptsize}]
            \node[state]  (A) {$1,1$};
            \node[state]  (B) [above = 2cm of A]   {$2,2$};
            \node[state]  (C) [right = 4cm of B]   {$1,2$};
            \node[state]  (D) [below = 2cm of C]   {$2,1$};
            \node[state]  (E) [right = 2cm of B]   {$3,1$};
            \node[state]  (F) [below = 2cm of E]   {$3,2$};
            \path[->]
            (A) edge[line width=0.4mm, bend left, black!45!green]     node{$0$}	(B)
    	    (C) edge[line width=0.4mm, bend left, black!45!green]     node{$0$}	(D)
            (D) edge[line width=0.4mm, bend left=75, RoyalBlue] node{$0$} (C)
            (A) edge[line width=0.4mm, bend left=75, RoyalBlue]     node{$0$}	(B)
            (E) edge[line width=0.4mm, RoyalBlue] node[above]{$0$} (B)
            (F) edge[line width=0.4mm, black!45!green] node[below]{$0$} (A)
            %\draw[-{Stealth}{Stealth}]
    	    (B) edge[line width=0.4mm, bend left, black!45!green]     node{$1$}	(A)
            (D) edge[line width=0.4mm, bend left, black!45!green] node{$1$} (C)
    	    (B) edge[line width=0.4mm, bend left=75, RoyalBlue]     node{$1$}	(A)
    	    (C) edge[line width=0.4mm, bend left=75, RoyalBlue]     node{$1$}	(D)
            (E) edge[line width=0.4mm, black!45!green] node[above]{$0$} (C)
            (F) edge[line width=0.4mm, RoyalBlue] node[below]{$1$} (D);
        \end{tikzpicture}
        &
        \begin{tikzpicture}[on grid, state/.style={circle,draw}, >= stealth', auto, prob/.style = {inner sep=1pt,font=\scriptsize}]
            \node[state]  (A) {$1,1$};
            \node[state]  (B) [above = 2cm of A]   {$2,2$};
            \node[state]  (C) [right = 4cm of B]   {$1,2$};
            \node[state]  (D) [below = 2cm of C]   {$2,1$};
            \node[state]  (E) [right = 2cm of B]   {$3,1$};
            \node[state]  (F) [below = 2cm of E]   {$3,2$};
            \path[->]
            (A) edge[line width=0.4mm, bend left, black!45!green]     node{$0$}	(B)
    	    (C) edge[line width=0.4mm, bend left, black!45!green]     node{$0$}	(D)
            (D) edge[line width=0.4mm, bend left=75, RoyalBlue] node{$0$} (C)
            (A) edge[line width=0.4mm, bend left=75, RoyalBlue]     node{$0$}	(B)
            (E) edge[line width=0.4mm, black!45!green] node[above]{$0$} (B)
            (F) edge[line width=0.4mm, RoyalBlue] node[below]{$1$} (A)
            %\draw[-{Stealth}{Stealth}]
    	    (B) edge[line width=0.4mm, bend left, black!45!green]     node{$1$}	(A)
            (D) edge[line width=0.4mm, bend left, black!45!green] node{$1$} (C)
    	    (B) edge[line width=0.4mm, bend left=75, RoyalBlue]     node{$1$}	(A)
    	    (C) edge[line width=0.4mm, bend left=75, RoyalBlue]     node{$1$}	(D)
            (E) edge[line width=0.4mm, RoyalBlue] node[above]{$0$} (C)
            (F) edge[line width=0.4mm, black!45!green] node[below]{$0$} (D);
        \end{tikzpicture}
        \\
            $M^a:=\{\text{Arm }1^a, \text{Arm }2\}$ & $M^b:=\{\text{Arm }1^b, \text{Arm }2\}$ \\
            \multicolumn{2}{c}{State transition is deterministic.}
        \end{tabular}
        \caption{
            Two restless bandits, each with two arms given in \figurename~\ref{fig:hard_local}.
            Exactly one arm is activated at each decision time and the initial global state is either $(3,1)$ or $(3,2)$.
            The global state is denoted by $(s_1,s_2)$ where $s_1$ is the state of Arm $1$ and $s_2$ of Arm $2$.
            The green arrows show the state transition when activating Arm $1$.
            The blue arrows show the state transition when activating Arm $2$.
            The numbers along the arrows show the expected reward when executing the actions.
            The global MDP $M^a$ is formed by Arm $1^a$ and Arm $2$ and $M^b$ by Arm $1^b$ and Arm $2$.
            Both MDPs are multichain and not weakly communicating because state $(3,1)$ and $(3,2)$ are transient and there are two recurrent classes under any policy: $\gX^1:=\{(1,1), (2,2)\}$ and $\gX^2:=\{(1,2),(2,1)\}$.
            The optimal gain in class $\gX^1$ is $0.5$ and in class $\gX^2$ is $1$.
            This shows that the actions in states $(3,1)$ and $(3,2)$ are decisive.
            In $M^a$, the optimal action in state $(3,1)$ is to rest Arm $2$ and in state $(3,2)$ is to activate Arm $2$.
            In contrast, in $M^b$, the optimal action in state $(3,1)$ is to activate Arm $2$ and in state $(3,2)$ is to rest Arm $2$.
            Starting by either $(3,1)$ or $(3,2)$, no learning algorithms have a sublinear expected regret in both examples.
        }
        \label{fig:hard_global}
    \end{figure}

    \textbf{Proof of \ref{it:non_learnable2}} -- It is derived from \ref{it:non_learnable1} by slightly modifying the transition of state $1$ of Arm $1$ as the following: if Arm $1$ is activated in state $1$, it transitions to state $3$ with probability $\varepsilon$ or to state $2$ with probability $1-\varepsilon$.
    Then, the modified bandits $M^a(\varepsilon)$ and $M^b(\varepsilon)$ are both weakly communicating.
    Specifying a very small $\varepsilon$, for example, $\varepsilon=1/T^2$ and following the proof of \ref{it:non_learnable1} conclude the proof. 
\end{proof}

The difference between \ref{it:non_learnable1} and \ref{it:non_learnable2} of Theorem~\ref{thm:non_learnable} is that the bandits in \ref{it:non_learnable1} do not depend on $T$ while the ones in \ref{it:non_learnable2} do.
So, the result of Theorem~\ref{thm:non_learnable}~\ref{it:non_learnable1} is stronger.
Yet, the reason that \ref{it:non_learnable2} is proposed is that the bandits in \ref{it:non_learnable1} are not weakly communicating (see \figurename~\ref{ch:mdp:fig:mdp_class} for the MDP classification).
According to Definition~\ref{ch:rl:defn:rg_infinite}, the regret is not defined in MDPs that are not weakly communicating.
So, the regret in \ref{it:non_learnable1} should be understood as the difference between the cumulative reward of an optimal policy and the cumulative reward of the learner. 
For \ref{it:non_learnable2}, the bandits are weakly communicating and Definition~\ref{ch:rl:defn:rg_infinite} is applied.

Theorem~\ref{thm:non_learnable} shows us that no matter how good a learning algorithm is, there is a restless bandit, all of whose arms are unichain, that makes the algorithm suffer an expected regret linear in the total steps regardless of how many total steps the algorithm wants to learn.
This means that no RL algorithms can perform uniformly well over the general class of restless bandits all of whose arms are unichain.
We prove this theorem by constructing a bandit such that one of its arms admits a local transient state (the state $3$ of Arm $1$ in \figurename~\ref{fig:hard_local}).
The constructed bandit has then two global transient states (the states $(3,1)$ and $(3,2)$ in \figurename~\ref{fig:hard_global}).
In fact, if a restless bandit has $n$ arms, each arm has $S$ states, and there exists one arm that admits one local transient state, then the bandit has $S^{n-1}$ global transient states.
This inspires us to study the bandits all of whose arms have no local transient state.

\subsubsection{Local recurrence does not imply global recurrence}

The following theorem shows that a restless bandit, all of whose arms are ergodic or recurrent, is not necessarily unichain.

\begin{thm}[Multichain restless bandit]
    \label{thm:multichain}
    \begin{enumerate}[label=(\roman*)]
        \item[]
        \item \label{thm:not_ergodic} Among restless bandits all of whose arms are recurrent, there is a bandit that is not weakly communicating. % the term noncommunicating is taken from [Puterman, 1994, page 353]. 
        \item \label{thm:ergodic_arms_multichain_RB} Among restless bandits all of whose arms are ergodic, there is a bandit that is multichain.
    \end{enumerate}
\end{thm}

\begin{proof}
    \textbf{Proof of \ref{thm:not_ergodic}} -- It is given by the counter-example in which the restless bandit has two arms as given by \figurename~\ref{fig:recur_non_communicate} and exactly one arm is activated at each decision time.
    For each arm, both ``activate'' and ``rest'' actions induce the same state transition.
    In such example, the restless bandit is a multichain and not weakly communicating MDP (see \figurename~\ref{ch:mdp:fig:mdp_class} for MDP classification) because there are two recurrent classes under any policy: one composed of $\{(1,1), (2,2)\}$ and the other one is $\{(1,2), (2,1)\}$.
    \begin{figure}[htbp]
        \centering
        \begin{tabular}{ccc}
        \begin{tikzpicture}[on grid, state/.style={circle,draw}, >= stealth', auto, prob/.style = {inner sep=1pt,font=\scriptsize}]
            \node[state, black!45!green,line width=0.4mm]  (A) {$1$};
            \node[state, black!45!green,line width=0.4mm]  (B) [below = 2cm of A]   {$2$};
            \path[->]
            (A) edge[bend left=75]     node{$0$}	(B)
            (A) edge[bend left, dashed, red]     node{$0$}	(B)
    	    (B) edge[bend left=75]     node{$1$}	(A)
            (B) edge[bend left, dashed, red]     node{$0$}	(A);
        \end{tikzpicture}
        &
        \begin{tikzpicture}[on grid, state/.style={circle,draw}, >= stealth', auto, prob/.style = {inner sep=1pt,font=\scriptsize}]
            \node[state, RoyalBlue,line width=0.4mm]  (A) {$1$};
            \node[state, RoyalBlue,line width=0.4mm]  (B) [below = 2cm of A]   {$2$};
            \path[->]
            (A) edge[bend left=75]     node{$0$}	(B)
            (A) edge[bend left, dashed, red]     node{$0$}	(B)
    	    (B) edge[bend left=75]     node{$1$}	(A)
    	    (B) edge[bend left, dashed, red]     node{$0$}	(A);
        \end{tikzpicture}
        &
        \begin{tikzpicture}[on grid, state/.style={circle,draw}, >= stealth', auto, prob/.style = {inner sep=1pt,font=\scriptsize}]
                \node[state]  (A) {$1,1$};
                \node[state]  (B) [right = 1.5cm of A]   {$1,2$};
                \node[state]  (C) [right = 2cm of B]   {$2,1$};
                \node[state]  (D) [left = 2cm of A]   {$2,2$};
                \path[->]
                (A) edge[bend right, black!45!green, line width=0.4mm]     node[above]{$0$}	(D)
                (A) edge[bend right=80, RoyalBlue, line width=0.4mm]     node[above]{$0$}	(D)
                (B) edge[bend right, black!45!green, line width=0.4mm]     node[below]{$0$}	(C)
                (B) edge[bend right=80, RoyalBlue, line width=0.4mm]     node[below]{$0$}	(C)
                (C) edge[bend right, black!45!green, line width=0.4mm]     node[above]{$0$}	(B)
                (C) edge[bend right=80, RoyalBlue, line width=0.4mm]     node[above]{$0$}	(B)
                % transition of optimal actions
                (D) edge[bend right, black!45!green, line width=0.4mm] node[below]{$1$} (A)
                (D) edge[bend right=80, RoyalBlue, line width=0.4mm] node[below]{$1$} (A);
        \end{tikzpicture}
        \\
            Arm $1$ & Arm $2$ & Global MDP having four states. \\
            \multicolumn{3}{c}{State transition is deterministic.}
        \end{tabular}
        \caption{
            A restless bandit with two arms and exactly one arm is activated at each decision time.
            %Arm $1$ is given and Arm $2$ is identical to Arm $1$ and given in \figurename~\ref{fig:hard_local}.
            The global state is denoted by $(s_1,s_2)$ where $s_1$ is the state of Arm $1$ and $s_2$ of Arm $2$.
            The green arrows show the state transition when activating Arm $1$.
            The blue arrows show the state transition when activating Arm $2$.
            The numbers along the green and blue arrows show the expected reward when executing the actions.\\
            For both arms, the black arrows show state transition of action activate and the dashed red ones for action rest.
            The numbers along the black and dashed red arrows show the expected reward when executing the actions.
        }
        \label{fig:recur_non_communicate}
    \end{figure}
    \medskip \\
    \textbf{Proof of \ref{thm:ergodic_arms_multichain_RB}} -- It is given by the following counter-example.
    Consider a restless bandit with two 8-state arms presented in \figurename~\ref{fig:ergodic_arm}.
    Both arms are identical in terms of state transition that is summarized in Table~\ref{tab:ergodic_arm}.
    We observe that when rest, state $1$ and $2$ have the same possible next states $2$ and $3$.
    Similarly, the pair $\{3,4\}$ has $\{4,5\}$, $\{5,6\}$ has $\{6,7\}$, and $\{7,8\}$ has $\{8,1\}$.
    Similar pattern is observed under action activate: $\{2,3\}$ has $\{3,4\}$, $\{4,5\}$ has $\{5,6\}$, $\{6,7\}$ has $\{7,8\}$, and $\{8,1\}$ has $\{1,2\}$.
    This means that we can construct a cycle of transition by alternating between rest (R) and activate (A):
    \begin{align*}
        \{1,2\} \overset{R}{\to} \{2,3\} \overset{A}{\to} \{3,4\} \overset{R}{\to} \{4,5\} \overset{A}{\to} \{5,6\} \overset{R}{\to} \{6,7\} \overset{A}{\to} \{7,8\} \overset{R}{\to} \{8,1\} \overset{A}{\to} \{1,2\}.
    \end{align*}
    With a proper synchronization between the states of Arm $1$ and Arm $2$, we can construct policies that induce two recurrent classes as presented in \figurename~\ref{fig:local_ergodic_multichain_RB}.
    Indeed, consider the following arrangement.
    \begin{align}
        &\text{Arm 1} : \{1,2\} \overset{R}{\to} \{2,3\} \overset{A}{\to} \{3,4\} \overset{R}{\to} \dots \overset{A}{\to} \{1,2\} \nonumber \\
        &\text{Arm 2} : \{4,5\} \overset{A}{\to} \{5,6\} \overset{R}{\to} \{6,7\} \overset{A}{\to} \dots \overset{R}{\to} \{4,5\} \nonumber \\
        &\text{Bandit} : \begin{pmatrix}1,5\\2,4\\2,5\end{pmatrix} \overset{2}{\to} \begin{pmatrix}2,6\\3,5\\3,6\end{pmatrix} \overset{1}{\to} \begin{pmatrix}3,7\\4,6\\4,7\end{pmatrix} \overset{2}{\to} \dots \overset{1}{\to} \begin{pmatrix}1,5\\2,4\\2,5\end{pmatrix} \label{eq:mdp_transition}
    \end{align}
    where the symbol $\overset{i}{\to}$ in \Eqref{eq:mdp_transition} means the transition when Arm $i$ is activated.
    It means that any policies that activate Arm $2$ for any odd $s\in[8]$ and rest Arm $2$ for any even $s\in[8]$ when the bandit is in state $(s,s+4), (s+1,s+3)$, and $(s+1,s+4)$ induce a recurrent class $\gX^1$ as given in \figurename~\ref{fig:local_ergodic_multichain_RB} (note that for $s$ big enough, state $9$ is state $1$, $10$ is $2$, etc.).
    The recurrent class $\gX^2$ in \figurename~\ref{fig:local_ergodic_multichain_RB} is constructed by simply changing the sequence of Arm 2's state in the arrangement above and adapting the state of the bandit accordingly:

    \begin{align*}
        &\text{Arm 1} : \{1,2\} \overset{R}{\to} \{2,3\} \overset{A}{\to} \{3,4\} \overset{R}{\to} \dots \overset{A}{\to} \{1,2\} \\
        &\text{Arm 2} : \{8,1\} \overset{A}{\to} \{1,2\} \overset{R}{\to} \{2,3\} \overset{A}{\to} \dots \overset{R}{\to} \{8,1\} \\
        &\text{Bandit} : \begin{pmatrix}1,1\\2,8\\2,1\end{pmatrix} \overset{2}{\to} \begin{pmatrix}2,2\\3,1\\3,2\end{pmatrix} \overset{1}{\to} \begin{pmatrix}3,3\\4,2\\4,3\end{pmatrix} \overset{2}{\to} \dots \overset{1}{\to} \begin{pmatrix}1,1\\2,8\\2,1\end{pmatrix}
    \end{align*}

    So, any policies that activate Arm $2$ for any odd $s\in[8]$ and rest Arm $2$ for any even $s\in[8]$ when the bandit is in state $(s,s), (s+1,s+7)$, and $(s+1,s)$ induce the class $\gX^2$.
    All in all, any policies that activate Arm $2$ for any odd $s\in[8]$ and rest Arm $2$ for any even $s\in[8]$ when the bandit is in state $(s,s), (s+1,s+7), (s+1,s), (s,s+4), (s+1,s+3)$ and $(s+1,s+4)$ induce two recurrent classes $\gX^1$ and $\gX^2$.
    That concludes the proof.
    \begin{table}[ht]
        \centering
        \begin{tabular}{|c|c|c|}
            \hline
            Current state & Next state when rest & Next state when activate \\\hline 
            $1$   & $2$ or $3$  &  $1$ or $2$ \\
            $2$   & $2$ or $3$  &  $3$ or $4$ \\
            $3$   & $4$ or $5$  &  $3$ or $4$ \\
            $4$   & $4$ or $5$  &  $5$ or $6$ \\
            $5$   & $6$ or $7$  &  $5$ or $6$ \\
            $6$   & $6$ or $7$  &  $7$ or $8$ \\
            $7$   & $8$ or $1$  &  $7$ or $8$ \\
            $8$   & $8$ or $1$  &  $1$ or $2$ \\ \hline
        \end{tabular}
        \caption{State transition of arms in \figurename~\ref{fig:ergodic_arm}. Each transition happens with probability $0.5$}
        \label{tab:ergodic_arm}
    \end{table}

    %in \figurename~\ref{fig:local_ergodic_multichain_RB}.

\end{proof}

Theorem~\ref{thm:multichain} means that even though all arms are ergodic, the corresponding bandit is not necessarily ergodic and its mixing time can be infinite.
This result is ``not very intuitive'', and it is very important because assuming that all arms are ergodic is a popular condition in learning restless bandits (see \eg, \cite{ortner2012regret, jung2019thompson}).

%Theorem~\ref{thm:multichain} shows that a restless bandit all whose arms are ergodic can have an infinite mixing time.
%This is because the bandit itself is multichain.
To go further, the following theorem will show that even with an additional assumption that the bandit itself is also ergodic, the global mixing time can still be as big as we want.
That is, we show that the MDP properties of bandit such as the diameter, the mixing time, and the span of global bias function can be arbitrarily large.

\begin{figure}[ht]
    \centering
        \begin{tabular}{cc}
        \begin{tikzpicture}[on grid, state/.style={ellipse,draw}, >= stealth', auto, prob/.style = {inner sep=1pt,font=\scriptsize}]
            \node[state, black!45!green, line width=0.4mm]  (A) {$8$};
            \node[state, black!45!green, line width=0.4mm]  (B) [right = 1.6cm of A]    {$1$};
            \node[state, black!45!green, line width=0.4mm]  (C) [right = 1.6cm of B]    {$2$};
            \node[state, black!45!green, line width=0.4mm]  (D) [below = 1.6cm of C]    {$3$};
            \node[state, black!45!green, line width=0.4mm]  (E) [below = 1.6cm of D]    {$4$};
            \node[state, black!45!green, line width=0.4mm]  (F) [left = 1.6cm of E]    {$5$};
            \node[state, black!45!green, line width=0.4mm]  (G) [left = 1.6cm of F]    {$6$};
            \node[state, black!45!green, line width=0.4mm]  (H) [below = 1.6cm of A]    {$7$};
            \path[->]
                (A) edge[red, dashed, bend right=30] node{} (B)
                edge[red, dashed, loop above] node{} (A)
                (B) edge[red, dashed, bend right=30] node{} (C)
                edge[red, dashed, bend right=30] node{} (D)
                (C) edge[red, dashed, bend right=30] node{} (D)
                edge[red, dashed, loop above] node{} (C)
                (D) edge[red, dashed, bend right=30] node{} (E)
                edge[red, dashed, bend right=30] node{} (F)
                (E) edge[red, dashed, bend right=30] node{} (F)
                edge[red, dashed, loop below] node{} (E)
                (F) edge[red, dashed, bend right=30] node{} (G)
                edge[red, dashed, bend right=30] node{} (H)
                (G) edge[red, dashed, bend right=30] node{} (H)
                edge[red, dashed, loop below] node{} (G)
                (H) edge[red, dashed, bend right=30] node{} (A)
                edge[red, dashed, bend right=30] node{} (B)

                (A) edge[bend left=30]     node{}	(B)
                edge[bend left=50]     node{}	(C)
                (B) edge[loop above] node{} (B)
                edge[bend left=30]     node{}	(C)
                (C) edge[bend left=30]     node{}	(D)
                edge[bend left=50]     node{}	(E)
                (D) edge[loop right] node{} (D)
                edge[bend left=30]     node{}	(E)
                (E) edge[bend left=30]     node{}	(F)
                edge[bend left=50]     node{}	(G)
                (F) edge[loop below] node{} (F)
                edge[bend left=30]     node{}	(G)
                (G) edge[bend left=30]     node{}	(H)
                edge[bend left=50]     node{}	(A)
                (H) edge[loop left] node{} (H)
                edge[bend left=30]     node{}	(A);
        \end{tikzpicture} 
        &
        \begin{tikzpicture}[on grid, state/.style={ellipse,draw}, >= stealth', auto, prob/.style = {inner sep=1pt,font=\scriptsize}]
            \node[state, RoyalBlue,line width=0.4mm]  (A) {$8$};
            \node[state, RoyalBlue,line width=0.4mm]  (B) [right = 1.6cm of A]    {$1$};
            \node[state, RoyalBlue,line width=0.4mm]  (C) [right = 1.6cm of B]    {$2$};
            \node[state, RoyalBlue,line width=0.4mm]  (D) [below = 1.6cm of C]    {$3$};
            \node[state, RoyalBlue,line width=0.4mm]  (E) [below = 1.6cm of D]    {$4$};
            \node[state, RoyalBlue,line width=0.4mm]  (F) [left = 1.6cm of E]    {$5$};
            \node[state, RoyalBlue,line width=0.4mm]  (G) [left = 1.6cm of F]    {$6$};
            \node[state, RoyalBlue,line width=0.4mm]  (H) [below = 1.6cm of A]    {$7$};
            \path[->]
                (A) edge[red, dashed, bend right=30] node{} (B)
                edge[red, dashed, loop above] node{} (A)
                (B) edge[red, dashed, bend right=30] node{} (C)
                edge[red, dashed, bend right=30] node{} (D)
                (C) edge[red, dashed, bend right=30] node{} (D)
                edge[red, dashed, loop above] node{} (C)
                (D) edge[red, dashed, bend right=30] node{} (E)
                edge[red, dashed, bend right=30] node{} (F)
                (E) edge[red, dashed, bend right=30] node{} (F)
                edge[red, dashed, loop below] node{} (E)
                (F) edge[red, dashed, bend right=30] node{} (G)
                edge[red, dashed, bend right=30] node{} (H)
                (G) edge[red, dashed, bend right=30] node{} (H)
                edge[red, dashed, loop below] node{} (G)
                (H) edge[red, dashed, bend right=30] node{} (A)
                edge[red, dashed, bend right=30] node{} (B)

                (A) edge[bend left=30]     node{}	(B)
                edge[bend left=50]     node{}	(C)
                (B) edge[loop above] node{} (B)
                edge[bend left=30]     node{}	(C)
                (C) edge[bend left=30]     node{}	(D)
                edge[bend left=50]     node{}	(E)
                (D) edge[loop right] node{} (D)
                edge[bend left=30]     node{}	(E)
                (E) edge[bend left=30]     node{}	(F)
                edge[bend left=50]     node{}	(G)
                (F) edge[loop below] node{} (F)
                edge[bend left=30]     node{}	(G)
                (G) edge[bend left=30]     node{}	(H)
                edge[bend left=50]     node{}	(A)
                (H) edge[loop left] node{} (H)
                edge[bend left=30]     node{}	(A);
            \end{tikzpicture} \\
            {\small Arm $1$} & {\small Arm $2$} \\
            \multicolumn{2}{c}{\small Each state transition happens with probability $0.5$.}
        \end{tabular}
    \caption{
        Two restless Markovian arms, each having $8$ states.
        Both arms are identical in terms of transition structure.
        The black arrows show state transition of action activate and the dashed red ones for action rest.
        To ease the exposition, the expected reward is not shown because it is not relevant to the analysis.
        Also, each state transition happens with probability $0.5$.
        We provide a list of transition in Table~\ref{tab:ergodic_arm}.
        The restless bandit having these two arms and exactly one arm is activated at each decision time is multichain because some policies induce two classes of recurrent states as shown in \figurename~\ref{fig:local_ergodic_multichain_RB}.
    }
    \label{fig:ergodic_arm}
\end{figure}

\begin{figure}
    \centering
    \begin{tabular}{c}
    \begin{tikzpicture}[on grid, state/.style={ellipse,draw}, >= stealth', auto, prob/.style = {inner sep=1pt,font=\normalsize}]
        \node[state, inner sep=1pt]  (A) {\scriptsize$\begin{tabular}{c}1,5\\2,4\\2,5\end{tabular}$};
        \node[state, inner sep=1pt]  (B) [right = 2cm of A]    {\scriptsize$\begin{tabular}{c}2,6\\3,5\\3,6\end{tabular}$};
        \node[state, inner sep=1pt]  (C) [right = 2cm of B]    {\scriptsize$\begin{tabular}{c}3,7\\4,6\\4,7\end{tabular}$};
        \node[state, inner sep=1pt]  (D) [right = 2cm of C]    {\scriptsize$\begin{tabular}{c}4,8\\5,7\\5,8\end{tabular}$};
        \node[state, inner sep=1pt]  (E) [below = 2cm of D]    {\scriptsize$\begin{tabular}{c}5,1\\6,8\\6,1\end{tabular}$};
        \node[state, inner sep=1pt]  (F) [left = 2cm of E]    {\scriptsize$\begin{tabular}{c}6,2\\7,1\\7,2\end{tabular}$};
        \node[state, inner sep=1pt]  (G) [left = 2cm of F]    {\scriptsize$\begin{tabular}{c}7,3\\8,2\\8,3\end{tabular}$};
        \node[state, inner sep=1pt]  (H) [left = 2cm of G]    {\scriptsize$\begin{tabular}{c}8,4\\1,3\\1,4\end{tabular}$};
        \node[text width=2cm] (J) [left = 1.5cm of A] {\footnotesize Recurrent\\Class $\gX^1$};
        \path[->]
        (A) edge[RoyalBlue,line width=0.4mm]     node{}	(B)
        (B) edge[black!30!green,line width=0.4mm]     node{}	(C)
        (C) edge[RoyalBlue,line width=0.4mm]     node{}	(D)
        (D) edge[black!30!green,line width=0.4mm]     node{}	(E)
	    (E) edge[RoyalBlue,line width=0.4mm]     node{}	(F)
	    (F) edge[black!30!green,line width=0.4mm]     node{}	(G)
        (G) edge[RoyalBlue,line width=0.4mm]     node{}	(H)
        (H) edge[black!30!green,line width=0.4mm]     node{}	(A);
    \end{tikzpicture} \\ \hfill\\
    \begin{tikzpicture}[on grid, state/.style={ellipse,draw}, >= stealth', auto, prob/.style = {inner sep=1pt,font=\scriptsize}]
        \node[state, inner sep=1pt]  (A) {\scriptsize$\begin{tabular}{c}1,1\\2,8\\2,1\end{tabular}$};
        \node[state, inner sep=1pt]  (B) [right = 2cm of A]    {\scriptsize$\begin{tabular}{c}2,2\\3,1\\3,2\end{tabular}$};
        \node[state, inner sep=1pt]  (C) [right = 2cm of B]    {\scriptsize$\begin{tabular}{c}3,3\\4,2\\4,3\end{tabular}$};
        \node[state, inner sep=1pt]  (D) [right = 2cm of C]    {\scriptsize$\begin{tabular}{c}4,4\\5,3\\5,4\end{tabular}$};
        \node[state, inner sep=1pt]  (E) [below = 2cm of D]    {\scriptsize$\begin{tabular}{c}5,5\\6,4\\6,3\end{tabular}$};
        \node[state, inner sep=1pt]  (F) [left = 2cm of E]    {\scriptsize$\begin{tabular}{c}6,6\\7,5\\7,4\end{tabular}$};
        \node[state, inner sep=1pt]  (G) [left = 2cm of F]    {\scriptsize$\begin{tabular}{c}7,7\\8,6\\8,4\end{tabular}$};
        \node[state, inner sep=1pt]  (H) [left = 2cm of G]    {\scriptsize$\begin{tabular}{c}8,8\\1,7\\1,5\end{tabular}$};
        \node[text width=2cm] (J) [left = 1.5cm of A] {\footnotesize Recurrent\\Class $\gX^2$};
        \path[->]
        (A) edge[RoyalBlue,line width=0.4mm]     node{}	(B)
        (C) edge[RoyalBlue,line width=0.4mm]     node{}	(D)
	    (E) edge[RoyalBlue,line width=0.4mm]     node{}	(F)
        (G) edge[RoyalBlue,line width=0.4mm]     node{}	(H)
        (B) edge[black!30!green,line width=0.4mm]     node{}	(C)
        (D) edge[black!30!green,line width=0.4mm]     node{}	(E)
	    (F) edge[black!30!green,line width=0.4mm]     node{}	(G)
        (H) edge[black!30!green,line width=0.4mm]     node{}	(A);
    \end{tikzpicture} \\
    \end{tabular}
    \caption{
        State transition within two recurrent classes of a restless bandit with 2 arms presented in \figurename~\ref{fig:ergodic_arm}.
        The first recurrent class is $\gX^1 :=\{(s,s{+}4),(s{+}1,s{+}3),(s{+}1,s{+}4): s\in[8]\}$ and the second one is $\gX^2 :=\{(s,s),(s{+}1,s),(s{+}1,s{+}7): s\in[8]\}$.
        These are two recurrent classes under any policies that activate Arm 2 for any odd $s\in[8]$ and rest Arm 2 for any even $s\in[8]$ when the bandit is in states $(s,s),(s,s+4),(s+1,s),(s+1,s+3),(s+1,s+4),(s+1,s+7)$.
        The blue arrows show the state transitions when Arm $2$ is activated and the green ones show the one when Arm $1$ is activated.
        Each ellipse regroups the states having the same state transition under the same action.
        Note that each ellipse can transition to itself like in $\gX^1$ state $(2,4)$ can transition to state $(2,5)$, but it is not shown.
        The rest of the states $\gX\setminus(\gX^1\cup\gX^2)$ are transient.
    }
    \label{fig:local_ergodic_multichain_RB}
\end{figure}

%\medskip

\begin{thm}[MDP properties of restless bandit]
    \label{thm:parameters}
    \begin{enumerate}[label=(\roman*)]
        \item[]
        \item \label{thm:diam} There is a restless bandit having $n$ 2-state arms, all of whom have a diameter of size $2$, that is ergodic and has a diameter of size $2^n$. % the term noncommunicating is taken from [Puterman, 1994, page 353]. 
        %\item \label{thm:aperiodic_RB_ergo} There exists a Restless bandit whose arms are all ergodic that is not recurrent. 
        \item \label{thm:mixing} There is a constant $c>1$ such that for each constant $C>1$, there is a restless bandit, all of whose arms are ergodic and have the local mixing time smaller than $c$, that is ergodic and has the global mixing time larger than $C$.
        \item \label{thm:span} There is a constant $c>0$ such that for each constant $C>0$, there is a restless bandit, all of whose arms have a span of local bias function under any policy upper bounded by $c$, that is communicating and has a span of global bias function lower bounded by $C$.
    \end{enumerate}
\end{thm}
\begin{proof}
    \textbf{Proof of \ref{thm:diam}} -- consider an example of $n$ arms, each arm is an MDP with state space $\{1,2\}$ and has $0.5$ probability of changing state under both actions.
    It should be clear that the bandit is ergodic.
    Indeed, any state of bandit can be reached from any other global states with probability $(1/2)^n$.
    Hence, the diameter of the bandit is $2^n$.
    \medskip \\

    \textbf{Proof of \ref{thm:mixing}} -- Let $p_1$ and $p_2$ be the state transition functions of Arm $1$ and Arm $2$ given in \figurename~\ref{fig:ergodic_arm} respectively.
    $p_1$ and $p_2$ are both ergodic and induce the same state transition that is given in Table~\ref{tab:ergodic_arm}.
    Suppose that the local mixing time of $p_1$ and $p_2$ is $t_{mix}>1$.
    Note that we can specify $p_1$ and $p_2$ such that $t_{mix}$ is small.
    Let's denote the bandit having two arms $p_1$ and $p_2$ by $M$.
    By Theorem~\ref{thm:multichain}, $M$ is multichain.
    So, by Definition~\ref{ch:restless:mixing}, the mixing time of $M$ is infinite.

    Now, consider two 8-state restless arms having transition functions $u_1$ and $u_2$ where $u_1$ and $u_2$ induce the same state transition: any local state $s\in[8]$ transitions to any other local state $s'\in[8]$ with probability $\frac18$ under either action rest or activate.
    It is then clear that $u_1$ and $u_2$ are both ergodic and have a local mixing time of size $1$.
    Any bandit having two arms $u_1$ and $u_2$ is an ergodic MDP.

    For any $\varepsilon\in(0,1)$, consider two 8-state restless arms having transition function $p'_1=(1-\varepsilon)p_1+\varepsilon u_1$ and $p'_2=(1-\varepsilon)p_2+\varepsilon u_2$.
    That is, at each state transition, a two-face coin whose head probability is $\varepsilon$ is tossed.
    If the coin heads up, the arm evolves like $u$.
    If the coin tails up, the arm evolves like $p$.
    So, $p'_1$ and $p'_2$ are both ergodic.
    We know that if $\varepsilon=0$, the local mixing time of $p'_1$ and $p'_2$ is $t_{mix}$ and if $\varepsilon=1$, the mixing time is $1$.
    Then, there exists $\varepsilon_0>0$ such that the local mixing time of $p'_1$ and $p'_2$ is $t_{mix}/2$.
    For any $\varepsilon\in(0,\varepsilon_0]$, any bandit having exactly two arms $p'_1$ and $p'_2$ is an ergodic MDP denoted by $M'(\varepsilon)$.
    The global mixing time of $M'(\varepsilon)$ is then defined.
    However, when $\varepsilon\to0$, $M'(\varepsilon)\to M$.
    This is equivalent to say that when $\varepsilon\to0$, the mixing time of $M'(\varepsilon)$ tends to infinity.
    \medskip \\

    \textbf{Proof of \ref{thm:span}} -- We use the same technique in the proof of \ref{thm:mixing}.
    First, consider the bandit having $4$ global states as in Figure~\ref{fig:recur_non_communicate}.
    We denote this bandit by $M$ and the arms' transition functions by $p_1$ and $p_2$ and reward functions by $r_1$ and $r_2$.
    It should be clear that for each arm, the span of local bias under any policy is bounded in $[0,1]$, but the bandit $M$ is multichain.
    So, the span of global bias function is infinite.

    Consider now two 2-state arms with reward functions $r_1$ and $r_2$, and transition functions $u_1$ and $u_2$ that induce the same state transition: any local state $s\in[2]$ transitions to any other local state $s'\in[2]$ with probability $\frac12$.
    So, $u_1$ and $u_2$ are both ergodic and the span of local bias function is bounded in $[0,1]$.

    With the same technique above, for any $\varepsilon\in(0,1)$, consider two 2-state arms having reward functions $r_1$ and $r_2$, and transition functions $p'_1=(1-\varepsilon)p_1+\varepsilon u_1$ and $p'_2=(1-\varepsilon)p_2+\varepsilon u_2$.
    So, for any $\varepsilon\in(0,1)$, $p'_1$ and $p'_2$ are both ergodic and have the span of local bias function bounded in $[0,1]$.
    We denote any bandit having exactly two arms $(r_1,p'_1)$ and $(r_2,p'_2)$ by $M'(\varepsilon)$.
    We will see in Theorem~\ref{thm:aperiodic_RB_comm} that for any $\varepsilon\in(0,1)$, $M'(\varepsilon)$ is a communicating MDP.
    However, when $\varepsilon\to0$, $M'(\varepsilon)\to M$.
    This is equivalent to say that when $\varepsilon\to0$, the span of global bias function of $M'(\varepsilon)$ tends to infinity.
\end{proof}

With Theorem~\ref{thm:parameters}, the curse of dimensionality in restless bandits manifests not only in the state size but also in the MDP properties of the bandit.
Hence, it is equally important to take into account the dependency between the number of arms and the MDP properties of the restless bandits when deriving the regret bound of RL algorithms.

\subsection{Positive results}

In this section, we show a few results in which the assumption on local arms implies ``desirable'' properties on the bandit.
The term ``desirable'' refers to the fact that the properties frequently required by the general-purpose RL algorithms are satisfied.
This term does not imply that the learning algorithm has a regret bound sublinear in the total steps nor that its regret bound is explicitly polynomially in the number of arms.

\begin{thm}[Communicating restless bandit]
    \label{thm:aperiodic_RB_comm} A restless bandit all of whose arms are ergodic is a communicating MDP.
\end{thm}
\begin{proof}
    We prove the theorem by its contraposition. Assume that a given bandit is not communicating (either weakly communicating or not weakly communicating).
    Then, in this bandit, there are global states that are not reachable from each other.
    This implies two possibilities: (1) for some arms, some local states are not recurrent, (2) all arms are recurrent but periodic.
    Each possibility implies that there exists at least one arm that is not ergodic.
\end{proof}
This theorem means that if all arms are ergodic, then the bandit has a finite diameter.
The following theorem also provides a positive result on the structure of bandit.

\begin{thm}[Unichain restless bandit]
    \label{thm:unichain}
    A restless bandit all of whose arms are unichain with at least one local state reachable in one step from any other local states under both rest and activate actions is unichain with at least one global state reachable in one step from any other global states under any policies.
    %For restless bandits with a finite number of arms, if all arms are unichain with at least one local state reachable in one step from any other local states under both rest and activate actions, then the corresponding bandit is also unichain with at least one global state reachable in one step from any other global states under any policies.
\end{thm}
\begin{proof}
    Consider the bandit $M$ described in Section~\ref{ch:restless:sec:restless}.
    For each arm $i\in[n]$, let $z_i$ be the state that is reachable in one time step from any other states under both rest and activate actions.
    We have that $p(z_i \mid s_i, a_i)>0$ for all $i\in[n], x_i\in\gS_i,a_i\in\{0,1\}$.
    For any global action $\va\in\gA(m)$, any global states $\vs,\vs'\in\gX,$ the bandit $M$'s state transition is given by ${p(\vs' \mid \vs, \va) =\prod_{i=1}^n p(s'_i \mid s_i, a_i)}$.
    Then, for any action $\va\in\gA(m)$, any state $\vs\in\gX$, we have
    \begin{align*}
        p(\vz \mid \vs, \va) =\prod_{i=1}^n p(z_i \mid s_i,a_i) > 0,
    \end{align*}
    because $n$ is finite and for any $i$, $p(z_i \mid s_i, a_i)>0$.
    Hence, the global state $\vz$ is reachable from any other global states in one step under any policies.
    In consequence, no policies induce a global Markov chain that has multiple closed irreducible recurrent classes.
    That concludes the proof.
\end{proof}

Theorem~\ref{thm:unichain} means that if the ergodicity coefficient of all arms is strictly smaller than $1$, then the ergodicity coefficient of corresponding bandit is also strictly smaller than $1$.
As we mentioned above, the ergodicity coefficient is an MDP parameter that provides an upper bound on the mixing time and the span of bias function of the MDP.
We will clearly see this idea when we discuss the regret analysis of RB-TSDE \cite{akbarzadeh2022learning} with precisely definition of the ergodicity coefficient (see \Eqref{eq:ergodic_M}) in the sections below.

%\subsection{Regret definition and required assumptions}
\section{Learning algorithms for restless bandits}
\label{ch:restless:sec:generic}

\subsection{Discussion on using Whittle index policy as the baseline in regret definition}
\label{ch:restless:discuss_policy}

%Restless bandits are the MDPs with the curse of dimensionality.
In Chapter~\ref{ch:learning_rested}, we use Gittins index policy as the baseline policy for evaluating the regret of learning algorithms in discounted rested bandit because this index policy is optimal.
Yet, Whittle index policy is only asymptotically optimal under some technical conditions \cite{weber1990index} and suboptimal in general.
This brings two options to the discussion.
\begin{enumerate}[label=(\alph*)]
    \item Shall we compare the algorithm to an oracle that knows an optimal policy of any restless bandit?
    \item \label{it:second_baseline} Shall we use Whittle index policy as a baseline policy knowing that it is suboptimal in general?
\end{enumerate}
%We will then focus the discussion on the second option.

Assuming an oracle is what is done in \cite{ortner2012regret, jung2019thompson, wang2020restless} when learning in restless bandits and in \cite{osband2014near, rosenberg2020oracle, xu2020reinforcement} when learning in factored-MDPs.
The authors then solely interest in the statistical aspect of the learning algorithms.

We now focus on the second option \ref{it:second_baseline}.
First, if the bandit is indexable, then Whittle index policy is defined.
Also, regret definition requires that the gain of baseline policy is state-independent.
%since Whittle index policy is not optimal, the gain under this policy can be.
So, a possible assumption to use Whittle index policy as the baseline policy is that the unknown bandit is indexable and Whittle index policy is unichain, \ie, the Markov chain induced by Whittle index policy has a single recurrent class.
%Regarding the computational complexity, from Chapter~\ref{ch:index_computation}, checking the indexability of bandit $M$ costs only $\landauO(nS^3)$ but checking if the global Markov chain is unichain costs $\landauO(S^{2n})$ based on Tarjan's strongly connected component algorithm.
%So, the assumption on the global structure of bandit $M$ is computationally hard to verify once again due the global state size.
This discussion also extends to the learning approaches.
For OFU methods, we have shown in Chapter~\ref{ch:learning_rested} that optimistic algorithms that apply confidence bonus on arm's transition cannot leverage index policy to choose an imagined bandit $M^k$ with index policy $\pi^k$ that guarantees the optimism at episode $k$.
Worse yet, there is no guarantee that the optimistic restless bandit $M^k$ is indexable, let alone the optimism.
%So, an assumption that there exists an oracle that knows how to choose $M^k$ that is indexable
%So, we believe that using the Whittle index policy as a baseline is not computationally practical for the optimism.
For posterior sampling, we need the imagined bandit $M^k$ to be indexable and the corresponding Whittle index policy $\pi^k$ to induce state-independent gain.
This can be done by making an assumption on the support of the prior distribution.
That is, any bandit drawn from the support of the prior distribution is indexable and admits unichain Whittle index policy.
We will see in the following that this is what is done in the work of \cite{akbarzadeh2022learning}.

\subsection{Algorithms with regret guarantee}

Similarly to what we have seen in Chapter~\ref{ch:learning_rested}, the current best general-purpose RL algorithms have a the regret bound $\tilde{\landauO}\Bigl(\sqrt{HS^n{n \choose m}T}\Bigr)$ in the restless bandit $M$ described in Section~\ref{ch:restless:sec:restless} where $H$ is the upper bound on the span of the global optimal bias function, and $S^n$ and ${n \choose m}$ are the state and action sizes of the bandit $M$ respectively.
There are two issues in this regret bound.
\begin{enumerate}[label=(\alph*)]
    \item \label{it:state_exponential} One noticeable problem is the term $S^n$ that is the state size of $M$. This problem is resolved in the work of \cite{ortner2012regret, jung2019thompson, akbarzadeh2022learning}.
    \item \label{it:diameter_exponential} The other problem is the implicit dependency between the upper bound $H$ and the number of arms $n$. As we have seen in Theorem~\ref{thm:parameters}, it is perfectly possible that a bandit has the span of global bias function as big as we want.
\end{enumerate}

%To make the discussion more precise, we provide a sketch of proof of RB-TSDE \cite{akbarzadeh2022learning} in the following.
%To make the discussion more precise, in the following, we provide an overview of regret analysis of algorithms that are UCRL2-like or TSDE-like and talk about the techniques of \cite{ortner2012regret, jung2019thompson, akbarzadeh2022learning} to solve the issue \ref{it:state_exponential}.
%The readers who are convinced by the two issues \ref{it:state_exponential} and \ref{it:diameter_exponential} can skip the following and go to the conclusion of the chapter.

In the following, we provide a sketch of proof of the regret bound of RB-TSDE \cite{akbarzadeh2022learning}, that is a modified version of TSDE \cite{ouyang2017learning} to restless Markovian bandit, and a discussion on their result.

%This definition implicitly requires that the unknown MDP $M$ is weakly communicating and the reward is bounded.

\subsubsection{Overview of regret analysis of RB-TSDE {\cite{akbarzadeh2022learning}}}

In this section, we start by presenting how RB-TSDE \cite{akbarzadeh2022learning} works and the assumptions needed in \cite{akbarzadeh2022learning}. Next, we provide an overview of regret analysis of RB-TSDE under the assumptions. We finish the section with a discussion on their result.

RB-TSDE is a Bayesian learning algorithm for restless Markovian bandits with average reward criterion.
Extended from TSDE \cite{ouyang2017learning}, RB-TSDE updates its policy episodically as the following.
Let $t^k$ be the time step at which the episode $k$ begins with the convention $t^1:=1$.
For each arm $i\in[n]$, let $N_t(s_i,a_i)$ be the number of times up to time step $t$ that the learner executes action $a_i$ when arm $i$ is in state $s_i$.
RB-TSDE terminates episode $k\ge1$ at time step $t> t^k$ and updates its policy if
\begin{equation}
    \label{ch:restless:eq:update_crit}
    t-t^{k-1}>2t^k \text{ or } N_t(s_i,a_i) > 2N_{t^k}(s_i,a_i) \text{ for some }(s_i,a_i).
\end{equation}
RB-TSDE \cite{akbarzadeh2022learning} exploits the structure of restless bandit by choosing a prior distribution $\phi_i$ for each unknown arm $\langle\gS_i, \{0,1\}, r_i, p_i\rangle$ before the learning.
At the beginning of episode $k\ge1$, RB-TSDE uses the collected observations $o_{t^k}$ to derive a posterior $\phi_i(\cdot\mid o_{t^k})$, and draws a sample $(r^k_i,p^k_i)$ according to $\phi_i(\cdot\mid o_{t^k})$ for each arm $i$.
The samples $\{(r^k_i,p^k_i)\}_{i\in[n]}$ is then used to compute Whittle index policy $\pi^k$.

RB-TSDE \cite{akbarzadeh2022learning} computes Whittle index policy $\pi^k$ of the sampled bandit $M^k$ because Whittle index policy of the unknown bandit $M$, denoted by $\pi^\lambda$, is used as the baseline policy for evaluating the regret of RB-TSDE.
So, the work of \cite{akbarzadeh2022learning} assumes three conditions: Let $\mathrm{supp}(\phi_i)$ be the support of prior distribution $\phi_i$ for each arm $i$.
Let $\phi$ be the joint prior distibution of all arms and $\mathrm{supp}(\phi):=\bigotimes_{i=1}^n\mathrm{supp}(\phi_i)$ be the support of the joint prior $\phi$.
\begin{asmp}
    \label{asmp:indexable}
    For any $i\in[n]$ and $(r'_i,p'_i)\in\mathrm{supp}(\phi_i)$, the arm $\langle\gS_i,\{0,1\},r'_i,p'_i\rangle$ is indexable.
\end{asmp}
\begin{asmp}
    \label{asmp:unichain}
    Any bandit $M'\in\mathrm{supp}(\phi)$ admits Whittle index policy $\pi'$ that is unichain.
\end{asmp}
\begin{asmp}
    \label{asmp:ergodic}
    There exists $\beta^*<1$ such that any bandit $M'\in\mathrm{supp}(\phi)$ admits an ergodicity coefficient $\beta_{M'}\le \beta^*$ where
    \begin{align}
        \label{eq:ergodic_M}
        \beta_{M'} = 1-\min_{\substack{\vs,\vs'\in\gX \\\va,\va'\in\gA(m)}} \sum_{\vz\in\gX} \min\{p'(\vz \mid \vs, \va), p'(\vz \mid \vs', \va')\}.
    \end{align}
    %For any $i\in[n]$ and $(r'_i,p'_i)\in\mathrm{supp}(\phi_i)$, the arm $\langle\gS_i,\{0,1\},r'_i,p'_i\rangle$ is indexable.
\end{asmp}
The reasons why Assumptions~\ref{asmp:indexable} and \ref{asmp:unichain} are needed are already discussed in Section~\ref{ch:restless:discuss_policy}.
The Assumption~\ref{asmp:ergodic} allows us to bound the span of global bias function in function of $\beta^*$.
In addition, we should mention that by our Theorem~\ref{thm:unichain}, Assumption~\ref{asmp:ergodic} actually implies Assumption~\ref{asmp:unichain}.

Assume that the unknown bandit $M$ is drawn from $\mathrm{supp}(\phi)$.
Under Assumptions~\ref{asmp:indexable} and \ref{asmp:unichain}, Whittle index policy $\pi^\lambda$ is well-defined and induces a state-independent gain $g^\widx_M$ in the unknown bandit $M$.
Similarly to Definition~\ref{ch:rl:defn:rg_infinite}, the regret of RB-TSDE after $T$ steps is given by
\begin{align}
    \Reg(\textnormal{RB-TSDE},M,T) := Tg^\widx_M - \sum_{t=1}^{T} \sum_{i=1}^nr_{t,i}. \label{ch:restless:eq:regret}
\end{align}
Its Bayesian regret is given by \eqref{eq:bayes_rg}.

Now, we focus on bounding the regret of RB-TSDE.
Let $K_T$ be the total number of episodes up to time $T$.
Then,
\begin{align}
    \Reg(\textnormal{RB-TSDE},M,T) 
    &= \sum_{k=1}^{K_T}\sum_{t=t^k}^{t^{k+1}-1} \Bigl(g^\widx_M -\sum_{i=1}^{n}r(s_{t,i},a_{t,i})\Bigr) \label{ch:restless:eq:regret1}\\
    &\qquad +\underbrace{\sum_{i=1}^{n}\Bigl(r(s_{t,i},a_{t,i})-r_{t,i}\Bigr)}_{\Delta_{t,0}} \nonumber
\end{align}

%The algorithm $\gL$ updates its policy in an episodic manner. %using doubling trick.
%Let $M^k$ be the imagined bandit of the algorithm for episode $k$ and $\pi^k$ be the Whittle index policy of $M^k$.
Recall $\pi^k$ is the Whittle index policy of the imagined bandit $M^k$.
Under Assumption~\ref{asmp:indexable}, $\pi^k$ is well-defined and under Assumption~\ref{asmp:unichain}, the gain of $\pi^k$ is state-independent.
Then, the Bellman evaluation equation for $\pi^k$ in $M^k$ gives: for global state $\vs_t\in\gX$, $\va_t=\pi^k(\vs_t)$,
\begin{equation}
    \label{eq:be_imagined}
    0 =  \sum_{i=1}^nr^k(s_{t,i}, a_{t,i}) -g^{\pi^k}_{M^k} +\sum_{\vs'\in\gX}p^k(\vs' \mid \vs_t, \va_t)h^{\pi^k}_{M^k}(\vs') -h^{\pi^k}_{M^k}(\vs_t).
\end{equation}
Then, adding \Eqref{eq:be_imagined}, $0=\sum_{\vs'\in\gX}p(\vs' \mid \vs_t, \va_t)\Bigl(h^{\pi^k}_{M^k}(\vs')-h^{\pi^k}_{M^k}(\vs')\Bigr)$, and $0=h^{\pi^k}_{M^k}(\vs_{t+1})-h^{\pi^k}_{M^k}(\vs_{t+1})$ to \Eqref{ch:restless:eq:regret1}, and regrouping terms give
\begin{align*}
    \Reg(\textnormal{RB-TSDE},M,T)
    &=\sum_{k=1}^{K_T}\sum_{t=t^k}^{t^{k+1}-1} \underbrace{(g^\widx_M{-}g^{\pi^k}_{M^k})}_{\Delta_{t,1}} {+}\underbrace{\sum_{i=1}^n\Bigl(r^k(s_{t,i},a_{t,i}){-}r(s_{t,i},a_{t,i})\Bigr)}_{\Delta_{t,2}} \\
    &\quad {+}\underbrace{\sum_{\vs'\in\gX}\Bigl(p^k(\vs' \mid \vs_t, \va_t){-}p(\vs' \mid \vs_t, \va_t)\Bigr)h^{\pi^k}_{M^k}(\vs')}_{\Delta_{t,3}} {+}\underbrace{h^{\pi^k}_{M^k}(\vs_{t+1}){-}h^{\pi^k}_{M^k}(\vs_{t})}_{\Delta_{t,4}} \\
    &\quad {+}\underbrace{\sum_{\vs'\in\gX} p(\vs' \mid \vs_t, \va_t)h^{\pi^k}_{M^k}(\vs')-h^{\pi^k}_{M^k}(\vs_{t+1})}_{\Delta_{t,5}} +\Delta_{t,0}.
\end{align*}
The six quantities are bounded as the following. % according to the approach used.

\begin{itemize}
    \item The terms $\Delta_{t,0}$ and $\Delta_{t,5}$ are martingale difference terms whose expected value is zero.
    \item The expected value of $\sum_{k=1}^{K_T}\sum_{t=t^k}^{t^{k+1}-1}\Delta_{t,1}$ is bounded by $r_{max} K_T$ \cite[Appendix~A.2]{akbarzadeh2022learning}
    \item The term $\Delta_{t,2}$ is bounded by Hoeffding's inequality (or simply zero if the rewards are deterministic).
    \item The telescopic sum $\sum_{k=1}^{K_T}\sum_{t=t^k}^{t^{k+1}-1} \Delta_{t,4}$ is bounded by $\sum_{k=1}^{K_T} sp(\vh^{\pi^k}_{M^k})$.
\end{itemize}

The term $\Delta_{t,3}$ is bounded by $sp(\vh^{\pi^k}_{M^k})\norm{\vp^k(\cdot\mid \vs_t,\va_t)-\vp(\cdot\mid \vs_t,\va_t)}_{\ell_1}$.
In general MDPs, the concentration of global transition $\sum_{\vs'\in\gX} \abs{p^k(\vs'\mid \vs,\va) -p(\vs'\mid \vs,\va)}$ is bounded proportionally to $\sqrt{\abs{\gX}}$ (see \cite{qian2020concentration}).
However, restless bandit is a well-structure MDP.
The work of \cite{akbarzadeh2022learning} exploits this structure as the following:
For any global state $\vs\in\gX$ and global action $\va\in\gA(m)$, it follows from \cite[Lemma~13]{jung2019thompson} that
\begin{align*}
    \sum_{\vs'\in\gX} \abs{p^k(\vs'\mid \vs,\va) -p(\vs'\mid \vs,\va)}
    \le \sum_{i=1}^{n} \sum_{s'_i\in\gS_i} \abs{p^k(s'_i\mid s_i,a_i) -p(s'_i\mid s_i,a_i)}
\end{align*}
where $\sum_{s'_i\in\gS_i} \abs{p^k(s'_i\mid s_i,a_i) -p(s'_i\mid s_i,a_i)}$ is the concentration of local transition which is bounded proportionally to $\sqrt{S}$ using Weissman's inequality.

Using the doubling trick \eqref{ch:restless:eq:update_crit}, \cite[Lemma~A.1]{akbarzadeh2022learning} gives $K_T\le2\sqrt{nST\ln(T)}$. So, what is left to bound is the span of global bias function $sp(\vh^{\pi^k}_{M^k})$.
By \cite[Lemma~5.1]{akbarzadeh2022learning}, for any $k\ge1$, $sp(\vh^{\pi^k}_{M^k})\le \displaystyle\frac{2r_{max}}{1-\beta^*}$.

In summary, RB-TSDE \cite{akbarzadeh2022learning} enjoys the following Bayesian regret bound.
\begin{prop}[{\cite[Theorem~4.1]{akbarzadeh2022learning}}]
    Assume that $M\in\mathrm{supp}(\phi)$.
Under Assumptions~\ref{asmp:indexable}-\ref{asmp:ergodic}, 
    \begin{equation}
        \label{eq:rb_tsde_rg}
        \BayReg(\textnormal{RB-TSDE},\phi,T) < 40\frac{r_{max}}{1-\beta^*}nS\sqrt{T\ln(T)}
    \end{equation}
\end{prop}
This result presents an exponential improvement compared to the current best general-purpose RL algorithms whose regret bound is $\tilde{\landauO}\displaystyle\Bigl(\sqrt{HS^n{n \choose m}T}\Bigr)$.

However, the upper bound on the span of global bias function $H:=\displaystyle\frac{2r_{max}}{1-\beta^*}$ in \eqref{eq:rb_tsde_rg} has no explicit dependency with the number of arms $n$.

In the following section, we provide our last result that shows the dependency between $\beta^*$ and $n$ under Assumption~\ref{asmp:ergodic}.

\subsubsection{Local and global ergodicity coefficients in restless Markovian bandit}

Consider the bandit $M$ described in Section~\ref{ch:restless:sec:restless}.
%, the following theorem shows how the assumption on the ergodicity coefficient of local arms affects the ergodicity coefficient of the bandit.

\begin{thm}[Local and global ergodicity coefficients]
    For each arm $i\in[n]$, the ergodicity coefficient $\gamma_i$ of the arm is defined by
    \begin{align*}
        \gamma_i = 1-\min_{\substack{s_i,s'_i\in\gS_i \\a,a'\in\{0,1\}}} \sum_{z_i\in\gS_i} \min\{p(z_i \mid x_i, a), p(z_i \mid y_i, a')\}.
    \end{align*}
    Similarly, the ergodicity coefficient $\beta_M$ of the bandit $M$ is defined in the same manner as \eqref{eq:ergodic_M}.

    If for any arm $i$, $\gamma_i<1$, then $\beta_M<1$.

    Moreover, if there exists $\varepsilon>0$ such that for any arm $i\in[n]$,
    \begin{equation}
        \label{eq:gamma_ep}
        \gamma_i \le 1-\varepsilon,
    \end{equation}
    then $\beta_M \le 1-\varepsilon^n$.
    \label{thm:ergodicity_coeff}
\end{thm}
\begin{proof}
    For any two global states $\vs,\vs'\in\gX$ and any two actions $\va,\va'\in\gA(m)$, we have
    \begin{align*}
        \sum_{\vz\in\gX} \min\{p(\vz \mid \vs, \va), p(\vz \mid \vs', \va')\}
        &= \sum_{\vz\in\gX} \min\left\{\prod_{i=1}^np(z_i \mid s_i, a_i), \prod_{i=1}^np(z_i \mid s'_i, a'_i)\right\} \\
        &\ge \sum_{\vz\in\gX}\prod_{i=1}^n \min\{p(z_i \mid s_i, a_i), p(z_i \mid s'_i, a'_i)\} \\
        &= \prod_{i=1}^n \left(\sum_{z_i\in\gS_i} \min\{p(z_i \mid s_i, a_i), p(z_i \mid s'_i, a'_i)\}\right).
    \end{align*}
    Since for any $i\in[n]$, $\gamma_i<1$, we have that
    \begin{align*}
        \sum_{z_i\in\gS_i} \min\{p(z_i \mid s_i, a_i), p(z_i \mid s'_i, a'_i)\}
        &\ge \min_{\substack{x_i,y_i\in\gS_i \\a,a'\in\{0,1\}}} \sum_{z_i\in\gS_i} \min\{p(z_i \mid x_i, a), p(z_i \mid y_i, a')\} \\
        &= 1-\gamma_i >0.
    \end{align*}
    In consequences, ${\sum_{\vz\in\gX} \min\{p(\vz \mid \vs, \va), p(\vz \mid \vs', \va')\}{>}0}$.
    We conclude that $\beta_M{<}1$.
    \smallskip
    Moreover, if there exists $\varepsilon>0$ such that \eqref{eq:gamma_ep} holds for any $i\in[n]$, then it follows that
    \begin{align*}
        \prod_{i=1}^n \left(\sum_{z_i\in\gS_i} \min\{p(z_i \mid s_i, a_i), p(z_i \mid s'_i, a'_i)\}\right)
        &\ge \varepsilon^n.
    \end{align*}
    We can conclude that $\beta_M\le 1-\varepsilon^n$ using its definition.
\end{proof}

With Theorem~\ref{thm:ergodicity_coeff}, Assumption~\ref{asmp:ergodic} is equivalent to saying that there exists $\varepsilon>0$ such that any bandit $M'\in\mathrm{supp}(\phi)$ satisfies the condition \eqref{eq:gamma_ep} of Theorem~\ref{thm:ergodicity_coeff}, and $\beta^*=1-\varepsilon^n$.
The latter implies that $H:=\displaystyle\frac{2r_{max}}{1-\beta^*}=\displaystyle2r_{max}\left(\frac{1}{\varepsilon}\right)^n$ which means that \eqref{eq:rb_tsde_rg} remains exponential in $n$.
However, this certainly does not cancel the exponential improvement in the regret bound of RB-TSDE \cite{akbarzadeh2022learning} over the general-purpose RL algorithms in learning restless Markovian bandit problems.

\section{Conclusion}
\label{ch:restless:sec:conclude}

In this chapter, we provide different examples that show that no RL algorithms can perform uniformly well over the general class of restless bandits all of whose arms are unichain. Moreover, our examples show that defining a class of restless bandits that have desirable global properties by relying on the assumption on local arms is difficult.
In particular, a restless bandit can be multichain even though all of its arms are ergodic.
We also present a few results such as a restless bandit is communicating if all arms are ergodic.
We discuss the requirement when using Whittle index policy as the baseline policy in regret definition for learning restless bandits and provides an overview of regret analysis of RB-TSDE \cite{akbarzadeh2022learning}, a modified version of TSDE \cite{ouyang2017learning}, whose regret bound provides an exponential improvement over the current best general-purpose RL algorithms.

We believe that it is challenging to derive an upper bound on the span of global bias function or the global diameter of restless bandit that is polynomially in the number of arms.
In fact, it is impossible to do so for general restless bandits as we have seen in this chapter that the MDP properties of some restless bandits such as the global diameter, or the span of global bias function can be exponential in the number of arms.
This calls for restrictive assumptions on bandits in order to to design an algorithm with a regret bound explicitly polynomially in the number of arms similarly to the work of \cite{wang2020restless}.
This also calls for model-free algorithms that directly learn Whittle index of the unknown restless bandit in the direction of \cite{fu2019towards, gibson2021novel, nakhleh2021neurwin, avrachenkov2022whittle}.

%\subsubsection{UCRL2 algorithm}
%
%The assumption used in UCRL2 is that the unknown MDP $M$ is \textbf{communicating} (otherwise, the diameter is infinite) and has bounded non-negative reward.
%This assumption implies that the optimal gain $g^*_M$ is state-independent and the regret is well defined.
%It is shown in \cite{jaksch2010near} that using UCRL2 algorithm provides the following: for each episode $k\ge1$,
%\begin{itemize}
%    \item the imagined MDP $M^k$ chosen by extended value iteration is communicating
%    \item the gain $g^{\pi^k}_{M^k}$ is state-independent
%    \item the diameter of $M^k$ is bounded, $D^k\le D$
%    \item the span of bias of policy $\pi^k$ in $M^k$ is bounded, $sp(\vh^{\pi^k}_{M^k})\le D^k\le D$.
%\end{itemize}
%
%%The second item is true because the set of plausible MDPs $\gM^k$ can be considered as a MDP with finitely many actions.
%%If $M$ belongs to $\gM^k$, then $\gM^k$ is also \emph{communicating}.
%%$g^k$ can be considered as the optimal gain of this finitely many actions MDP $\gM^k$.
%%So, $g^k$ is state-independent.
%%The third item is true because $M$ belongs to $\gM^k$ together with $\gM^k$ is a MDP with finitely many actions imply that the diameter of $\gM^k$ is upper bounded by the diameter of $M$.
%%Under the assumption, \cite[Theorem~4]{bartlett2012regal} implies that the span of the optimal bias (the bias that verifies Bellman optimality equation) is bounded by the diameter of the MDP times the optimal gain.
%
%So, for UCRL2, the six quantities above are bounded as the following.
%\begin{itemize}
%    \item By martingale argument and Azuma-Hoeffding's inequality for martingale difference sequence,
%        \begin{itemize}
%            \item the sum $\sum_{k=1}^{K_T}\sum_{t=t^k}^{t^{k+1}-1}\Delta_{t,0}$ is bounded by $\landauO(r_{max}\sqrt{T\ln(T)})$ with high probability
%            \item the sum $\sum_{k=1}^{K_T}\sum_{t=t^k}^{t^{k+1}-1}\Delta_{t,5}$ is bounded by $\landauO(D\sqrt{T\ln(T)})$ with high probability
%        \end{itemize}
%    \item \label{it:optimism} The term $\Delta_{t,1}$ is non-positive thanks to the optimism
%    \item The term $\Delta_{t,2}$ is bounded by Hoeffding's inequality (or simply zero if the rewards are deterministic);
%    \item The telescopic sum $\sum_{k=1}^{K_T}\sum_{t=t^k}^{t^{k+1}-1} \Delta_{t,4}$ is bounded by $\sum_{k=1}^{K_T} sp(\vh^{\pi^k}_{M^k}){\le} DK_T$
%    \item Finally, the term $\Delta_{t,3}$ is bounded by $sp(\vh^{\pi^k}_{M^k}) \norm{\vp^k-\vp}_{\ell_1}\le D\norm{\vp^k-\vp}_{\ell_1}$ and $\norm{\vp^k-\vp}_{\ell_1}$ by Weissman's inequality.
%\end{itemize}
%Moreover, by using doubling trick, \cite[Proposition~18]{jaksch2010near} gives, for any $T\ge \abs{\gX}\abs{\gA}$, the total number of episodes $K_T\le \abs{\gX}\abs{\gA}\log_2(\frac{8T}{\abs{\gX}\abs{\gA}})$.
%
%Hoeffding's and Weissman's inequalities induce another term of the form $\frac{1}{\sqrt{N^k(\vs_t,\va_t)}}$.
%The sum $\sum_{k=1}^{K_T}\sum_{t=t^k}^{t^{k+1}-1}\frac{1}{\sqrt{N^k(\vs_t,\va_t)}}$ is bounded by $\landauO(\sqrt{\abs{\gX}\abs{\gA(m)}T})$ similarly to what we have seen in Chapter~\ref{ch:learning_rested}. %Section~\ref{sec:} of Chapter~\ref{ch:rested}.
%
%\subsubsection{TSDE algorithm}
%
%TSDE is a Bayesian algorithm. So, it starts by choosing a prior distribution $\phi$ about the unknown MDP $M$.
%Let $\mathrm{supp}(\phi)$ denote the support of the distribution $\phi$.
%So, $\phi$ is chosen such that $M\in\mathrm{supp}(\phi)$.
%The assumptions used for TSDE are the expected reward of $M$ is non-negative and bounded, and the following.
%\begin{itemize}
%    \item $\mathrm{supp}(\phi)$ is a set of \textbf{weakly communicating} MDPs
%    \item there exists $H<+\infty$ such that any MDP in $\mathrm{supp}(\phi)$ has the span of optimal bias bounded by $H$.
%\end{itemize}
%These assumptions imply that for each episode $k\ge1$,
%\begin{itemize}
%    \item the imagined MDP $M^k$ sampled by the posterior is weakly communicating
%    \item the gain $g^{\pi^k}_{M^k}$ is state-independent
%    \item the span of bias of policy $\pi^k$ in $M^k$ is bounded by $H$, $sp(\vh^{\pi^k}_{M^k})\le H$.
%\end{itemize}
%So, for TSDE, the six quantities above are bounded as the following.
%\begin{itemize}
%    \item $\Delta_{t,0}$ and $\Delta_{t,5}$ are martingale difference term whose expected values are zero
%    \item The expected value of $\sum_{k=1}^{K_T}\sum_{t=t^k}^{t^{k+1}-1}\Delta_{t,1}$ is bounded by the upper bound of $K_T$ times $r_{max}$
%    \item The second term $\Delta_{t,2}$ is bounded by Hoeffding's inequality (or simply zero if the rewards are deterministic);
%    \item The telescopic sum $\sum_{k=1}^{K_T}\sum_{t=t^k}^{t^{k+1}-1} \Delta_{t,4}$ is bounded by $HK_T$
%    \item Finally, the term $\Delta_{t,3}$ is bounded by $H\norm{\vp^k-\vp}_{\ell_1}$ and $\norm{\vp^k-\vp}_{\ell_1}$ by Weissman's inequality.
%\end{itemize}
%
%Moreover, the doubling trick of TSDE gives $K_T\le \sqrt{2\abs{\gX}\abs{\gA}T\ln(T)}$ \cite[Lemma~1]{ouyang2017learning}.
%The term induced by Hoeffding's and Weissman's inequalities is bound exactly the same as UCRL2 does.
%
%So, directly apply UCRL2 or TSDE would incur a few problems
%\begin{itemize}
%    \item the assumptions on bandit may be computationally infeasible to check
%    \item the regret bound is exponential in the number of arms $n$.
%\end{itemize}

%Hence, if \eqref{eq:gamma_ep} holds for all $i\in[n]$, then the global span of optimal bias in the bandit $M$ is bounded like 
%\begin{align}
%    sp(\vh^*_M)\le \frac{r_{max}}{\varepsilon^n} \label{eq:span_expo}
%\end{align}
%where $\vh^*_M$ is the optimal bias of the bandit $M$.
%Indeed, if \eqref{eq:gamma_ep} holds for all $i\in[n]$, then $\Gamma \le 1-\varepsilon^n$ and the bandit $M$ is unichain.
%Let $\pi^*$ be an optimal policy and $\vr^{\pi^*}$ and $\mP^{\pi^*}$ be the expected reward vector and transition matrix under policy $\pi^*$.
%For any global state $\vs\in\gX$, $r^{\pi^*}(\vs)\in[0,r_{max}]$.
%Then, the optimal gain $g^*_M$ is bounded in $[0, r_{max}]$.
%We recall that the Bellman evaluation equation for unichain policy $\pi^*$ in vector form is $\vh^*=\vr^{\pi^*} - g^*\vone +\mP^{\pi^*}\vh^*$.
%Then, $sp(\vh^*_M)\le r_{max} +sp(\mP^{\pi^*}\vh^*_M)\le r_{max} +\Gamma sp(\vh^*_M)$ because $sp(\mP^{\pi^*}\vh^*_M)\le \Gamma sp(\vh^*_M)$.
%So, $sp(\vh^*_M)\le r_{max}/\varepsilon^n$.


\endgroup
