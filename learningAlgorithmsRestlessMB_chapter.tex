\begingroup
\let\clearpage\relax

\chapter{Learning in Average Reward Restless Markovian Bandits}
\label{ch:learning_restless}

In the previous chapter, we adapt a few learning algorithms to discounted rested Markovian bandits and showed that their regrets, which are upper bounded sublinearly in the number of arms, match the minimax Bayesian regret that we derived in the chapter.
We also showed that any optimistic index policy that, relying on the confidence bonus on the transition of arms, computes optimistic index on each arm independently from the other arms does not guarantee the optimism in face of uncertainty (OFU) principle.
In this chapter, we consider the learning problem in restless Markovian bandit with average reward criterion.
Such a bandit suffers from the curse of dimensionality and general-purpose learning algorithms are not efficient when applied to restless bandit. Recently, a few learning algorithms have been specifically designed to work for restless bandits. Yet, they most often work only for very particular subclasses of restless bandits, or they hold under conditions that are very hard to verify.
Hence, in this chapter, we provide some explanations why the performance of a learning algorithm for a restless bandit depends, in general, on some structural conditions (like diameter or bias) of the global MDP. We show that the properties of the local arms (like ergodic or small diameter) do not, in general, imply similar properties for the global MDPs. This implies that defining a class of restless bandit that has desirable properties (like small diameter) is difficult. 

In Section~\ref{ch:restless:related}, we provide the context of learning in restless bandit problems and discuss the existing works in reinforcement learning with generic MDPs and with restless bandit problems.
We then recall the notations and problem formulation of undiscounted restless bandit in Section~\ref{ch:restless:sec:restless}.
Section~\ref{ch:restless:sec:example} accumulates examples and counter-examples that respectively provide desirable and non-desirable properties for learning in restless bandit.
In Section~\ref{ch:restless:sec:generic}, we give an overview of regret analysis of two classical algorithms, namely UCRL2 \cite{jaksch2010near} and TSDE \cite{ouyang2017learning}, when considering the restless bandit as a generic MDP.
We present the existing results that exploit the structure of restless bandit to bound the regret and discuss about the conditions to assume when learning restless bandit in Section~\ref{ch:restless:sec:structure}.
Section~\ref{ch:restless:sec:conclude} concludes this chapter.

\section{Introduction}
\label{ch:restless:related}

Restless Markovian bandits are the specific MDPs that manifest the curse of dimensionality.
The obvious effect of the curse is the bandit state size that is exponential in the number of arms.
%Moreover, not only the state size that is cursed, but also the MDP parameters such as diameter.
%However, to the best of our knowledge, there is yet no clear dependency between the diameter or mixing time of the global MDP and the number of arms in general restless Markovian bandit.

For learning generic MDPs with average reward criterion, the current best algorithms have a regret over $T$ time steps bounded by $\landauO(\sqrt{DSAT})$ in the unknown MDP with state size $S$, action size $A$, and diameter $D$ (see Table~\ref{ch:rl:tab:infinite} for more algorithms with regret guarantee).
This regret bound is valid for communicating MDPs.
To what we have observed, the parameters of the MDP that appear in regret upper bound are:
\begin{itemize}
    \item the diameter $D$, defined by Definition~\ref{ch:rl:defn:diameter}, in \eg, \cite{jaksch2010near, fruit2020improved, tossou2019near}
    \item the upper bound on span of optimal bias $H\ge sp(\vh^*)$ in \eg, \cite{bartlett2012regal, ouyang2017learning, fruit2018efficient, zhang2019regret}
    \item the mixing time $t_{mix}$, defined by Definition~\ref{ch:restless:defn:mixing_time}, in \eg, \cite{ortner2020regret}.
\end{itemize}
Then, applying these algorithms to learn restless Markovian bandits with average reward criterion provokes a few critical issues:
\begin{itemize}
    %\item verifying the structure of restless bandit is computationally ``expensive'' 
    \item the state size of restless bandit is exponential in the number of arms which makes the regret not scalable with the number of arms
    \item the diameter $D$ (if defined), mixing time $t_{mix}$ (if defined), or the upper bound on span of the optimal bias $H$ may also be exponential in the number of arms
    \item compute an optimal policy in restless bandit is PSPACE-hard \cite{papadimitriou1994complexity}, let alone the computation of optimistic policy
\end{itemize}
%These parameters are finite for MDPs with specific structure.
%That is, the mixing time is defined for ergodic MDPs and the diameter of a MDP is finite if and only if the MDP is communicating.
%These algorithms are then applicable to restless Markovian bandits only if the latter have the specified structure.

%It is then reasonable to assume some structure properties of the bandit.
For partially observed restless bandit, the work of \cite{ortner2012regret} derives colored-UCRL algorithm, a modified version of UCRL2, that achieves a regret sublinear in time.
Colored-UCRL ``successfully'' removes the exponentiality in the number of arms from the state size of the bandit in its regret bound.
However, the mixing time parameter yet appears in the regret bound and the dependency between the mixing time and the number of arms is unclear.
The same discussion goes to the work of \cite{jung2019thompson} that adapts TSDE to the same setting of restless bandit in \cite{ortner2012regret}.
To the best of our knowledge, Restless-UCB of \cite{wang2020restless} is the first algorithm that has a regret bounded explicitly linearly in the number of arms for partially observed restless bandit.
Yet, we must mention that this result is valid for a very restrictive class of restless bandit in which each arm is the birth-death Markov reward process.
Also, Restless-UCB requires a generative model to be able to uniformly explore the dynamic of each arm before commiting on its optimistic planning.
All of these works also assume an oracle that knows how to compute an optimal policy given a restless bandit.

The work of \cite{akbarzadeh2022learning} adapts TSDE to the fully observed restless bandit problem and ``successfully'' removes the exponentiality in the number of arms from the state size of the bandit in the Bayesian regret bound of RB-TSDE, the modified version of TSDE.
However, the ergodicity coefficient of the global MDP appears in the regret bound and the dependency between such a coefficient and the number of arms is unclear.
%It may be useful to note that the ergodicity coefficient provides an upper bound on the mixing time of the MDP.

Finally, due to the curse of dimensionality, verifying the structure of the bandit may be computationally expensive, let alone the learning aspect.
This inspires us to study the implication of structure of arms in the structure of the bandit because verifying the structure of all arms is done linearly in the number of arms.

%That is, in average reward criterion, the structure of the MDP plays an important role in the performance of the learning algorithms.


%Reinforcement learning algorithms work depending on the assumption of MDP structure such as ergodic, communicating, and weakly communicating.

%(here should motivate the problem + add related work about learning and why it is related to diameters and other properties)

%(We should define local properties and global properties).

In this chapter, we show that:
\begin{itemize}
    \item there is a bandit, whose arms all are unichain, that is not learnable
    \item for restless bandits whose arms all are ergodic,
        \begin{itemize}
            \item there is a bandit that is multichain
            \item there is an ergodic bandit whose diameter is exponential in the number of arms
            \item there is an ergodic bandit whose mixing time can be as big as we want
        \end{itemize}
\end{itemize}

%TODO
%\label{ch:restless:tab:infinite}
%\end{table}

%A few existing works have tried to remove the exponentiality in the state size of restless bandit such as \cite{jung2019regret, ortner2012regret, wang2020restless} for partially observed setting and \cite{akbarzadeh2022learning} for fully observed setting.
%However, removing the exponentiality of the number of arms in the $D, t_{mix}$, or $H$ remains unclear.
%Moreover, these quantities are defined only if the MDP has a specific structure.
%For instance, the mixing time is defined only if the MDP is ergodic.
%Such an assumption is costly to verified in restless bandit due the curse of dimensionality in the state size of the global MDP.

\section{Undiscounted restless Markovian bandit}
\label{ch:restless:sec:restless}

We consider a restless Markovian bandit having $n$ arms.
Each arm $\langle\gS_i, \{0,1\}, r_i, p_i\rangle$ is a MDP with finite state space $\gS_i$ of size $S$ and binary action space $\{0,1\}$ where $0$ denotes the action ``rest'' and $1$ denotes the action ``activate''.
If arm $i$ is in state $s_i$ and the decision maker executes $a_i\in\{0,1\}$, the arm incurs a random reward with expected value $r_i(s_i,a_i)$ and transitions to state $s'_i\in\gS_i$ with probability $p_i(s'_i\mid s_i,a_i)$.
Similarly to what is done in Chapter~\ref{ch:rested}, we assume that the state space of the arms are pairwise distinct: $\gS_i\cap\gS_j=\emptyset$ for any $i\neq j$.
So, we will drop the index $i$ from expected reward and transition if no confusion is possible: we denote them by $r(s_i,a_i)$ instead of $r_i(s_i,a_i)$ and by $p(s'_i\mid s_i,a_i)$ instead of $p_i(s'_i\mid s_i,a_i)$.

The sequential decision problem is presented as the following.
At time step $1$, the state of all arms denoted by $\vs_1:=(s_{1,1},\dots,s_{1,n})$ is sampled according to some initial distribution $\rho$ over the state space $\gX:=\gS_1\times\dots\times\gS_n$.
At time step $t\ge1$, the decision maker observes the current state of all arms denoted by $\vs_t:=(s_{t,1},\dots,s_{t,n})$ and activates exactly $m$ arms encoded by action $\va_t:=(a_{t,1},\dots,a_{t,n})$ such that $\va_t\in\{0,1\}^n$ and $\sum_{i=1}^{n} a_{t,i}=m$ where $m\in[n]$ is constant over time.
%The decision maker then receives a random reward $r_t=\sum_{i=1}^{n}r_{t,i}$ where $r_{t,i}$ is a random reward from arm $i$.
Each arm $i$ incurs then a random reward $r_{t,i}$ and makes
a transition to new state $s_{t+1,i}$ in function of $s_{t,i}$ and $a_{t,i}$ but independently from the other arms.
So, the restless Markovian bandit is a specific MDP -- that we denote by $M$ -- whose state space is $\gX$ and action space is $\gA(m):=\{\va\in\{0,1\}^n : \sum_{i=1}^{n}a_i =m\}$.
We say that $M$ is the \emph{global} MDP and its arm is the \emph{local} MDP.
Without loss of generality, we assume that for any global state $\vs\in\gX$ and global action $\va\in\gA(m)$, the expected reward is bounded like $\sum_{i=1}^{n}r(s_i,a_i)\in[0,r_{max}]$.

The decision maker wants to compute a policy $\pi:\gX\mapsto\gA(m)$ that maximizes the gain as defined in Section~\ref{ch:mdp:sec:gain}: for any $\vs\in\gX$,
\begin{equation}
    \label{ch:restless:eq:obj}
    g^\pi_M(\vs):=\lim_{T\to+\infty}\frac1T \E^\pi\left[ \sum_{t=1}^{T} \sum_{i=1}^nr_{t,i} \mid \vs_1=\vs\right].
\end{equation}

As presented in Chapter~\ref{ch:mdp}, if the MDP $M$ has finite state and action spaces, then an optimal policy $\pi^*$ such that for all $\vs\in\gX, g^{\pi^*}(\vs)=g^*(\vs):=\max_{\pi}g^{\pi}(\vs)$ exists and is deterministic.
However, it is shown in \cite[Theorem~4]{papadimitriou1994complexity} that computing an optimal policy in restless bandit $M$ is PSPACE-hard.

%In about 1980, Peter Whittle proposed a heuristic in the form of largest index rule, later known as \textbf{Whittle index policy}, for undiscounted restless bandit problem.
%The computational complexity of this heuristic is linear in the number of arms which makes it scalable for problems with large number of arms.  
%We discuss more about this index policy in the following section.
%Similar to what is done in Section~\ref{ssec:rested_formul}, 
%In consequence, for any global state-action pair $(\vs,\va)$ with $\vs\in\gX$ and $\va\in\gA$, the expected reward from the MDP $M$ is given by $r(\vs,\va)=\sum_{i=1}^{n}r(s_i,a_i)$.
%Moreover, $M$ transitions to next state $\vs'\in\gX$ with probability $p(\vs'\mid \vs,\va)=\prod_{i=1}^n p(s'_i\mid s_i,a_i)$.

%The activated arm incurs a random reward discounted like $\gamma^{t-1}r_t$ where $\gamma\in(0,1)$ is the discount factor.

\subsection{Structural properties of global MDP}
\label{ssec:mdp_params}

We presented the classification of MDPs in Definition~\ref{ch:mdp:defn:mdp_class} of Chapter~\ref{ch:mdp}.
We also divided the MDP space as presented in \figurename~\ref{ch:mdp:fig:mdp_class}.
In addition, we say that a MDP is \emph{recurrent} if for all policy $\pi$, the matrix $\mP^\pi$ defines a Markov chain where all states are recurrent (equivalently, for all states $s$ and $s'$, a Markov chain starting in $s$ will visit $s'$ with probability $1$).
A recurrent or unichain MDP is called \emph{aperiodic} if all matrices $\mP^\pi$ are aperiodic.
While the definition of ergodic MDP is given in Definition~\ref{ch:mdp:defn:mdp_class}, we also say that a MDP is \emph{ergodic} if it is recurrent and aperiodic.
Following \cite[Definition 5.1]{wei2020model},
\begin{defn}
    \label{ch:restless:defn:mixing_time}
    The mixing time of an ergodic MDP with state space $\gS$ is defined as
    \begin{align*}
        t_{mix} := \max_{\pi}\min\left\{ t\ge1 : \norm{(\mP^\pi)^t(s,\cdot) - \mu^\pi}_{\ell_1} \le \frac14, \forall s\in\gS\right\}.
    \end{align*}
\end{defn}
By Definition~\ref{ch:restless:defn:mixing_time}, the mixing time is the maximum time required for any policy starting at any initial state to make the state distribution $\frac14$-close (in $\ell_1$ norm) to the stationary distribution, the state distribution in steady regime.

We recall that the diameter of a MDP (see Definition~\ref{ch:rl:defn:diameter}) is finite if and only if the MDP is communicating.
It is shown in \cite[Appendix A]{jaksch2010near} that for any MDP with state space $\gS$ and action space $\gA$, the diameter is lower bounded by $\log_{|\gA|}|\gS|-3$.
This implies that the diameter of a restless bandit $M$ described above is lower bounded by % with $n$ arms, each arm has the same state size $S$ and each decision time exactly $m$ arms are activated, is lower bounded by
\begin{equation*}
    \log_{|\gA(m)|}|\gX| -3 = \sum_{i=1}^{n}\log_{|\gA(m)|}|\gS_i| -3 =n\log_{{n \choose m}}S -3.
\end{equation*}
This is the lower bound of the diameter of general restless Markovian bandit that is communicating.

%\subsection{Span}
%
%Following \cite[Chapter~8]{puterman2014markov}, in finite-state Markov reward process $(\vr, \mP)$, we define $\bar{\mP}=\displaystyle\lim_{N\to+\infty}\frac1N\sum_{t=0}^{N-1}\mP^{t}$ as the Cesaro limit of the sequence $\{\mP^t\}_{t\ge0}$.
%This limit exists for finite-state process (see Section~A.4 of Appendix~A of \cite{puterman2014markov}).
%If $\bar{\mP}$ is stochastic, then the gain is $\vg=\bar{\mP}\vr$.
%The bias is defined according to the structure of Markov chain:
%\begin{enumerate}
%    \item if the Markov chain is \emph{aperiodic}, then the bias is $\vh=\displaystyle\sum_{t=0}^\infty(\mP^t-\bar{\mP})\vr$
%    \item if the Markov chain is \emph{periodic}, then the bias is $\vh=\displaystyle\lim_{N\to\infty}\frac1N\sum_{k=1}^N\sum_{t=0}^{k-1}(\mP^t-\bar{\mP})\vr$
%\end{enumerate}
%
%Let $\vh\in\sR^S$. The span of $\vh$ is given by $sp(\vh)=\max_xh(x) - \min_xh(x)$.

\section{Examples and counter-examples}
\label{ch:restless:sec:example}

In the following, we study the structure of the global MDPs when the local arms all are well structured.
To do so, we will provide a few simple examples and counter-examples.
In those examples, the bandit always has two arms and exactly one arm is activated at each decision time.
To ease the exposition, Arm $1$ is drawn in green color and Arm $2$ in dark purple.
In each arm, the transitions of action activate is drawn in back color and those of action rest is drawn in dashed red color.
In the global MDP, the state transitions when activating Arm $1$ is drawn in green arrows and those when activating Arm $2$ is drawn in dark purple.
The transitions under optimal action are drawn in double-head arrows.
The expected reward and probability of transition are noted along the transition arrows.
However, if the transition is deterministic, \ie, the probability is $1$, we only note the expected reward along the transition arrows.


%The following theorem shows that a restless bandit whose arms are all unichain is not necessarily unichain. This is true under the additional condition that all arms are aperiodic.
\subsection{Negative results}

In general, the expected regret is frequently bounded based on the concentration inequalities. 
These inequalities encode how well an unknown parameter is estimated.
So, it should be safe to say that ``the regret is bounded if the unknown parameters of the MDP are well estimated''.
Meanwhile, for a restless Markovian arm having $S$ states, there are at most $2S+2S^2$ parameters.
This means that a restless bandit having $n$ arms, each with $S$ states, is a specific MDP in which there are $n(2S+2S^2)$ parameters instead of $\landauO(S^{n})$.
Intuitively, if each arm visits all of its states frequently, then the $2S+2S^2$ unknown parameters should be well estimated as well as the $n(2S+2S^2)$ unknown parameters of the global MDP.
Then, the expected regret in learning such a restless bandit should be bounded linearly in $n$.
Unfortunately, we show in this section that this reasoning is not always applicable. 

\subsubsection{Non learnable restless bandit}

We show in the following theorem that the assumption that each arm is unichain and has a bounded local span of bias of any local policy is not enough to make a restless bandit learnable.

\begin{thm}[Non learnable restless bandit]
    \label{thm:non_learnable}
    We consider reinforcement learning problem with restless Markovian bandit.
    \begin{enumerate}[label=(\roman*)]
        \item \label{it:non_learnable1} For any learning algorithm $\gL$, there is a restless bandit having arms all unichain, each arm has a bounded local span of bias of any local policy, such that for any total number of time steps $T\ge2$, the expected regret of $\gL$ at least $\frac{T}4$.
        \item \label{it:non_learnable2} For any learning algorithm $\gL$, any total number of time steps $T\ge2$, there is a restless bandit having arms all unichain, each arm has a bounded local span of bias of any local policy, that is weakly communicating and such that, the expected regret of $\gL$ at least $\frac{T}5$.
    \end{enumerate}
\end{thm}
\begin{proof}
    \textbf{Proof of \ref{it:non_learnable1}} -- consider two restless bandit problems given in \figurename~\ref{fig:hard_global}.
    The arms of both bandits are given in \figurename~\ref{fig:hard_local}.
    All arms are unichain and have a local span of optimal bias bounded by $0.5$.
    Both bandits $M^a$ and $M^b$ have two recurrent classes $\gX^1$ whose optimal gain is $0.5$ and $\gX^2$ whose optimal gain is $1$.
    So, the regret gap when ending up in $\gX^1$ instead of $\gX^2$ is $\frac12$.
    At time step $1$, both $M^a$ and $M^b$ are in state $(3,1)$ with probability $0.5$ and state $(3,2)$ with probability $0.5$.
    Consider any learning algorithm $\gL$ that knows the parameters of both $M^a$ and $M^b$ but does not know with which one it is interacting.
    In states $(3,1)$, $\gL$ activates Arm $1$ with probability $\mu_1$ and in $(3,2)$, $\gL$  rests Arm $1$ with probability $\mu_2$ where $\mu_1$ and $\mu_2$ are our degree of freedoms to minimize the expected regret.

    Facing $M^a$, $\gL$ ends up in $\gX^1$ with probability $(1-\mu_1)$ when starting in $(3,1)$ and with probability $(1-\mu_2)$ when starting in $(3,2)$.
    Then, its expected regret is
    \begin{align*}
        \ex{\Reg(\gL,M^a,T)}
        &=0.5\Bigl((1-\mu_1)\frac{T}2 +\mu_1\times 0\Bigr) +0.5\Bigl((1-\mu_2)\frac{T}2 +\mu_2\times 0\Bigr)\\
        &=\frac{T}2\left(1-\frac{\mu_1+\mu_2}2\right).
    \end{align*}
    Facing $M^b$, $\gL$ ends up in $\gX^1$ with probability $\mu_1$ when starting in $(3,1)$ and with probability $\mu_2$ when starting in $(3,2)$ and its expected regret is
    \begin{align*}
        \ex{\Reg(\gL,M^b,T)}
        &=0.5\Bigl((1-\mu_1)\times 0 +\mu_1\frac{T}2\Bigr) +0.5\Bigl((1-\mu_2)\times 0 +\mu_2\frac{T}2\Bigr)\\
        &=\frac{T}2\times\frac{\mu_1+\mu_2}2.
    \end{align*}
    Then $\gL$ wants to adjust $\mu_1$ and $\mu_2$ such that
    \begin{equation}
        \label{eq:rg_linear}
        \min_{\mu_1,\mu_2\in[0,1]}\left(\max\biggl\{\frac{T}2\left(1-\frac{\mu_1+\mu_2}2\right), \frac{T}2\times\frac{\mu_1+\mu_2}2\biggr\}\right).
    \end{equation}
    For the best possible choice of $(\mu_1,\mu_2)$, the minimum value of \eqref{eq:rg_linear} is $\displaystyle\frac{T}4$.
    This means that no learning algorithms can achieve an expected regret smaller than $\frac{T}4$ on both models at the same time.
    That concludes the proof.
    \medskip \\

    \textbf{Proof of \ref{it:non_learnable2}} -- It is derived from \ref{it:non_learnable1} by slightly modifying the transition of state $1$ of Arm $1$ as the following: if Arm $1$ is activated in state $1$, it transitions to state $3$ with probability $\varepsilon$ or to state $2$ with probability $1-\varepsilon$.
    Then, the modified global MDPs $M^a(\varepsilon)$ and $M^b(\varepsilon)$ are both weakly communicating.
    By specifying a very small $\varepsilon$, for example, $\varepsilon=1/T^2$ and following the proof of \ref{it:non_learnable1} conclude the proof. 

    \begin{figure}[htbp]
        \centering
        \begin{tabular}{ccc}
        \begin{tikzpicture}[on grid, state/.style={circle,draw}, >= stealth', auto, prob/.style = {inner sep=1pt,font=\scriptsize}]
            \node[state, black!45!green,line width=0.4mm]  (A) {$1$};
            \node[state, black!45!green,line width=0.4mm]  (B) [below = 2cm of A]   {$2$};
            \node[state, black!45!green,line width=0.4mm]  (C) [below left = 1cm and 2cm of A]   {$3$};
            \path[->]
            (A) edge[bend left=75]     node{$0$}	(B)
            (A) edge[bend left, dashed, red]     node{$0$}	(B)
    	    (B) edge[bend left=75]     node{$1$}	(A)
    	    (C) edge[bend left]     node{$0$}	(A)
            (C) edge[bend right, dashed, red]     node[below]{$0$}	(B)
            (B) edge[bend left, dashed, red]     node{$0$}	(A);
        \end{tikzpicture}
        &
        \begin{tikzpicture}[on grid, state/.style={circle,draw}, >= stealth', auto, prob/.style = {inner sep=1pt,font=\scriptsize}]
            \node[state, black!45!green,line width=0.4mm]  (A) {$1$};
            \node[state, black!45!green,line width=0.4mm]  (B) [below = 2cm of A]   {$2$};
            \node[state, black!45!green,line width=0.4mm]  (C) [below left = 1cm and 2cm of A]   {$3$};
            \path[->]
            (A) edge[bend left=75]     node{$0$}	(B)
            (A) edge[bend left, dashed, red]     node{$0$}	(B)
    	    (B) edge[bend left=75]     node{$1$}	(A)
    	    (C) edge[bend left, dashed, red]     node{$0$}	(A)
            (C) edge[bend right]     node[below]{$0$}	(B)
            (B) edge[bend left, dashed, red]     node{$0$}	(A);
        \end{tikzpicture}
        &
        \begin{tikzpicture}[on grid, state/.style={circle,draw}, >= stealth', auto, prob/.style = {inner sep=1pt,font=\scriptsize}]
            \node[state, RoyalBlue,line width=0.4mm]  (A) {$1$};
            \node[state, RoyalBlue,line width=0.4mm]  (B) [below = 2cm of A]   {$2$};
            \path[->]
            (A) edge[bend left=75]     node{$0$}	(B)
            (A) edge[bend left, dashed, red]     node{$0$}	(B)
    	    (B) edge[bend left=75]     node{$1$}	(A)
    	    (B) edge[bend left, dashed, red]     node{$0$}	(A);
        \end{tikzpicture}
        \\
            Arm $1^a$ & Arm $1^b$ & Arm $2$ 
        \end{tabular}
        \caption{
            Restless Markovian arms. Arm $1^a$ and Arm $1^b$ has 3 states and Arm $2$ has 2 states.
            The black arrows show state transition of action activate and the dashed red one for action rest.
            The numbers along the arrows show the expected reward when executing the actions.
            The span of the optimal bias in any arms is $0.5$.
            The two corresponding global MDPs is given in \figurename~\ref{fig:hard_global}
        }
        \label{fig:hard_local}
    \end{figure}
    
    \begin{figure}[htbp]
        \centering
        \begin{tabular}{cc}
        \begin{tikzpicture}[on grid, state/.style={circle,draw}, >= stealth', auto, prob/.style = {inner sep=1pt,font=\scriptsize}]
            \node[state]  (A) {$1,1$};
            \node[state]  (B) [above = 2cm of A]   {$2,2$};
            \node[state]  (C) [right = 4cm of B]   {$1,2$};
            \node[state]  (D) [below = 2cm of C]   {$2,1$};
            \node[state]  (E) [right = 2cm of B]   {$3,1$};
            \node[state]  (F) [below = 2cm of E]   {$3,2$};
            \path[->]
            (A) edge[line width=0.4mm, bend left, black!45!green]     node{$0$}	(B)
    	    (C) edge[line width=0.4mm, bend left, black!45!green]     node{$0$}	(D)
            (D) edge[line width=0.4mm, bend left=75, RoyalBlue] node{$0$} (C)
            (A) edge[line width=0.4mm, bend left=75, RoyalBlue]     node{$0$}	(B)
            (E) edge[line width=0.4mm, RoyalBlue] node[above]{$0$} (B)
            (F) edge[line width=0.4mm, black!45!green] node[below]{$0$} (A)
            %\draw[-{Stealth}{Stealth}]
    	    (B) edge[line width=0.4mm, bend left, black!45!green]     node{$1$}	(A)
            (D) edge[line width=0.4mm, bend left, black!45!green] node{$1$} (C)
    	    (B) edge[line width=0.4mm, bend left=75, RoyalBlue]     node{$1$}	(A)
    	    (C) edge[line width=0.4mm, bend left=75, RoyalBlue]     node{$1$}	(D)
            (E) edge[line width=0.4mm, black!45!green] node[above]{$0$} (C)
            (F) edge[line width=0.4mm, RoyalBlue] node[below]{$1$} (D);
        \end{tikzpicture}
        &
        \begin{tikzpicture}[on grid, state/.style={circle,draw}, >= stealth', auto, prob/.style = {inner sep=1pt,font=\scriptsize}]
            \node[state]  (A) {$1,1$};
            \node[state]  (B) [above = 2cm of A]   {$2,2$};
            \node[state]  (C) [right = 4cm of B]   {$1,2$};
            \node[state]  (D) [below = 2cm of C]   {$2,1$};
            \node[state]  (E) [right = 2cm of B]   {$3,1$};
            \node[state]  (F) [below = 2cm of E]   {$3,2$};
            \path[->]
            (A) edge[line width=0.4mm, bend left, black!45!green]     node{$0$}	(B)
    	    (C) edge[line width=0.4mm, bend left, black!45!green]     node{$0$}	(D)
            (D) edge[line width=0.4mm, bend left=75, RoyalBlue] node{$0$} (C)
            (A) edge[line width=0.4mm, bend left=75, RoyalBlue]     node{$0$}	(B)
            (E) edge[line width=0.4mm, black!45!green] node[above]{$0$} (B)
            (F) edge[line width=0.4mm, RoyalBlue] node[below]{$1$} (A)
            %\draw[-{Stealth}{Stealth}]
    	    (B) edge[line width=0.4mm, bend left, black!45!green]     node{$1$}	(A)
            (D) edge[line width=0.4mm, bend left, black!45!green] node{$1$} (C)
    	    (B) edge[line width=0.4mm, bend left=75, RoyalBlue]     node{$1$}	(A)
    	    (C) edge[line width=0.4mm, bend left=75, RoyalBlue]     node{$1$}	(D)
            (E) edge[line width=0.4mm, RoyalBlue] node[above]{$0$} (C)
            (F) edge[line width=0.4mm, black!45!green] node[below]{$0$} (D);
        \end{tikzpicture}
        \\
            $M^a:=\{\text{Arm }1^a, \text{Arm }2\}$ & $M^b:=\{\text{Arm }1^b, \text{Arm }2\}$
        \end{tabular}
        \caption{
            Two restless bandits, each with two arms given in \figurename~\ref{fig:hard_local}.
            Exactly one arm is activated at each decision time and the initial global state is either $(3,1)$ or $(3,2)$.
            The global state is denoted by $(s_1,s_2)$ where $s_1$ is the state of Arm $1$ and $s_2$ of Arm $2$.
            The green arrows show the state transition when activating Arm $1$.
            The purple arrows show the state transition when activating Arm $2$.
            The numbers along the arrows show the expected reward when executing the actions.
            The global MDP $M^a$ is formed by Arm $1^a$ and Arm $2$ and $M^b$ by Arm $1^b$ and Arm $2$.
            Both MDPs are multichain and not weakly communicating because state $(3,1)$ and $(3,2)$ are transient states and there are two recurrent classes: $\gX^1:=\{(1,1), (2,2)\}$ and $\gX^2:=\{(1,2),(2,1)\}$.
            The optimal gain in class $\gX^1$ is $0.5$ and in class $\gX^2$ is $1$.
            This shows that the actions in states $(3,1)$ and $(3,2)$ are decisive.
            In $M^a$, the optimal action in state $(3,1)$ is to rest Arm $2$ and in state $(3,2)$ is to activate Arm $2$.
            In contrast, in $M^b$, the optimal action in state $(3,1)$ is to activate Arm $2$ and in state $(3,2)$ is to rest Arm $2$.
            Starting by either $(3,1)$ or $(3,2)$, no learning algorithms have a sublinear expected regret in both examples.
        }
        \label{fig:hard_global}
    \end{figure}
\end{proof}

The difference between \ref{it:non_learnable1} and \ref{it:non_learnable2} of Theorem~\ref{thm:non_learnable} is that the global MDPs in \ref{it:non_learnable1} do not depend on $T$ while the ones in \ref{it:non_learnable2} does.
So the result of \ref{it:non_learnable1} is stronger.
The reason that \ref{it:non_learnable2} is proposed is because the global MDPs in \ref{thm:non_learnable1} are not weakly communicating.
According to Definition~\ref{ch:rl:defn:rg_infinite}, the regret is not defined in MDPs that are not weakly communicating.
So, the regret in \ref{thm:non_learnable1} should be understood as the difference between the cumulative reward of an optimal policy and the cumulative reward of the learner. 
For \ref{it:non_learnable2}, the global MDPs are weakly communicating and Definition~\ref{ch:rl:defn:rg_infinite} is used for regret.

Theorem~\ref{thm:non_learnable} inspires use to define a more restrictive assumption on local arm.
%We should mention that the global MDPs used in the proof of Theorem~\ref{thm:non_learnable} are not weakly communicating. So, their diameter is infinite.
%We will see in the following that there exists restless bandits whose diameter is exponential in the number of arms.

\subsubsection{Hard to learn restless bandits whose arms all are ergodic}

The following theorem shows that a restless bandit whose arms are all ergodic or all recurrent is not necessarily unichain.

\begin{thm}[Multichain restless bandit]
    \label{thm:multichain}
    For restless bandits with finite number of arms and each arm has finite number of states,
    \begin{enumerate}[label=(\roman*)]
        \item \label{thm:not_ergodic} there exists a restless bandit whose arms are all recurrent that is not weakly communicating. % the term noncommunicating is taken from [Puterman, 1994, page 353]. 
        \item \label{thm:ergodic_arms_multichain_RB} there exists a restless bandit whose arms are all ergodic that is multichain.
    \end{enumerate}
\end{thm}

\begin{proof}
    \textbf{Proof of \ref{thm:not_ergodic}} -- It is given by the counter-example in which the restless bandit has two arms as given by \figurename~\ref{fig:recur_non_communicate} and exactly one arm is activated at each decision time.
    For each arm, both ``activate'' and ``rest'' actions induce the same state transition.
    In such example, the restless bandit is a multichain and not weakly communicating MDP because if we start at state $(1,1)$ or $(2,2)$, then there are two recurrent classes: one composed of $\{(1,1), (2,2)\}$ and the other one is $\{(1,2), (2,1)\}$.
    \begin{figure}[ht]
        \centering
        \begin{tabular}{ccc}
        \begin{tikzpicture}[on grid, state/.style={circle,draw}, >= stealth', auto, prob/.style = {inner sep=1pt,font=\scriptsize}]
            \node[state, black!45!green,line width=0.4mm]  (A) {$1$};
            \node[state, black!45!green,line width=0.4mm]  (B) [below = 2cm of A]   {$2$};
            \path[->]
            (A) edge[bend left=75]     node{$0$}	(B)
            (A) edge[bend left, dashed, red]     node{$0$}	(B)
    	    (B) edge[bend left=75]     node{$1$}	(A)
            (B) edge[bend left, dashed, red]     node{$0$}	(A);
        \end{tikzpicture}
        &
        \begin{tikzpicture}[on grid, state/.style={circle,draw}, >= stealth', auto, prob/.style = {inner sep=1pt,font=\scriptsize}]
            \node[state, RoyalBlue,line width=0.4mm]  (A) {$1$};
            \node[state, RoyalBlue,line width=0.4mm]  (B) [below = 2cm of A]   {$2$};
            \path[->]
            (A) edge[bend left=75]     node{$0$}	(B)
            (A) edge[bend left, dashed, red]     node{$0$}	(B)
    	    (B) edge[bend left=75]     node{$1$}	(A)
    	    (B) edge[bend left, dashed, red]     node{$0$}	(A);
        \end{tikzpicture}
        &
        \begin{tikzpicture}[on grid, state/.style={circle,draw}, >= stealth', auto, prob/.style = {inner sep=1pt,font=\scriptsize}]
                \node[state]  (A) {$1,1$};
                \node[state]  (B) [right = 1.5cm of A]   {$1,2$};
                \node[state]  (C) [right = 2cm of B]   {$2,1$};
                \node[state]  (D) [left = 2cm of A]   {$2,2$};
                \path[->]
                (A) edge[bend right, black!45!green, line width=0.4mm]     node[above]{$0$}	(D)
                (A) edge[bend right=80, RoyalBlue, line width=0.4mm]     node[above]{$0$}	(D)
                (B) edge[bend right, black!45!green, line width=0.4mm]     node[below]{$0$}	(C)
                (B) edge[bend right=80, RoyalBlue, line width=0.4mm]     node[below]{$0$}	(C)
                (C) edge[bend right, black!45!green, line width=0.4mm]     node[above]{$0$}	(B)
                (C) edge[bend right=80, RoyalBlue, line width=0.4mm]     node[above]{$0$}	(B)
                % transition of optimal actions
                (D) edge[bend right, black!45!green, line width=0.4mm] node[below]{$1$} (A)
                (D) edge[bend right=80, RoyalBlue, line width=0.4mm] node[below]{$1$} (A);
        \end{tikzpicture}
        \\
            Arm $1$ & Arm $2$ & Global MDP having four states.
        \end{tabular}
        \caption{
            A restless bandit with two arms and exactly one arm is activated at each decision time.
            %Arm $1$ is given and Arm $2$ is identical to Arm $1$ and given in \figurename~\ref{fig:hard_local}.
            The global state is denoted by $(s_1,s_2)$ where $s_1$ is the state of Arm $1$ and $s_2$ of Arm $2$.
            The green arrows show the state transition when activating Arm $1$.
            The blue arrows show the state transiiton when activating Arm $2$.
            The numbers along the green and blue arrows show the expected reward when executing the actions.\\
            For both arms, the black arrows show state transition of action activate and the dashed red one for action rest.
            The numbers along the black and dashed red arrows show the expected reward when executing the actions.
        }
        \label{fig:recur_non_communicate}
    \end{figure}
    \medskip \\
    \textbf{Proof of \ref{thm:ergodic_arms_multichain_RB}} -- It is given by the following counter-example.
    Consider a restless bandit with two 8-state arms presented in \figurename~\ref{fig:ergodic_arm}.
    Both arms are identical in term of state transition that is summarized in Table~\ref{tab:ergodic_arm}.
    We observe that when rest, state $1$ and $2$ have the same possible next states $2$ and $3$.
    Similarly, the pair $\{3,4\}$ has $\{4,5\}$, $\{5,6\}$ has $\{6,7\}$, and $\{7,8\}$ has $\{8,1\}$.
    Similar pattern is observed under action activate: $\{2,3\}$ has $\{3,4\}$, $\{4,5\}$ has $\{5,6\}$, $\{6,7\}$ has $\{7,8\}$, and $\{8,1\}$ has $\{1,2\}$.
    This means that we can construct a cycle of transition by switching between rest (R) and activate (A):
    \begin{align*}
        \{1,2\} \overset{R}{\to} \{2,3\} \overset{A}{\to} \{3,4\} \overset{R}{\to} \{4,5\} \overset{A}{\to} \{5,6\} \overset{R}{\to} \{6,7\} \overset{A}{\to} \{7,8\} \overset{R}{\to} \{8,1\} \overset{A}{\to} \{1,2\}.
    \end{align*}
    With a proper synchronization between the states of Arm $1$ and Arm $2$, we can construct policies that induce two recurrent classes as presented in \figurename~\ref{fig:local_ergodic_multichain_RB}.
    Indeed, consider the following arrangement.
    \begin{align}
        &\text{Arm 1} : \{1,2\} \overset{R}{\to} \{2,3\} \overset{A}{\to} \{3,4\} \overset{R}{\to} \dots \overset{A}{\to} \{1,2\} \nonumber \\
        &\text{Arm 2} : \{4,5\} \overset{A}{\to} \{5,6\} \overset{R}{\to} \{6,7\} \overset{A}{\to} \dots \overset{R}{\to} \{4,5\} \nonumber \\
        &\text{MDP} : \begin{pmatrix}1,5\\2,4\\2,5\end{pmatrix} \overset{2}{\to} \begin{pmatrix}2,6\\3,5\\3,6\end{pmatrix} \overset{1}{\to} \begin{pmatrix}3,7\\4,6\\4,7\end{pmatrix} \overset{2}{\to} \dots \overset{1}{\to} \begin{pmatrix}1,5\\2,4\\2,5\end{pmatrix} \label{eq:mdp_transition}
    \end{align}
    where the symbol $\overset{i}{\to}$ in \Eqref{eq:mdp_transition} means the transition when Arm $i$ is activated.
    It means that any policies that activate Arm $2$ for any odd $s\in[8]$ and rest Arm $2$ for any even $s\in[8]$ when the global MDP is in state $(s,s+4), (s+1,s+3)$, and $(s+1,s+4)$ induce a recurrent class $\gX^1$ as given in \figurename~\ref{fig:local_ergodic_multichain_RB} (note that for $s$ big enough, state $9$ is state $1$, $10$ is $2$, etc).
    The recurrent class $\gX^2$ in \figurename~\ref{fig:local_ergodic_multichain_RB} is constructed by simply changing the sequence of Arm 2's state in the arrangement above and adapting the state of global MDP accordingly:
    \begin{align*}
        &\text{Arm 2} : \{8,1\} \overset{A}{\to} \{1,2\} \overset{R}{\to} \{2,3\} \overset{A}{\to} \dots \overset{R}{\to} \{8,1\} \\
        &\text{MDP} : \begin{pmatrix}1,1\\2,8\\2,1\end{pmatrix} \overset{2}{\to} \begin{pmatrix}2,2\\3,1\\3,2\end{pmatrix} \overset{1}{\to} \begin{pmatrix}3,3\\4,2\\4,3\end{pmatrix} \overset{2}{\to} \dots \overset{1}{\to} \begin{pmatrix}1,1\\2,8\\2,1\end{pmatrix}
    \end{align*}
    So, any policies that activate Arm $2$ for any odd $s\in[8]$ and rest Arm $2$ for any even $s\in[8]$ when the global MDP is in state $(s,s), (s+1,s+7)$, and $(s+1,s)$ induce the class $\gX^2$.
    All in all, any policies that activate Arm $2$ for any odd $s\in[8]$ and rest Arm $2$ for any even $s\in[8]$ when the global MDP is in state $(s,s), (s+1,s+7), (s+1,s), (s,s+4), (s+1,s+3)$ and $(s+1,s+4)$ induce two recurrent classes $\gX^1$ and $\gX^2$.
    That concludes the proof.
    \begin{table}[ht]
        \centering
        \begin{tabular}{|c|c|c|}
            \hline
            Current state & Next state when rest & Next state when activate \\\hline 
            $1$   & $2$ or $3$  &  $1$ or $2$ \\
            $2$   & $2$ or $3$  &  $3$ or $4$ \\
            $3$   & $4$ or $5$  &  $3$ or $4$ \\
            $4$   & $4$ or $5$  &  $5$ or $6$ \\
            $5$   & $6$ or $7$  &  $5$ or $6$ \\
            $6$   & $6$ or $7$  &  $7$ or $8$ \\
            $7$   & $8$ or $1$  &  $7$ or $8$ \\
            $8$   & $8$ or $1$  &  $1$ or $2$ \\ \hline
        \end{tabular}
        \caption{State transition of arms in \figurename~\ref{fig:ergodic_arm}.}
        \label{tab:ergodic_arm}
    \end{table}
    %in \figurename~\ref{fig:local_ergodic_multichain_RB}.
    \begin{figure}
        \centering
        \begin{tabular}{cc}
        \begin{tikzpicture}[on grid, state/.style={ellipse,draw}, >= stealth', auto, prob/.style = {inner sep=1pt,font=\scriptsize}]
            \node[state, black!45!green, line width=0.4mm]  (A) {$8$};
            \node[state, black!45!green, line width=0.4mm]  (B) [right = 1.6cm of A]    {$1$};
            \node[state, black!45!green, line width=0.4mm]  (C) [right = 1.6cm of B]    {$2$};
            \node[state, black!45!green, line width=0.4mm]  (D) [below = 1.6cm of C]    {$3$};
            \node[state, black!45!green, line width=0.4mm]  (E) [below = 1.6cm of D]    {$4$};
            \node[state, black!45!green, line width=0.4mm]  (F) [left = 1.6cm of E]    {$5$};
            \node[state, black!45!green, line width=0.4mm]  (G) [left = 1.6cm of F]    {$6$};
            \node[state, black!45!green, line width=0.4mm]  (H) [below = 1.6cm of A]    {$7$};
            \path[->]
                (A) edge[red, dashed, bend right=30] node{} (B)
                edge[red, dashed, loop above] node{} (A)
                (B) edge[red, dashed, bend right=30] node{} (C)
                edge[red, dashed, bend right=30] node{} (D)
                (C) edge[red, dashed, bend right=30] node{} (D)
                edge[red, dashed, loop above] node{} (C)
                (D) edge[red, dashed, bend right=30] node{} (E)
                edge[red, dashed, bend right=30] node{} (F)
                (E) edge[red, dashed, bend right=30] node{} (F)
                edge[red, dashed, loop below] node{} (E)
                (F) edge[red, dashed, bend right=30] node{} (G)
                edge[red, dashed, bend right=30] node{} (H)
                (G) edge[red, dashed, bend right=30] node{} (H)
                edge[red, dashed, loop below] node{} (G)
                (H) edge[red, dashed, bend right=30] node{} (A)
                edge[red, dashed, bend right=30] node{} (B)

                (A) edge[bend left=30]     node{}	(B)
                edge[bend left=50]     node{}	(C)
                (B) edge[loop above] node{} (B)
                edge[bend left=30]     node{}	(C)
                (C) edge[bend left=30]     node{}	(D)
                edge[bend left=50]     node{}	(E)
                (D) edge[loop right] node{} (D)
                edge[bend left=30]     node{}	(E)
                (E) edge[bend left=30]     node{}	(F)
                edge[bend left=50]     node{}	(G)
                (F) edge[loop below] node{} (F)
                edge[bend left=30]     node{}	(G)
                (G) edge[bend left=30]     node{}	(H)
                edge[bend left=50]     node{}	(A)
                (H) edge[loop left] node{} (H)
                edge[bend left=30]     node{}	(A);
        \end{tikzpicture} 
        &
        \begin{tikzpicture}[on grid, state/.style={ellipse,draw}, >= stealth', auto, prob/.style = {inner sep=1pt,font=\scriptsize}]
            \node[state, RoyalBlue,line width=0.4mm]  (A) {$8$};
            \node[state, RoyalBlue,line width=0.4mm]  (B) [right = 1.6cm of A]    {$1$};
            \node[state, RoyalBlue,line width=0.4mm]  (C) [right = 1.6cm of B]    {$2$};
            \node[state, RoyalBlue,line width=0.4mm]  (D) [below = 1.6cm of C]    {$3$};
            \node[state, RoyalBlue,line width=0.4mm]  (E) [below = 1.6cm of D]    {$4$};
            \node[state, RoyalBlue,line width=0.4mm]  (F) [left = 1.6cm of E]    {$5$};
            \node[state, RoyalBlue,line width=0.4mm]  (G) [left = 1.6cm of F]    {$6$};
            \node[state, RoyalBlue,line width=0.4mm]  (H) [below = 1.6cm of A]    {$7$};
            \path[->]
                (A) edge[red, dashed, bend right=30] node{} (B)
                edge[red, dashed, loop above] node{} (A)
                (B) edge[red, dashed, bend right=30] node{} (C)
                edge[red, dashed, bend right=30] node{} (D)
                (C) edge[red, dashed, bend right=30] node{} (D)
                edge[red, dashed, loop above] node{} (C)
                (D) edge[red, dashed, bend right=30] node{} (E)
                edge[red, dashed, bend right=30] node{} (F)
                (E) edge[red, dashed, bend right=30] node{} (F)
                edge[red, dashed, loop below] node{} (E)
                (F) edge[red, dashed, bend right=30] node{} (G)
                edge[red, dashed, bend right=30] node{} (H)
                (G) edge[red, dashed, bend right=30] node{} (H)
                edge[red, dashed, loop below] node{} (G)
                (H) edge[red, dashed, bend right=30] node{} (A)
                edge[red, dashed, bend right=30] node{} (B)

                (A) edge[bend left=30]     node{}	(B)
                edge[bend left=50]     node{}	(C)
                (B) edge[loop above] node{} (B)
                edge[bend left=30]     node{}	(C)
                (C) edge[bend left=30]     node{}	(D)
                edge[bend left=50]     node{}	(E)
                (D) edge[loop right] node{} (D)
                edge[bend left=30]     node{}	(E)
                (E) edge[bend left=30]     node{}	(F)
                edge[bend left=50]     node{}	(G)
                (F) edge[loop below] node{} (F)
                edge[bend left=30]     node{}	(G)
                (G) edge[bend left=30]     node{}	(H)
                edge[bend left=50]     node{}	(A)
                (H) edge[loop left] node{} (H)
                edge[bend left=30]     node{}	(A);
            \end{tikzpicture} \\
            Arm $1$ & Arm $2$
        \end{tabular}
        \caption{
            Two restless Markovian arms, each having $8$ states.
            Both arms are identical in term of transition structure.
            The black arrows show state transition of action activate and the dashed red one for action rest.
            To ease the exposition, the expected reward is not shown.
            Also, each state transition happens with probability $0.5$.
            That is, each arm goes from states $1$ and $2$ to state $2$ or $3$, states $3$ and $4$ to state $4$ or $5$, etc, under action rest, and goes from states $1$ and $8$ to state $1$ or $2$, etc, under action activate.
            We provide a list of transition in Table~\ref{tab:ergodic_arm}.
            The restless bandit having these two arms and exactly one arm is activated at each decision time is multichain because some policies induce two classes of recurrent states as shown in \figurename~\ref{fig:local_ergodic_multichain_RB}.
        }
        \label{fig:ergodic_arm}
    \end{figure}
    \begin{figure}
        \centering
        \begin{tabular}{c}
        \begin{tikzpicture}[on grid, state/.style={ellipse,draw}, >= stealth', auto, prob/.style = {inner sep=1pt,font=\scriptsize}]
            \node[state]  (A) {$\begin{tabular}{c}1,5\\2,4\\2,5\end{tabular}$};
            \node[state]  (B) [right = 2.4cm of A]    {$\begin{tabular}{c}2,6\\3,5\\3,6\end{tabular}$};
            \node[state]  (C) [right = 2.4cm of B]    {$\begin{tabular}{c}3,7\\4,6\\4,7\end{tabular}$};
            \node[state]  (D) [right = 2.4cm of C]    {$\begin{tabular}{c}4,8\\5,7\\5,8\end{tabular}$};
            \node[state]  (E) [below = 3cm of D]    {$\begin{tabular}{c}5,1\\6,8\\6,1\end{tabular}$};
            \node[state]  (F) [left = 2.4cm of E]    {$\begin{tabular}{c}6,2\\7,1\\7,2\end{tabular}$};
            \node[state]  (G) [left = 2.4cm of F]    {$\begin{tabular}{c}7,3\\8,2\\8,3\end{tabular}$};
            \node[state]  (H) [left = 2.4cm of G]    {$\begin{tabular}{c}8,4\\1,3\\1,4\end{tabular}$};
            \node[text width=2cm] (J) [left = 1.5cm of A] {Recur.\\Class $\gX^1$};
            \path[->]
            (A) edge[RoyalBlue,line width=0.4mm]     node{}	(B)
            (B) edge[black!30!green,line width=0.4mm]     node{}	(C)
            (C) edge[RoyalBlue,line width=0.4mm]     node{}	(D)
            (D) edge[black!30!green,line width=0.4mm]     node{}	(E)
    	    (E) edge[RoyalBlue,line width=0.4mm]     node{}	(F)
    	    (F) edge[black!30!green,line width=0.4mm]     node{}	(G)
            (G) edge[RoyalBlue,line width=0.4mm]     node{}	(H)
            (H) edge[black!30!green,line width=0.4mm]     node{}	(A);
        \end{tikzpicture} \\ \hfill\\
        \begin{tikzpicture}[on grid, state/.style={ellipse,draw}, >= stealth', auto, prob/.style = {inner sep=1pt,font=\scriptsize}]
            \node[state]  (A) {$\begin{tabular}{c}1,1\\2,8\\2,1\end{tabular}$};
            \node[state]  (B) [right = 2.4cm of A]    {$\begin{tabular}{c}2,2\\3,1\\3,2\end{tabular}$};
            \node[state]  (C) [right = 2.4cm of B]    {$\begin{tabular}{c}3,3\\4,2\\4,3\end{tabular}$};
            \node[state]  (D) [right = 2.4cm of C]    {$\begin{tabular}{c}4,4\\5,3\\5,4\end{tabular}$};
            \node[state]  (E) [below = 3cm of D]    {$\begin{tabular}{c}5,5\\6,4\\6,3\end{tabular}$};
            \node[state]  (F) [left = 2.4cm of E]    {$\begin{tabular}{c}6,6\\7,5\\7,4\end{tabular}$};
            \node[state]  (G) [left = 2.4cm of F]    {$\begin{tabular}{c}7,7\\8,6\\8,4\end{tabular}$};
            \node[state]  (H) [left = 2.4cm of G]    {$\begin{tabular}{c}8,8\\1,7\\1,5\end{tabular}$};
            \node[text width=2cm] (J) [left = 1.5cm of A] {Recur.\\Class $\gX^2$};
            \path[->]
            (A) edge[RoyalBlue,line width=0.4mm]     node{}	(B)
            (C) edge[RoyalBlue,line width=0.4mm]     node{}	(D)
    	    (E) edge[RoyalBlue,line width=0.4mm]     node{}	(F)
            (G) edge[RoyalBlue,line width=0.4mm]     node{}	(H)
            (B) edge[black!30!green,line width=0.4mm]     node{}	(C)
            (D) edge[black!30!green,line width=0.4mm]     node{}	(E)
    	    (F) edge[black!30!green,line width=0.4mm]     node{}	(G)
            (H) edge[black!30!green,line width=0.4mm]     node{}	(A);
        \end{tikzpicture}
        \end{tabular}
        \caption{
            State transition within two recurrent classes of a restless with 2 arms presented in \figurename~\ref{fig:ergodic_arm}.
            The first recurrent class is $\gX^1 =\{(s,s+4),(s+1,s+3),(s+1,s+4): s\in[8]\}$ and the second one is $\gX^2 =\{(s,s),(s+1,s),(s+1,s+7): s\in[8]\}$.
            These are two recurrent classes under any policies that activate Arm 2 for any odd $s\in[8]$ and Arm 1 for any even $s\in[8]$ when the bandit is in states $(s,s),(s,s+4),(s+1,s),(s+1,s+3),(s+1,s+4),(s+1,s+7)$.
            The blue arrows show the state transition when Arm $2$ is activated and the green ones show the one when Arm $1$ is activated.
            Each ellipse regroups the states having the same state transition under the same action.
            Note that each ellipse can transition to itself like in $\gX^1$ state $(2,4)$ can transition to state $(2,5)$ but it not shown.
            The rest of the states $\gX\setminus(\gX^1\cup\gX^2)$ are transient.
        }
        \label{fig:local_ergodic_multichain_RB}
    \end{figure}
\end{proof}

Theorem~\ref{thm:multichain} means that even though all arms are ergodic, the diameter or the mixing time of the global MDP are infinite or undefined.
%That is, the global MDP is multichain and weakly communicating.
%The following theorem shows that having the global MDP that is ergodic 

\begin{thm}
    For restless bandit with finite number of arms, each arm has a finite number of states,
    \begin{enumerate}[label=(\roman*)]
        \item \label{thm:diam} there exists a bandit having $n$ 2-state arms, each arm has the diameter bounded by $2$, that is ergodic and has a diameter bounded by $2^n$. % the term noncommunicating is taken from [Puterman, 1994, page 353]. 
        %\item \label{thm:aperiodic_RB_ergo} There exists a Restless bandit whose arms are all ergodic that is not recurrent. 
        \item \label{thm:mixing} there exists a bandit, whose arms all are ergodic with bounded mixing time, that is ergodic and has a mixing time in the same order of $\landauO(1/\varepsilon)$ for any $\varepsilon\in(0,1)$.
        \item \label{thm:span} there exists a bandit, whose arms all have a bounded local span of bias of any local policy, that is communicating and has a global span of optimal bias in the same order of $\landauO(1/\varepsilon)$ for any $\varepsilon\in(0,1)$.
    \end{enumerate}
\end{thm}
\begin{proof}
    \textbf{Proof of \ref{thm:diam}} -- consider an example of $n$ arms, each arm is a MDP with state space $\{1,2\}$ and has $0.5$ probability of changing state under both actions.
    It should be clear the global MDP is ergodic.
    The probability that the global MDP goes from state $(1,\dots,1)$ to state $(2,\dots,2)$ is $(1/2)^n$.
    Hence, the diameter of the global MDP is $2^n$.
    \medskip \\

    \textbf{Proof of \ref{thm:mixing}} -- Let $p_1$ and $p_2$ be the state transition functions of Arm $1$ and Arm $2$ given in \figurename~\ref{fig:ergodic_arm} respectively.
    $p_1$ and $p_2$ are both ergodic and induce the same state transition that is given in Table~\ref{tab:ergodic_arm}.
    Suppose that the mixing time of $p_1$ and $p_2$ is $t_{mix}>1$.
    Note that we can specify $p_1$ and $p_2$ such that $t_{mix}$ is small.
    Let's denote the bandit having two arms $p_1$ and $p_2$ by $M$.
    We have seen that $M$ is multichain.
    So the mixing time of $M$ is infinite.

    Now, consider two 8-state restless arms having transition function $u_1$ and $u_2$ where $u_1$ and $u_2$ induce the same state transition: any state $s\in[8]$ transitions to any other state $s'\in[8]$ with probability $\frac18$ under either action rest or activate.
    It is then clear that $u_1$ and $u_2$ are both ergodic and have the mixing time equals $1$.
    Any bandit having two arms $u_1$ and $u_2$ is an ergodic global MDP.

    For any $\varepsilon\in(0,1)$, consider two 8-state arms having transition function $p'_1=(1-\varepsilon)p_1+\varepsilon u_1$ and $p'_2=(1-\varepsilon)p_2+\varepsilon u_2$.
    That is, at each state transition, we toss a two-face coin whose head probability is $\varepsilon$.
    If the coin heads up, the arm evolves like $u$.
    If the coin tails up, the arm evoles like $p$.
    So, $p'_1$ and $p'_2$ are both ergodic.
    We know that if $\varepsilon=0$, the mixing time of $p'_1$ and $p'_2$ is $t_{mix}$ and if $\varepsilon=1$, the mixing time is $1$.
    Then, there exists $\varepsilon_0>0$ such that the mixing time of $p'_1$ and $p'_2$ is $t_{mix}/2$.
    For any $\varepsilon\in(0,\varepsilon_0]$, any bandit having two arms $p'_1$ and $p'_2$ is an ergodic global MDP denoted by $M'(\varepsilon)$.
    So, the mixing time of $M'(\varepsilon)$ is defined.
    However, when $\varepsilon\to0$, $M'(\varepsilon)\to M$.
    This is equivalent to say that when $\varepsilon\to0$, the mixing time of $M'(\varepsilon)$ tends to infinity.
    \medskip \\

    \textbf{Proof of \ref{thm:span}} -- We use the same technique in the proof of \ref{thm:mixing}.
    First of all, consider the bandit having $4$ global states as in Figure~\ref{fig:recur_non_communicate}.
    We denote this bandit by $M$ and the arms' transition functions by $p_1$ and $p_2$ and reward functions by $r_1$ and $r_2$.
    As we mentioned above, the local span of optimal bias of each arm is $0.5$ but $M$ is multichain.
    So, the global span of optimal bias is infinite.

    Consider now two two-state arms with reward functions $r_1$ and $r_2$, and transition functions $u_1$ and $u_2$ that induce the same state transition: any state $s\in[2]$ transitions to any other state $s'\in[2]$ with probability $\frac12$.
    So, $u_1$ and $u_2$ are both ergodic and the local span of optimal bias is simply $1$.

    With the same technique above, for any $\varepsilon\in(0,1)$, consider two two-state arms having reward functions $r_1$ and $r_2$, and transition functions $p'_1=(1-\varepsilon)p_1+\varepsilon u_1$ and $p'_2=(1-\varepsilon)p_2+\varepsilon u_2$.
    So, for any $\varepsilon\in(0,1)$, $p'_1$ and $p'_2$ are both ergodic and have the local span of optimal bias equals $\displaystyle\frac{1}{2-\varepsilon}$.
    We denote any bandit having two arms $(r_1,p'_1)$ and $(r_2,p'_2)$ by $M'(\varepsilon)$.
    We will see in Theorem~\ref{thm:aperiodic_RB_comm} that for any $\varepsilon\in(0,1)$, $M'(\varepsilon)$ is a communicating MDP.
    However, when $\varepsilon\to0$, $M'(\varepsilon)\to M$.
    This is equivalent to say that when $\varepsilon\to0$, the global span of optimal bias of $M'(\varepsilon)$ tends to infinity.
\end{proof}

\subsection{Positive results}

In this section, we show a few results in which the assumption on local arms implies ``positive'' properties on the global MDP.
The term ``positive'' refers to the fact that the properties frequently required by the learning algorithms for generic MDPs are satisfied.
This term does not imply neither the learning algorithm achieves sublinear expected regret nor their regret bounded is explicitly linear in the number of arms.

\begin{thm}
    \label{thm:aperiodic_RB_comm} A restless bandit whose arms all are ergodic is a communicating MDP.
\end{thm}
\begin{proof}
    We prove the theorem by its contraposition. Assume that a given global MDP is not communicating (either weakly communicating or not weakly communicating).
    In this MDP, there are global states that are not reachable from each other.
    This implies to two possibilities: (1) for some arms, some local states are not recurrent, (2) all arms are recurrent but periodic.
    Each possibility implies that there exists at least one arm that is not ergodic.
\end{proof}
This theorem means that if all arms are ergodic, then the global MDP has a finite diameter.
The following theorem also provides a result on the structure of global MDP.

\begin{thm}
    \label{thm:unichain}
    For restless bandit with finite number of arms, if all arms are unichain with at least one state reachable in one time step from any other states under both rest and activate actions, then the corresponding global MDP is also unichain with at least one global state reachable in one time step from any other global states under any policies.
\end{thm}
\begin{proof}
    Suppose that the bandit have $n$ arms and exactly $m$ arms are activated at each time step.
    For each arm $i\in[n]$, let $z_i$ be the state that is reachable in one time step from any other states under both rest and activate actions.
    We have that $p(z_i \mid s_i, a_i)>0$ for all $i\in[n], x_i\in\gS_i,a_i\in\{0,1\}$.
    For any action $\va\in\gA(m)$, any state $\vs,\vs'\in\gX,$ the global MDP $M$'s transition is given by ${p(\vs' \mid \vs, \va) =\prod_{i=1}^n p(s'_i \mid s_i, a_i)}$.
    Then, for any action $\va\in\gA(m)$, any state $\vs\in\gX$, we have
    \begin{align*}
        p(\vz \mid \vs, \va) =\prod_{i=1}^n p(z_i \mid s_i,a_i) > 0,
    \end{align*}
    because $n$ is finite and for any $i$, $p(z_i \mid s_i, a_i)>0$.
    Hence, the global state $\vz$ is reachable from any other global states in one step under any policies.
    In consequence, no policies induce a global Markov chain that has multiple closed irreducible recurrent classes.
    %there cannot be multiple recurrent classes that are closed under any policies.
    That concludes the proof.
\end{proof}

Last but not least, we show how the assumption on the ergodicity coefficient of local arms affects the ergodicity coefficient of the global MDP.
\begin{thm}
    \label{thm:ergodicity_coeff}
    %Given a restless bandit $M$ 
    For each arm $i\in[n]$, the ergodicity coefficient $\gamma_i$ of the arm is defined by
    \begin{align*}
        \gamma_i = 1-\min_{\substack{s_i,s'_i\in\gS_i \\a,a'\in\{0,1\}}} \sum_{z_i\in\gS_i} \min\{p(z_i \mid x_i, a), p(z_i \mid y_i, a')\}.
    \end{align*}
    Similarly, for any bandit with $n$ arms and any $m\in[n]$, the ergodicity coefficient $\Gamma$ of the bandit is defined by
    \begin{align*}
        \Gamma = 1-\min_{\substack{\vs,\vs'\in\gX \\\va,\va'\in\gA(m)}} \sum_{\vz\in\gX} \min\{p(\vz \mid \vs, \va), p(\vz \mid \vs', \va')\}.
    \end{align*}
    If $\gamma_i<1$ for any arm $i$, then $\Gamma<1$.

    Moreover, if there exists $\varepsilon>0$ such that for any arm $i\in[n]$,
    \begin{equation}
        \label{eq:gamma_ep}
        %\min_{\substack{s_i,s'_i\in\gS_i \\a,a'\in\{0,1\}}} \sum_{z_i\in\gS_i} \min\{p(z_i \mid s_i, a), p(z_i \mid s'_i, a')\} \ge \varepsilon,
        \gamma_i \le 1-\varepsilon,
    \end{equation}
    then $\Gamma \le 1-\varepsilon^n$.
\end{thm}
Hence, for the restless bandit $M$ described in Section~\ref{ch:restless:sec:restless}, if \eqref{eq:gamma_ep} holds for all $i\in[n]$, then the global span of optimal bias is bounded like 
\begin{align}
    sp(\vh^*_M)\le \frac{r_{max}}{\varepsilon^n} \label{eq:span_expo}
\end{align}
where $\vh^*_M$ is the optimal bias of the global MDP $M$.
Indeed, if \eqref{eq:gamma_ep} holds for all $i\in[n]$, then $\Gamma \le 1-\varepsilon^n$ and the global MDP $M$ is unichain.
Let $\pi^*$ be an optimal policy and $\vr^{\pi^*}$ and $\mP^{\pi^*}$ be the expected reward vector and transition matrix under policy $\pi^*$.
For any global state $\vs\in\gX$, $r^{\pi^*}(\vs)\in[0,r_{max}]$.
Then, the optimal gain $g^*_M$ is bounded in $[0, r_{max}]$.
We recall that the Bellman evaluation equation for unichain policy $\pi^*$ in vector form is $\vh^*=\vr^{\pi^*} - g^*\vone +\mP^{\pi^*}\vh^*$.
Then, $sp(\vh^*_M)\le r_{max} +sp(\mP^{\pi^*}\vh^*_M)\le r_{max} +\Gamma sp(\vh^*_M)$ because $sp(\mP^{\pi^*}\vh^*_M)\le \Gamma sp(\vh^*_M)$.
So, $sp(\vh^*_M)\le r_{max}/\varepsilon^n$.

\begin{proof}
    For any two global states $\vs,\vs'\in\gX$ and any two actions $\va,\va'\in\gA(m)$, we have
    \begin{align*}
        \sum_{\vz\in\gX} \min\{p(\vz \mid \vs, \va), p(\vz \mid \vs', \va')\}
        &= \sum_{\vz\in\gX} \min\left\{\prod_{i=1}^np(z_i \mid s_i, a_i), \prod_{i=1}^np(z_i \mid s'_i, a'_i)\right\} \\
        &\ge \sum_{\vz\in\gX}\prod_{i=1}^n \min\{p(z_i \mid s_i, a_i), p(z_i \mid s'_i, a'_i)\} \\
        &= \prod_{i=1}^n \left(\sum_{z_i\in\gS_i} \min\{p(z_i \mid s_i, a_i), p(z_i \mid s'_i, a'_i)\}\right).
    \end{align*}
    %\KK{Add more detail about the second equality}
    Since for any $i\in[n]$, $\gamma_i<1$, we have that
    \begin{align*}
        \sum_{z_i\in\gS_i} \min\{p(z_i \mid s_i, a_i), p(z_i \mid s'_i, a'_i)\}
        &\ge \min_{\substack{x_i,y_i\in\gS_i \\a,a'\in\{0,1\}}} \sum_{z_i\in\gS_i} \min\{p(z_i \mid x_i, a), p(z_i \mid y_i, a')\} \\
        &= 1-\gamma_i >0.
    \end{align*}
    In consequences, ${\sum_{\vz\in\gX} \min\{p(\vz \mid \vs, \va), p(\vz \mid \vs', \va')\}{>}0}$.
    We conclude that $\Gamma{<}1$.\\
    Moreover, if there exists $\varepsilon>0$ such that \eqref{eq:gamma_ep} holds for any $i\in[n]$, then it follows that
    \begin{align*}
        \prod_{i=1}^n \left(\sum_{z_i\in\gS_i} \min\{p(z_i \mid s_i, a_i), p(z_i \mid s'_i, a'_i)\}\right)
        &\ge \varepsilon^n.
    \end{align*}
    We can conclude that $\Gamma\le 1-\varepsilon^n$ using its definition.
\end{proof}


%\subsection{Regret definition and required assumptions}
\section{Restless bandit as a generic MDP}
\label{ch:restless:sec:generic}

\KK{I have not found a better title yet.}

By Definition~\ref{ch:rl:defn:rg_infinite}, the regret of a learning algorithm $\gL$ after $T$ time steps in a restless bandit $M$ that is weakly communicating is given by
\begin{align}
    \Reg(\gL,M,T) := Tg^*_M - \sum_{t=1}^{T} \sum_{i=1}^nr_{t,i}. \label{eq:regret}
\end{align}
%This definition implicitly requires that the unknown MDP $M$ is weakly communicating and the reward is bounded.
%This definition implicitly requires that the reward is bounded and the optimal gain $g^*$ is state-independent.

For Bayesian learning algorithm $\gL$, the prior distribution $\phi$ of $M$ is chosen by  $\gL$.
We recall the Bayesian regret of $\gL$ from \eqref{eq:bayes_rg}.
\begin{align}
    \BayReg(\gL, \phi, T) := \ex{\ex{\Reg(\gL, M, T) \mid M}}.
\end{align}

In the following, we present the framework of UCRL2 to bound the regret.
%Then, we go through the assumptions required for such methodology.

\subsection{UCRL2 framework}

The algorithm updates its policy in episodic manner using doubling trick.
Let $K_T$ be the total number of episodes (or policy updates) up to time $T$.
Let $t^k$ be the time step when episode $k$ begins with the convention that $t^1:=1$.
Let $M^k$ be the imagined global MDP of the algorithm for episode $k$ and $\pi^k$ be the policy used in episode $k$.
We require $\pi^k$ to be unichain to induce state-independent gain and the Bellman evaluation equation for $\pi^k$ in the imagined MDP $M^k$: for global state $\vs\in\gX$, $\va=\pi^k(\vs)$,
\begin{align*}
    0 =  \sum_{i=1}^nr^k(s_i, a_i) -g^{\pi^k}_{M^k} +\sum_{\vs'\in\gX}p^k(\vs' \mid \vs, \va)h^{\pi^k}_{M^k}(\vs') -h^{\pi^k}_{M^k}(\vs).
\end{align*}
Then,
\begin{align*}
    \Reg(\gL, M, T)
    &= \sum_{k=1}^{K_T}\sum_{t=t^k}^{t^{k+1}-1} \Bigl(g^*_M -\sum_{i=1}^{n}r(s_{t,i},a_{t,i})\Bigr) +\underbrace{\sum_{i=1}^{n}\Bigl(r(s_{t,i},a_{t,i})-r_{t,i}\Bigr)}_{\Delta_{t,0}} \\
    &=\sum_{k=1}^{K_T}\sum_{t=t^k}^{t^{k+1}-1} \underbrace{(g^*_M{-}g^{\pi^k}_{M^k})}_{\Delta_{t,1}} {+}\underbrace{\sum_{i=1}^n(r^k{-}r)(s_{t,i},a_{t,i})}_{\Delta_{t,2}} \\
    &\quad {+}\underbrace{\sum_{\vs'\in\gX}(p^k-p)(\vs' \mid \vs_t, \va_t)h^{\pi^k}_{M^k}(\vs')}_{\Delta_{t,3}} {+}\underbrace{h^{\pi^k}_{M^k}(\vs_{t+1}){-}h^{\pi^k}_{M^k}(\vs_{t})}_{\Delta_{t,4}} \\
    &\quad {+}\underbrace{\sum_{\vs'\in\gX} p(\vs' \mid \vs_t, \va_t)h^{\pi^k}_{M^k}(\vs')-h^{\pi^k}_{M^k}(\vs_{t+1})}_{\Delta_{t,5}} +\Delta_{t,0}.
\end{align*}
The six quantities are bounded differently according to the approach used.

\subsubsection{UCRL2 algorithm}

The assumption used in UCRL2 is that the unknown MDP $M$ is \textbf{communicating} (otherwise, the diameter is infinite) and has bounded diameter $D$ and bounded non-negative reward.
This assumption implies that the optimal gain $g^*_M$ is state-independent and the regret is well defined.
It is shown in \cite{jaksch2010near} that using UCRL2 algorithm provides the following: for each episode $k\ge1$,
\begin{itemize}
    \item the imagined MDP $M^k$ chosen by extended value iteration is communicating
    \item the gain $g^{\pi^k}_{M^k}$ is state-independent
    \item the diameter of $M^k$ is bounded, $D^k\le D$
    \item the span of bias of policy $\pi^k$ in $M^k$ is bounded, $sp(\vh^{\pi^k}_{M^k})\le D^k\le D$.
\end{itemize}

%The second item is true because the set of plausible MDPs $\gM^k$ can be considered as a MDP with finitely many actions.
%If $M$ belongs to $\gM^k$, then $\gM^k$ is also \emph{communicating}.
%$g^k$ can be considered as the optimal gain of this finitely many actions MDP $\gM^k$.
%So, $g^k$ is state-independent.
%The third item is true because $M$ belongs to $\gM^k$ together with $\gM^k$ is a MDP with finitely many actions imply that the diameter of $\gM^k$ is upper bounded by the diameter of $M$.
%Under the assumption, \cite[Theorem~4]{bartlett2012regal} implies that the span of the optimal bias (the bias that verifies Bellman optimality equation) is bounded by the diameter of the MDP times the optimal gain.

So, for UCRL2, the six quantities above are bounded as the following.
\begin{itemize}
    \item By martingale argument and Azuma-Hoeffding's inequality for martingale difference sequence,
        \begin{itemize}
            \item the sum $\sum_{k=1}^{K_T}\sum_{t=t^k}^{t^{k+1}-1}\Delta_{t,0}$ is bounded by $\landauO(r_{max}\sqrt{T\ln(T)})$ with high probability
            \item the sum $\sum_{k=1}^{K_T}\sum_{t=t^k}^{t^{k+1}-1}\Delta_{t,5}$ is bounded by $\landauO(D\sqrt{T\ln(T)})$ with high probability
        \end{itemize}
    \item \label{it:optimism} The term $\Delta_{t,1}$ is non-positive thanks to the optimism
    \item The term $\Delta_{t,2}$ is bounded by Hoeffding's inequality (or simply zero if the rewards are deterministic);
    \item The telescopic sum $\sum_{k=1}^{K_T}\sum_{t=t^k}^{t^{k+1}-1} \Delta_{t,4}$ is bounded by $\sum_{k=1}^{K_T} sp(\vh^{\pi^k}_{M^k}){\le} DK_T$
    \item Finally, the term $\Delta_{t,3}$ is bounded by $sp(\vh^{\pi^k}_{M^k}) \norm{\vp^k-\vp}_{\ell_1}\le D\norm{\vp^k-\vp}_{\ell_1}$ and $\norm{\vp^k-\vp}_{\ell_1}$ by Weissman's inequality.
\end{itemize}
Moreover, by using doubling trick, \cite[Proposition~18]{jaksch2010near} gives, for any $T\ge \abs{\gX}\abs{\gA(m)}$, $K_T\le \abs{\gX}\abs{\gA(m)}\log_2(\frac{8T}{\abs{\gX}\abs{\gA(m)}})$.

Hoeffding's and Weissman's inequalities induce another term of the form $\frac{1}{\sqrt{n^k(\vs_t,\va_t)}}$.
The sum $\sum_{k=1}^{K_T}\sum_{t=t^k}^{t^{k+1}-1}\frac{1}{\sqrt{n^k(\vs_t,\va_t)}}$ is bounded by $\landauO(\sqrt{\abs{\gX}\abs{\gA(m)}T})$ similarly to what we have seen in Chapter~\ref{ch:rested}. %Section~\ref{sec:} of Chapter~\ref{ch:rested}.

\subsubsection{TSDE algorithm}

TSDE is a Bayesian algorithm. So, it starts by choosing a prior distribution $\phi$ about the unknown MDP $M$.
Let $\mathrm{supp}(\phi)$ denote the support of the distribution $\phi$.
So, $\phi$ is chosen such that $M\in\mathrm{supp}(\phi)$.
The assumptions used for TSDE are the expected reward of $M$ is non-negative and bounded, and the following.
\begin{itemize}
    \item $\mathrm{supp}(\phi)$ is a set of \textbf{weakly communicating} MDPs
    \item there exists $H<+\infty$ such that any MDP in $\mathrm{supp}(\phi)$ has the span of optimal bias bounded by $H$.
\end{itemize}
These assumptions imply that for each episode $k\ge1$,
\begin{itemize}
    \item the imagined MDP $M^k$ sampled from the posterior is weakly communicating
    \item the gain $g^{\pi^k}_{M^k}$ is state-independent
    \item the span of bias of policy $\pi^k$ in $M^k$ is bounded by $H$, $sp(\vh^{\pi^k}_{M^k})\le H$.
\end{itemize}
So, for TSDE, the six quantities above are bounded as the following.
\begin{itemize}
    \item $\Delta_{t,0}$ and $\Delta_{t,5}$ are martingale difference term whose expected values are zero
    \item The expected value of $\sum_{k=1}^{K_T}\sum_{t=t^k}^{t^{k+1}-1}\Delta_{t,1}$ is bounded by the upper bound of $K_T$ times $r_{max}$
    \item The second term $\Delta_{t,2}$ is bounded by Hoeffding's inequality (or simply zero if the rewards are deterministic);
    \item The telescopic sum $\sum_{k=1}^{K_T}\sum_{t=t^k}^{t^{k+1}-1} \Delta_{t,4}$ is bounded by $HK_T$
    \item Finally, the term $\Delta_{t,3}$ is bounded by $H\norm{\vp^k-\vp}_{\ell_1}$ and $\norm{\vp^k-\vp}_{\ell_1}$ by Weissman's inequality.
\end{itemize}

Moreover, the doubling trick of TSDE gives $K_T\le \sqrt{2\abs{\gX}\abs{\gA(m)}T\ln(T)}$ \cite[Lemma~1]{ouyang2017learning}.
The term induced by Hoeffding's and Weissman's inequalities is bound exactly the same as UCRL2 does.

So, directly apply UCRL2 or TSDE would incur a few problems
\begin{itemize}
    \item the assumptions on global MDP may be computationally infeasible to check
    \item the regret bound is exponential in the number of arms $n$.
\end{itemize}
Broadly speaking, the regret bound is exponential in $n$ because
\begin{itemize}
    \item the total number of episodes $K_T$ is in function of $S^n{n \choose m}$
    \item the concentration of global transition $\norm{\vp^k-\vp}_{\ell_1}$ is in function of $\sqrt{S^n}$.
\end{itemize}

\section{Exploit restless bandit's structure in learning}
\label{ch:restless:sec:structure}

\subsection{Assumptions on local arms}

As we have mentioned, computing an optimal policy in restless bandit is PSPACE-hard \cite{papadimitriou1994complexity}.
The computationally efficient policy known is Whittle index policy which is asymptotically optimal under some technical conditions \cite{weber1990index}.
However, Whittle index policy is not optimal in general.
This brings a discussion about what baseline should be used in the regret definition.
\begin{itemize}
    \item Shall we compare the algorithm to an oracle that knows an optimal policy of any restless bandit?
    \item Shall we use Whittle index policy as a baseline?
\end{itemize}
Assuming an oracle is what is done in \cite{osband2014near, rosenberg2020oracle, xu2020reinforcement} when learning in factored-MDPs.
We will then focus the discussion on the second question.

First of all, Whittle index policy is defined only if bandit is indexable.
Also, since Whittle index policy is not optimal, the gain under this policy can be state-dependent.
So, we need to assume that the unknown MDP is unichain and indexable.

For optimism, we have shown in Chapter~\ref{ch:rested} that optimistic algorithms that apply confidence bonus on arm's transition cannot leverage index policy to choose an imagined MDP $M^k$ with index policy $\pi^k$ that guarantees the optimism.
Aggravatingly, there is no guarantee that the imagined MDP $M^k$ is indexable, let alone the optimism.
%So, an assumption that there exists an oracle that knows how to choose $M^k$ that is indexable
So, we believe that using the Whittle index policy as a baseline is not practical for the optimism approach.

For posterior sampling, we need the imagined MDP $M^k$ to be indexable.
This can be done by making an assumption on the support of the prior distribution.
That is, any MDP in the support $\mathrm{supp}(\phi)$ is unichain and indexable.

\subsection{Techniques to reduce exponentiality in the number of arms}

An alternative doubling trick adapted to restless Markovian bandit is proposed in \cite{ortner2012regret, jung2019thompson, akbarzadeh2022learning} so that the total number of episodes is bounded linearly in $n$ (see \eg, \cite[Lemma~A.1]{akbarzadeh2022learning} or \cite[Lemma~8]{jung2019thompson}).

Also, for any global state $\vs\in\gX$ and global action $\va\in\gA(m)$, it follows from \cite[Lemma~13]{jung2019thompson} that
\begin{align*}
    \sum_{\vs'\in\gX} \abs{p^k(\vs'\mid \vs,\va) -p(\vs'\mid \vs,\va)}
    \le \sum_{i=1}^{n} \sum_{s'_i\in\gS_i} \abs{p^k(s'_i\mid s_i,a_i) -p(s'_i\mid s_i,a_i)}
\end{align*}
where $\sum_{s'_i\in\gS_i} \abs{p^k(s'_i\mid s_i,a_i) -p(s'_i\mid s_i,a_i)}$ is the concentration of local transition which is in function of $\sqrt{S}$.
That is, following the recent work \cite{ortner2012regret, jung2019thompson, akbarzadeh2022learning}, the concentration of global transition $\norm{\vp^k-\vp}_{\ell_1}$ is just in function of $n\sqrt{S}$.

\subsection*{Regret bound polynomial in the number arms?}

As we have seen in this chapter, the parameters of global MDP such as the diameter $D$, the mixing time, or the upper bound on span $H$ can be exponential in the number of arms.
For instance, if the conditions in Theorem~\ref{thm:thm:ergodicity_coeff} are satisfied, the global span of optimal bias is bounded exponentially in the number of arms as given in \eqref{eq:span_expo}.

We believe that, there is no clear dependency between the parameters of the global MDP and the number of arms in general.

\section{Conclusion}
\label{ch:restless:sec:conclude}

In this chapter, we provided different examples that show that defining a class of restless bandit that is learnable by relying on the assumption on local arms is hard.
In particular, we showed that there exists a restless bandit, which is weakly communicating and whose arms all are unichain with bounded local span, that is not learnable.
We also discussed about the conditions to assume in order to design a learning algorithm with regret guarantee.
Yet, in learning restless Markovian bandit, we believe the exponentiality in the number of arms in the regret bound of learning algorithm is not settled yet.
The chapter terminates here.

\endgroup
