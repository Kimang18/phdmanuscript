\begingroup
%\dominitoc
\chapter{Learning Algorithms for Restless Markovian Bandits}
\label{ch:learn_restless}

%\minitoc

\let\clearpage\relax

Restless multi-armed bandits (RMAB) are generalization of stochastic bandits in which each arm is represented by a Markov decision process. A RMAB can be seen as a Markov decision process with a state-space that is exponential in the number of arms. Hence, general-purpose learning algorithm are not efficient when applied to RMAB. Recently, a few learning algorithms have been specifically designed to work for restless bandits. Yet, they most often work only for very particular subclasses of restless bandits, or they hold under conditions that are very hard to verify.
    
In this document, we provide some explanations why. The performance of a learning algorithm for a RMAB depends in general on some structural conditions (like diameter or bias) of the global MDP.  In this document, we show that properties of the local arms (like unichain or small diameter) do not, in general, imply similar properties for the global Markov decision processes.  This implies that defining a class of RMAB that has desirable properties (like small diameter) is difficult. 

\section{Introduction}

(here should motivate the problem + add related work about learning and why it is related to diameters and other properties)

(We should define local properties and global properties).

We show that:
\begin{itemize}
    \item Local ergodic does not imply global weakly communicating because of periodicity 
    \item Local ergodic (= recurrent + aperiodic) does not imply globally ergodic (for now, our only example is unichain. Can we do more?)
    \item For globally ergodic, diameter can be exponential in the number of arms
\end{itemize}

\section{Basic definitions}

\subsection{MDP}

We consider a discrete-time MDP $M=(\gS,\gA,r,p)$ with a finite state-space $\gS$ and a finite action space $\gA$. At time $t$, if the process is in state $x$ and action $a$ is taken, it earns a reward $r(x,a)$ and jumps to state $y$ with probability $p(y|x,a)$. A stationary Markovian policy for such a MDP is a function $\pi:\gS\to\gA$. A policy $\pi$ induces a Markov chain with transition matrix $\mP^\pi$ where $P^\pi(x,y) = p(y | x, \pi(x))$. 

\subsection{RMAB}

A discrete-time multi-armed restless bandit problem with $n$ arms is a collection of $n$ arms. Each arm is a MDP with a finite state space $\gS_i$ and a binary action space $\gA=\{0,1\}$, where $0$ stands for "passive" and $1$ for "active". At each time instant, the decision maker chooses which of the $n$ arms to activate, with the constraint of activating exactly $m$ of the $n$ arms. The state space of the global MDP is $\bar{\gS} = \gS_1\times\dots\times\gS_n$ and the action space is composed of the vectors of $\{0,1\}^N$ whose components sum to $m$. We denote this action space by $\bar{\gA}$.  Given an action $\va\in\bar{\gA}$, all arms evolve independently; the arms are only coupled via the policy.

A RMAB is a MDP with specific structure. 

\subsection{Structural properties of MDP}

We classify MDPs in two ways:

\begin{enumerate}
    \item On the basis of the chain structure of the set of Markov chains induced by all stationary policies
    \item On the basis of patterns of states which are accessible from each other under some stationary policy.
\end{enumerate}


Following Puterman~\cite{puterman2014markov}, we say that a MDP is:
\begin{itemize}
    \item \emph{recurrent} if for all policy $\pi$, the matrix $\mP^\pi$ defines a Markov chain where all states are recurrent (equivalently, for all states $x$ and $y$, a Markov chain starting in $x$ will visit $y$ with probability $1$);
    \item \emph{unichain} if for all policy $\pi$, the matrix $\mP^\pi$ has a single recurrent class plus a possibly empty set of transient states;
    %\item \emph{communicating} if there exists a policy such that $P^\pi$ induces an ergodic Markov chain. 
    \item \emph{multichain} if the transition matrix corresponding to \emph{at least one} stationary policy contains two or more closed irreducible recurrent classes;
    \item \emph{communicating} if, for every pair of states, there exists a policy such that one state of the pair is accessible from other state of the same pair; and
    %\item \emph{weakly communicating} if there exists a policy such that $P^\pi$ has a unique recurrent class. No, cuz there is multichain weakly communicating
    \item \emph{weakly communicating} if there exists a closed set of states, with each state in that set accessible from every other state in that set under some deterministic stationary policy, plus a possibly empty set of transient states.
\end{itemize}
In addition, a recurrent or unichain MDP is called \emph{aperiodic} if all matrices $\mP^\pi$ are aperiodic.  We say that a MDP is \emph{ergodic} if it is recurrent and aperiodic. 

We note that:
\begin{itemize}
    \item Recurrent implies unichain and communicating.
    \item Communicating implies weakly communicating.
    %\item Weakly communicating implies communicating.
    \item Unichain implies weakly communicating.
\end{itemize}
%\KK{I think communicating implies weakly communicating.}

In \cite{fruit2018near}, it is proven that in weakly communicating MDPs, no algorithm can ever achieve a logarithmic growth of the regret without first suffering a linear regret for a number of steps that is exponential in the parameters of the MDP.

\subsection{Diameter}

For any stationary policy $\pi$, let $T^\pi(y \mid M, x)$ be the random variable for the first time step in which state $y$ is reached when starting at state $x$ and following the policy $\pi$ in MDP $M$.
From \cite[Definition 1]{jaksch2010near}, the diameter of a MDP $M$ is defined by
\begin{align*}
    \mathrm{diameter}(M) = \max_{x\neq y}\min_{\pi} \ex{T^\pi(y\mid M, x)}.
\end{align*}

Hence, any noncommunicating MDP has \emph{infinite diameter}.

It is shown in \cite[Appendix A]{jaksch2010near} that the diameter is at least $\log_{|\gA|}|\gS|-3$.
This implies that the diameter of RMAB model in which each arm has the same state size $S$, is at least
\begin{equation*}
    \log_{|\bar{\gA}|}|\bar{\gS}| -3 = \sum_{i=1}^{n}\log_{|\bar{\gA}|}|\gS^{(i)}| -3 =n\log_{|\bar{\gA}|}S -3
\end{equation*}

\subsection{Span}

\KK{What MDP structure should we assume?}

Following \cite[Chapter~8]{puterman2014markov}, in finite-state Markov reward process $(\vr, \mP)$, we define $\bar{\mP}=\displaystyle\lim_{N\to+\infty}\frac1N\sum_{t=0}^{N-1}\mP^{t}$ as the Cesaro limit of the sequence $\{\mP^t\}_{t\ge0}$.
This limit exists for finite-state process (see Section~A.4 of Appendix~A of \cite{puterman2014markov}).
If $\bar{\mP}$ is stochastic, then the gain is $\vg=\bar{\mP}\vr$.
The bias is defined according to the structure of Markov chain:
\begin{enumerate}
    \item if the Markov chain is \emph{aperiodic}, then the bias is $\vh=\displaystyle\sum_{t=0}^\infty(\mP^t-\bar{\mP})\vr$
    \item if the Markov chain is \emph{periodic}, then the bias is $\vh=\displaystyle\lim_{N\to\infty}\frac1N\sum_{k=1}^N\sum_{t=0}^{k-1}(\mP^t-\bar{\mP})\vr$
\end{enumerate}

Let $\vh\in\sR^S$. The span of $\vh$ is given by $sp(\vh)=\max_xh(x) - \min_xh(x)$.

\subsection{Mixing time}

Following \cite[Definition 5.1]{wei2020model},
\begin{defn}
    \label{def:mixing_time}
    The mixing time of an ergodic MDP is defined as
    \begin{align*}
        t_{mix} := \max_{\pi}\min\left\{ t\ge1 \mid \lVert (\mP^\pi)^t(x,\cdot) - \mu^\pi\rVert_1 \le \frac14, \forall x\right\},
    \end{align*}
    that is, the maximum time required for any policy starting at any initial state to make the state distribution $\frac14$-close (in $\ell_1$ norm) to the stationary distribution.
\end{defn}


\section{Structural properties: counter-examples and positive results}

%The following theorem shows that a restless bandit whose arms are all unichain is not necessarily unichain. This is true under the additional condition that all arms are aperiodic.
\subsection{Negative Results}
The following theorem shows that a restless bandit whose arms are all ergodic is not necessarily ergodic.
\begin{thm}
    \begin{enumerate}[label=(\roman*)]
        \item \label{thm:not_ergodic} There exists a RMAB whose arms are all recurrent that is noncommunicating. % the term noncommunicating is taken from [Puterman, 1994, page 353]. 
        %\item \label{thm:aperiodic_RB_ergo} There exists a Restless bandit whose arms are all ergodic that is not recurrent. 
        \item \label{thm:ergodic_arms_multichain_RB} There exists a RMAB whose arms are all ergodic that is multichain.
    \end{enumerate}
\end{thm}

\begin{proof}
    \textbf{Proof of \ref{thm:not_ergodic}} -- It is given by the counter-example in which the RMAB has two arms as given by \figurename~\ref{fig:recur_non_communicate}.
    For each arm, both "activate" and "rest" induce the same transition.
    In such example, the RMAB is a noncommunicating multichain MDP because if we start at state $(1,a)$ or $(2,b)$, then, there are two recurrent classes: one composed of $\{(1,a), (2,b)\}$ and the other one is $\{(1,b), (2,a)\}$.
    \begin{figure}[ht]
        \centering
        \begin{tabular}{ccc}
        \begin{tikzpicture}[on grid, state/.style={circle,draw}, >= stealth', auto, prob/.style = {inner sep=1pt,font=\scriptsize}]
            \node[state]  (A) {$1$};
            \node[state]  (B) [below = 2cm of A]   {$2$};
            \path[->]
            (A) edge[bend left]     node{}	(B)
    	    (B) edge[bend left]     node{}	(A);
        \end{tikzpicture}
        &
        \begin{tikzpicture}[on grid, state/.style={circle,draw}, >= stealth', auto, prob/.style = {inner sep=1pt,font=\scriptsize}]
            \node[state]  (A) {$a$};
            \node[state]  (B) [below = 2cm of A]   {$b$};
            \path[->]
            (A) edge[bend left]     node{}	(B)
    	    (B) edge[bend left]     node{}	(A);
        \end{tikzpicture}
        &
        \begin{tikzpicture}[on grid, state/.style={circle,draw}, >= stealth', auto, prob/.style = {inner sep=1pt,font=\scriptsize}]
            \node[state]  (A) {$1a$};
            \node[state]  (B) [right = 1.5cm of A]   {$1b$};
            \node[state]  (C) [right = 1.5cm of B]   {$2a$};
            \node[state]  (D) [right = 1.5cm of C]   {$2b$};
            \path[->]
            (A) edge[bend left=50]     node{}	(D)
    	    (B) edge[bend left]     node{}	(C)
    	    (C) edge[bend left]     node{}	(B)
            (D) edge[bend left=50] node{} (A);
        \end{tikzpicture}
        \\
            Arm $1$ & Arm $2$ & Global MDP
        \end{tabular}
        \caption{A RMAB with two 2-state arms that are recurrent.}
        \label{fig:recur_non_communicate}
    \end{figure}
    \medskip \\
    \textbf{Proof of \ref{thm:ergodic_arms_multichain_RB}} -- It is given by another counter-example in \figurename~\ref{fig:local_ergodic_multichain_RB}.
    \begin{figure}
        \centering
        \begin{tabular}{c}
        \begin{tikzpicture}[on grid, state/.style={ellipse,draw}, >= stealth', auto, prob/.style = {inner sep=1pt,font=\scriptsize}]
            \node[state]  (A) {$8$};
            \node[state]  (B) [right = 1.6cm of A]    {$1$};
            \node[state]  (C) [right = 1.6cm of B]    {$2$};
            \node[state]  (D) [below = 1.6cm of C]    {$3$};
            \node[state]  (E) [below = 1.6cm of D]    {$4$};
            \node[state]  (F) [left = 1.6cm of E]    {$5$};
            \node[state]  (G) [left = 1.6cm of F]    {$6$};
            \node[state]  (H) [below = 1.6cm of A]    {$7$};
            \node[text width=5cm] (I) [left = 1.4cm of H] {Arm's structure};
            \path[->]
                (A) edge[red, bend right=30] node{} (B)
                edge[red, loop above] node{} (A)
                (B) edge[red, bend right=30] node{} (C)
                edge[red, bend right=30] node{} (D)
                (C) edge[red, bend right=30] node{} (D)
                edge[red, loop above] node{} (C)
                (D) edge[red, bend right=30] node{} (E)
                edge[red, bend right=30] node{} (F)
                (E) edge[red, bend right=30] node{} (F)
                edge[red, loop below] node{} (E)
                (F) edge[red, bend right=30] node{} (G)
                edge[red, bend right=30] node{} (H)
                (G) edge[red, bend right=30] node{} (H)
                edge[red, loop below] node{} (G)
                (H) edge[red, bend right=30] node{} (A)
                edge[red, bend right=30] node{} (B)

                (A) edge[bend left=30]     node{}	(B)
                edge[bend left=50]     node{}	(C)
                (B) edge[loop above] node{} (B)
                edge[bend left=30]     node{}	(C)
                (C) edge[bend left=30]     node{}	(D)
                edge[bend left=50]     node{}	(E)
                (D) edge[loop right] node{} (D)
                edge[bend left=30]     node{}	(E)
                (E) edge[bend left=30]     node{}	(F)
                edge[bend left=50]     node{}	(G)
                (F) edge[loop below] node{} (F)
                edge[bend left=30]     node{}	(G)
                (G) edge[bend left=30]     node{}	(H)
                edge[bend left=50]     node{}	(A)
                (H) edge[loop left] node{} (H)
                edge[bend left=30]     node{}	(A);
        \end{tikzpicture} \\
        \begin{tikzpicture}[on grid, state/.style={ellipse,draw}, >= stealth', auto, prob/.style = {inner sep=1pt,font=\scriptsize}]
            \node[state]  (A) {$\begin{tabular}{c}15\\24\\25\end{tabular}$};
            \node[state]  (B) [right = 2cm of A]    {$\begin{tabular}{c}26\\35\\36\end{tabular}$};
            \node[state]  (C) [right = 2cm of B]    {$\begin{tabular}{c}37\\46\\47\end{tabular}$};
            \node[state]  (D) [right = 2cm of C]    {$\begin{tabular}{c}48\\57\\58\end{tabular}$};
            \node[state]  (E) [below = 3cm of D]    {$\begin{tabular}{c}51\\68\\61\end{tabular}$};
            \node[state]  (F) [left = 2cm of E]    {$\begin{tabular}{c}62\\71\\72\end{tabular}$};
            \node[state]  (G) [left = 2cm of F]    {$\begin{tabular}{c}73\\82\\83\end{tabular}$};
            \node[state]  (H) [left = 2cm of G]    {$\begin{tabular}{c}84\\13\\14\end{tabular}$};
            \node[text width=2cm] (J) [left = 1.5cm of A] {Recurrent\\Class $\bar{\gS}^{(1)}$};
            \path[->]
            (A) edge     node{}	(B)
            (B) edge     node{}	(C)
            (C) edge     node{}	(D)
            (D) edge     node{}	(E)
    	    (E) edge     node{}	(F)
    	    (F) edge     node{}	(G)
            (G) edge     node{}	(H)
            (H) edge     node{}	(A);
        \end{tikzpicture} \\ \hfill\\
        \begin{tikzpicture}[on grid, state/.style={ellipse,draw}, >= stealth', auto, prob/.style = {inner sep=1pt,font=\scriptsize}]
            \node[state]  (A) {$\begin{tabular}{c}11\\28\\21\end{tabular}$};
            \node[state]  (B) [right = 2cm of A]    {$\begin{tabular}{c}22\\31\\32\end{tabular}$};
            \node[state]  (C) [right = 2cm of B]    {$\begin{tabular}{c}33\\42\\43\end{tabular}$};
            \node[state]  (D) [right = 2cm of C]    {$\begin{tabular}{c}44\\53\\54\end{tabular}$};
            \node[state]  (E) [below = 3cm of D]    {$\begin{tabular}{c}55\\64\\63\end{tabular}$};
            \node[state]  (F) [left = 2cm of E]    {$\begin{tabular}{c}66\\75\\74\end{tabular}$};
            \node[state]  (G) [left = 2cm of F]    {$\begin{tabular}{c}77\\86\\84\end{tabular}$};
            \node[state]  (H) [left = 2cm of G]    {$\begin{tabular}{c}88\\17\\15\end{tabular}$};
            \node[text width=2cm] (J) [left = 1.5cm of A] {Recurrent\\Class $\bar{\gS}^{(2)}$};
            \path[->]
            (A) edge     node{}	(B)
            (B) edge     node{}	(C)
            (C) edge     node{}	(D)
            (D) edge     node{}	(E)
    	    (E) edge     node{}	(F)
    	    (F) edge     node{}	(G)
            (G) edge     node{}	(H)
            (H) edge     node{}	(A);
        \end{tikzpicture}
        \end{tabular}
        \caption{We consider a RMAB with 2 arms, each is an 8-state ergodic MDP where the arm goes from states $1$ and $2$ to state $2$ or $3$, states $3$ and $4$ to state $4$ or $5$, etc, under passive action, and goes from states $1$ and $8$ to state $1$ or $2$, etc, under active action. Both arms have the same structure.
        The first row shows the state transition of the arm: the black arrows represent active transition and the red ones represent passive transition.
        The second and third rows show the state transition of two recurrent classes under any policies that activate Arm 2 for any impair $x\in[8]$ and Arm 1 for any pair $x\in[8]$ when RMAB is in states $(x,x),(x,x+4),(x+1,x),(x+1,x+3),(x+1,x+4),(x+1,x+7)$.
        The first recurrent class is $\bar{\gS}^{(1)} =\{(x,x+4),(x+1,x+3),(x+1,x+4) : x\in[8]\}$ and the second one is $\bar{\gS}^{(2)} =\{(x,x),(x+1,x),(x+1,x+7) : x\in[8]\}$. The rest of the states $\bar{\gS}\setminus(\bar{\gS}^{(1)}\cup\bar{\gS}^{(2)})$ are transient regardless of whether Arm $1$ or Arm $2$ is pulled.
    }
        \label{fig:local_ergodic_multichain_RB}
    \end{figure}
\end{proof}

\subsubsection{Diameter, mixing time, and span}

\begin{thm}
    \begin{enumerate}[label=(\roman*)]
        \item \label{thm:diam} There exists an ergodic RMAB, whose arms all have a bounded diameter, that has a diameter exponential in the number of arms. % the term noncommunicating is taken from [Puterman, 1994, page 353]. 
        %\item \label{thm:aperiodic_RB_ergo} There exists a Restless bandit whose arms are all ergodic that is not recurrent. 
        \item \label{thm:mixing} There exists an ergodic RMAB, whose arms are all ergodic, that has an unbounded mixing time.
        \item \label{thm:span} There exists a communicating RMAB, whose arms all have a bounded span, that has an unbounded span.
    \end{enumerate}
\end{thm}
\begin{proof}
    \textbf{Proof of \ref{thm:diam}} -- consider an example of $n$ arms, each arm is a Markov chain with binary state space and $0.5$ probability of changing state under both actions.
The probability that RMAB goes from state $(1,\dots,1)$ to state $(0,\dots,0)$ is $(1/2)^n$.
Hence, the diameter is exponential in the number of arms.
\medskip \\
\textbf{Proof of \ref{thm:mixing}} -- Let $\mP^m$ be the transition matrix of the arm given in \figurename~\ref{fig:local_ergodic_multichain_RB}.
Let $\mU$ be the transition matrix such that the state transition is uniform in any state.
Any arm having $\mU$ as the transition matrix is ergodic and the RMAB that is composed of such arms is also ergodic.
Now, consider the arms having the transition matrix $\mP=(1-\varepsilon)\mP^m +\varepsilon \mU$ for $\varepsilon\in(0,1)$.
Such arms are ergodic and the RMAB having such arms is also ergodic.
However, the mixing time of such RMAB tends to infinity when $\varepsilon$ tends to $0$ because when $\varepsilon=0$ the RMAB is multichain as shown in Theorem~\ref{thm:ergodic_arms_multichain_RB}.
%Hence, the hypothesis of ergodicity on RMAB problem is still not enough for mixing time to be defined.
\medskip \\
\textbf{Proof of \ref{thm:span}} -- First of all, consider the RMAB in Figure~\ref{fig:recur_non_communicate}.
The reward in local state $1$ and $a$ is $1$ and in local state $2$ and $b$ is $0$ independently of the action taken.
Thus, the upper bound on the local span of each arm is $1$ but the RMAB is multichain whose upper bound on the global span is infinite.
Using the same technique above, let $\mP^m$ be the transition matrix of one arm in Figure~\ref{fig:recur_non_communicate} and $\mU$ be the transition matrix such that the state transition is uniform in any state.
The arms having the transition matrix $\mP=(1-\varepsilon)\mP^m +\varepsilon \mU$ for $\varepsilon\in(0,1)$ are ergodic with finite upper bound on the local span. 
The RMABs having such arms are communicating.
However, the upper bound on the global span tends to infinity when $\varepsilon$ tends to $0$.
\end{proof}

\subsubsection{Minimax lower bound on the expected regret}

\begin{thm}
    There exists an ergodic RMAB in which any generic reinforcement learning algorithm suffers a regret that is at least linear in $T$, the total number of time steps.
\end{thm}
\begin{proof}
    We show this theorem by constructing a RMAB whose span of the optimal bias is as big as total time $T$.
    Then, we use the existing result on the lower bound of general MDP to conclude the proof.

    Consider a RMAB with 2 arms whose transition is given in Figure~\ref{fig:lower_bound} and at each time step, we activate only one arm.
    This RMAB is weakly communicating.
    The reward in local states $1$ and $a$ is $1$, and states $2$ and $b$ is $0$.
    To maximize the average reward in this RMAB, we need Arm~$1$ to stay in state $1$ as frequent as possible.
    So, one optimal policy is
    \begin{align*}
        \pi^*(1a)=1, \pi^*(1b)=1, \pi^*(2a)=2, \pi^*(2b)=2.
    \end{align*}
    That is, when the RMAB is in state $1a$, it is optimal to activate Arm~$1$.
    Under policy $\pi^*$, the Markov chain $\mP^{\pi^*}$ is ergodic (see Figure~\ref{fig:lower_bound}(c)).
    The reward under this policy is $r^{\pi^*}(1a)=1, r^{\pi^*}(1b)=1, r^{\pi^*}(2a)=1, r^{\pi^*}(2b)=0$.
    Using Bellman equations with a condition that $h^*(2b)=0$, we get
    \begin{align*}
        g^*=h^*(1a)=\frac{3-\varepsilon}{4-2\varepsilon}, h^*(1b)=\frac1{\varepsilon(2-\varepsilon)},
        h^*(2a)=\frac{1+\varepsilon}{2\varepsilon}.
    \end{align*}
    Then, the span of $\vh^*$ is given by $sp(\vh^*)=\displaystyle\frac{1+\varepsilon}{2\varepsilon}$.
    Following \cite{jaksch2010near, zhang2019regret}, the minimax lower bound on the expected regret in this RMAB is $\displaystyle\Omega\left(\sqrt{\frac{1+\varepsilon}{2\varepsilon}T}\right)$.
    Then, for $\varepsilon=1/T$, any learning algorithm suffers a regret that is at least linear in $T$.
    \KK{How to define the upper bound on span? $H=sp(\vh^*)$ or $H=\max_{\pi}sp(\vh^\pi)$?}
    \KK{Define $H=\max_{\pi}sp(\vh^\pi)$ requires that the MDP is unichain}
    \KK{Define $H=sp(\vh^*)$ requires that the MDP is weakly communicating. This is what is used in \cite{bartlett2012regal,zhang2019regret}}
    
    \begin{figure}[ht]
        \centering
        \begin{tabular}{ccc}
        \begin{tikzpicture}[on grid, state/.style={circle,draw}, >= stealth', auto, prob/.style = {inner sep=1pt,font=\scriptsize}]
            \node[state]  (A) {$1$};
            \node[state]  (B) [below = 2cm of A]   {$2$};
            \path[->]
            (A) edge[bend left, color=red]     node{}	(B)
            (A) edge[bend left=80]     node{$1-\varepsilon$}	(B)
            (A) edge[loop above]     node{$\varepsilon$}	(A)
    	    (B) edge[bend left, color=red]     node{}	(A)
    	    (B) edge[bend left=80]     node{}	(A);
        \end{tikzpicture}
        &
        \begin{tikzpicture}[on grid, state/.style={circle,draw}, >= stealth', auto, prob/.style = {inner sep=1pt,font=\scriptsize}]
            \node[state]  (A) {$a$};
            \node[state]  (B) [below = 2cm of A]   {$b$};
            \path[->]
            (A) edge[bend left=80]     node{}	(B)
            (A) edge[bend left, color=red]     node{}	(B)
    	    (B) edge[bend left=80]     node{}	(A)
    	    (B) edge[bend left, color=red]     node{}	(A);
        \end{tikzpicture}
        &
        \begin{tikzpicture}[on grid, state/.style={circle,draw}, >= stealth', auto, prob/.style = {inner sep=1pt,font=\scriptsize}]
            \node[state]  (A) {$1a$};
            \node[state]  (B) [right = 1.5cm of A]   {$1b$};
            \node[state]  (C) [right = 1.5cm of B]   {$2a$};
            \node[state]  (D) [right = 1.5cm of C]   {$2b$};
            \path[->]
            (A) edge[bend left=50]     node{$1-\varepsilon$}	(D)
            (A) edge[bend left]     node{$\varepsilon$}	(B)
    	    (B) edge[bend left]     node{$1-\varepsilon$}	(C)
    	    (B) edge[bend left]     node{$\varepsilon$}	(A)
            (C) edge[bend left]     node{$1$}	(B)
            (D) edge[bend left=50] node{$1$} (A);
        \end{tikzpicture}
        \\
            (a) Arm $1$ & (b) Arm $2$ & (c) Markov chain under $\pi^*$
        \end{tabular}
        \caption{A RMAB with two 2-state arms. The red arrows show the state transition due to passive action and black ones show the state transition under active action.}
        \label{fig:lower_bound}
    \end{figure}
\end{proof}


\subsection{Positive Results}

\begin{thm}
    \label{thm:aperiodic_RB_comm} A RMAB whose arms are all ergodic is communicating. 
\end{thm}
\begin{proof}
    We prove the theorem by its contraposition. Assume that a given RMAB is noncommunicating.
    In this RMAB, there are global states that are not reachable from each other.
    This implies to two possibilities: 1. for some arms, some local states are not recurrent, 2. all arms are periodic.
    Each possibility implies that there exists at least one arm that is not ergodic.
\end{proof}

\begin{thm}
    \label{thm:unichain} If all arms are unichain MDP with at least one state reachable in one step from any other states under both passive and active actions, then the corresponding RMAB is also a unichain MDP with at least one state reachable in one step from any other global states under any policies.
\end{thm}
\begin{proof}
    For each arm $i\in[n]$, let $z_i$ be the state that is reachable in one step from any other states under both passive and active actions.
    We have that $p(z_i \mid x_i, a)>0$ for all $i\in[n], x_i\in\gS_i,a\in\{0,1\}$.
    %For simplification, we write $P^a(x_i,z_i) =P^a_i(x_i, z_i)$.
    For any action $\va\in\bar{\gA}$, any state $\vx,\vy\in\bar{\gS},$ the RMAB's state transition is given by ${\vp(\vy \mid \vx, \va) =\prod_{i=1}^n p(y_i \mid x_i, a_i)}$.
    Then, for any action $\va\in\bar{\gA}$, any state $\vx\in\bar{\gS}$, we have
    \begin{align*}
        \vp(\vz \mid \vx, \va) =\prod_{i=1}^n p(z_i \mid x_i,a_i) > 0,
    \end{align*}
    because $n$ is finite and for any $i$, $p(z_i \mid x_i, a_i)>0$.
    Hence, $\vz$ is reachable from any other global states in one step under any policies.
    Consequently, no policies induce the global Markov chain that has multiple closed irreducible recurrent classes.
    %there cannot be multiple recurrent classes that are closed under any policies.
    That concludes the proof.
\end{proof}

\begin{thm}
    \label{thm:ergodicity_coeff} For each arm $i\in[n]$, let $\beta_i$ be defined by
    \begin{align*}
        \beta_i = 1-\min_{\substack{x_i,y_i\in\gS_i \\a,a'\in\{0,1\}}} \sum_{z_i\in\gS_i} \min\{P^{a}(x_i,z_i), P^{a'}(y_i,z_i)\}.
    \end{align*}
    Similarly, for a RMAB, let $\vbeta$ be defined by
    \begin{align*}
        \vbeta = 1-\min_{\substack{\vx,\vy\in\bar{\gS} \\\va,\va'\in\bar{\gA}}} \sum_{\vz\in\bar{\gS}} \min\{\mP^{\va}(\vx,\vz), \mP^{\va'}(\vy,\vz)\}.
    \end{align*}
    If for any arm $i$ $\beta_i<1$, then, $\vbeta<1$.
    Moreover, if there exists $\varepsilon>0$ such that for any arm $i\in[n]$,
    \begin{align*}
        \min_{\substack{x_i,y_i\in\gS_i \\a,a'\in\{0,1\}}} \sum_{z_i\in\gS_i} \min\{P^{a}(x_i,z_i), P^{a'}(y_i,z_i)\} \ge \varepsilon,
    \end{align*}
    then, $\vbeta\le 1-\varepsilon^n$.
\end{thm}
\begin{proof}
    For any pair $(\vx,\vy)\in\bar{\gS}^2$ and any pair $(\va,\va')\in\bar{\gA}^2$,
    \begin{align*}
        \sum_{\vz\in\bar{\gS}} \min\{\mP^{\va}(\vx,\vz), \mP^{\va'}(\vy,\vz)\}
        &= \sum_{\vz\in\bar{\gS}} \min\left\{\prod_{i=1}^nP^{a_i}(x_i,z_i), \prod_{i=1}^nP^{a_i'}(y_i,z_i)\right\} \\
        &\ge \sum_{\vz\in\bar{\gS}}\prod_{i=1}^n \min\{P^{a_i}(x_i,z_i), P^{a_i'}(y_i,z_i)\} \\
        &= \prod_{i=1}^n \left(\sum_{z_i\in\gS_i} \min\{P^{a_i}(x_i,z_i), P^{a_i'}(y_i,z_i)\}\right)
    \end{align*}
    %\KK{Add more detail about the second equality}
    Since $\beta_i<1$, we have that $\sum_{z_i\in\gS_i} \min\{P^{a_i}(x_i,z_i), P^{a_i'}(y_i,z_i)\}>0$.
    In consequences, for any pair $(\vx,\vy)\in\bar{\gS}^2$ and any pair $(\va,\va')\in\bar{\gA}^2,$ we have ${\sum_{\vz\in\bar{\gS}} \min\{\mP^{\va}(\vx,\vz), \mP^{\va'}(\vy,\vz)\}>0}$.
    We conclude the proof using the definition of $\vbeta$.
\end{proof}


\section{Learning RMAB}

We consider learning a RMAB problem whose arms' rewards, $\{r_i\}_{i\in[n]}$, are known to the learner.
Only the arms' transition, $\{P_i\}_{i\in[n]}$, are unknown and need to be learnt via observations.

\subsection{Regret definition and minimal assumptions}

Classically, the regret of the learner after $T$ time steps is given by
\begin{align}
    \Reg(M,T) = Tg^* - \sum_{t=1}^{T} \vr(\bX_{t},\bA_{t}). \label{eq:regret}
\end{align}
This definition implicitly requires that the unknown MDP $M$ is weakly communicating.

\subsubsection{UCRL2 framework}

The algorithm updates its policy using doubling trick.
Let $M^k$ be the imagined MDP of the algorithm at episode $k$.
Let $\pi^k$ be an optimal policy in $M^k$.
We require $M^k$ to be weakly communicating in order to have the Bellman equations
\begin{align*}
    g^k +h^{\pi^k}(\vx) = \vr^k(\vx, \pi^k(\vx)) +\sum_{\vy}\vp^k(\vy \mid \vx, \pi^k(\vx))h^{\pi^k}(\vy).
\end{align*}
Then,
\begin{align*}
    \Reg(M, T)&=\sum_{k=1}^{K_T}\sum_{t=t_k}^{t_{k+1}-1} (g^*-g^k) {+}(\vr^k-\vr)(\mX_t,\mA_t) {+}\sum_{\vy}(\vp^k-\vp)(\vy \mid \mX_t, \mA_t)h^{\pi^k}(\vy)\\
              &\quad  +\sum_{\vy}\vp(\vy \mid \mX_t, \mA_t)h^{\pi^k}(\vy)-h^{\pi^k}(\mX_t)
\end{align*}

When $\{r_i\}_{i\in[n]}$ is given and uppber bounded by $1$, the second term of regret is zero and the first term is bounded by $K_T$.
The last term is bounded by Azuma-Hoeffding inequality for the sequence of martingale differences.

Since our algorithm uses Bayesian approach, we measure its performance by
\begin{align}
    \BayReg(T) = \ex{\Reg(T)}. \label{eq:bayreg}
\end{align}



For each arm $i\in[n]$, the unknown parameter $P_i$ belongs to a compact set $\Theta_i$.
Using Whittle index policy as a baseline, we need the two following assumptions:
\begin{asmp}
    \label{asmp:indexable}
    For any $i\in[n], P\in\Theta_i$, the arm $\langle\gS_i,\{0,1\},P,r_i\rangle$ is indexable.
\end{asmp}
\begin{asmp}
    \label{asmp:unichain}
    For any $i\in[n], P\in\Theta_i$, the arm $\langle\gS_i,\{0,1\},P,r_i\rangle$ has at least one state that is reachable in one step from any other states under any actions:
    \begin{align*}
        \beta_i = 1-\min_{\substack{x_i,y_i\in\gS_i \\a,a'\in\{0,1\}}} \sum_{z_i\in\gS_i} \min\{P^{a}(x_i,z_i), P^{a'}(y_i,z_i)\}<1.
    \end{align*}
\end{asmp}

Assumption~\ref{asmp:indexable} allows Whittle index policy to be meaningful.
By Theorem~\ref{thm:unichain}, Assumption~\ref{asmp:unichain} implies that the unknown RMAB is unichain. Consequently, the average gain of Whittle index policy is constant over all states of RMAB.
%Let $(\mP^W, \vr^W)$ be the controlled parameters of the corresponding RMAB under Whittle index policy $\vpi^W$ and $(g^W, h^W)$ be the average gain and bias function under $\vpi^W$ and satisfying
Let $(g^W, h^W)$ be the average gain and bias function under $\vpi^W$ and satisfying
\begin{align}
    g^W + h^W(\vx) = \vr(\vx,\va) +\sum_{\vz\in\bar{\gS}} \mP^{\va}(\vx, \vz)h^W(\vz),\hspace{1cm} \forall \vx\in\bar{\gS}, \va=\vpi^W(\vx). \label{eq:bellman}
\end{align}

\subsection{Regret}
\label{ssec:regret}

\KK{Add text}

\subsection{Learning Algorithm}
\label{ssec:learn_algo}

Our algorithm starts by choosing a prior $\phi_i$ over the parameter $P_i$ of arm $i$ for each $i\in[n]$.
The support of $\phi_i$ is the compact set $\Theta_i$. The algorithm updates its policy episodically: let $\gO_{k-1}$ be the observations made prior to the beginning of episode $k\ge1$ with the convention $\gO_{0}=\emptyset$.
Let $t_k$ be the time instant where episode $k$ begins and $t_1=1$.
The algorithm samples the parameters $P_{k,i}$ from the posterior distribution $\phi_i(\cdot \mid \gO_{k-1})$, compute Whittle indices of each arm $\langle\gS_i,\{0,1\},P_{k,i},r_i\rangle$ for $i\in[n]$ and applies the Whittle index policy $\vpi_k^W$ during episode $k$.
For any $i\in[n]$ and any state-action pair $(x_i,a_i)\in\gS_i\times\{0,1\}$, let $N_{k-1}(x_i,a_i):=\sum_{t'=1}^{t_k-1}\sI{(X_{t',i},A_{t',i})=(x_i,a_i)}$ be the number of visits to the pair $(x_i,a_i)$ up to the start of episode $k$ and $\nu_{k,t}(x_i,a_i):=\sum_{t'=t_k}^{t}\sI{(X_{t',i},A_{t',i})=(x_i,a_i)}$ be the number of times that arm $i$ visits the pair $(x_i,a_i)$ from time $t_k$ to $t$. Let $N_{k-1}^+(x_i,a_i):=\max\{1,N_{k-1}(x_i,a_i)\}$. Also, let $T_k:=t_{k+1}-t_{k}$ be the length of episode $k$. At time instant $t>t_k$, the algorithm starts episode $k+1$ if
\begin{align}
    t-t_k > T_{k-1} \text{ or } \sum_{x_i,a_i} \frac{\nu_{k,t}(x_i,a_i)}{N_{k-1}^+(x_i,a_i)}>1 \text{ for some } i\in[n]. \label{eq:epi_cond}
\end{align}
The first condition of \eqref{eq:epi_cond} ensures that the length of each episode is not too long and grows linearly with the number of episodes.
The second condition is adapted from \emph{extended doubling trick} which is proposed in \cite{tossou2019near}.
This condition ensures that if either a new state-action pair $(x_i,a_i)$ is visited or the number of visits to at least one of the visited state-action pairs is doubled or the number of visits to visited state-action pairs is double in average for some arm $i$, then, start a new episode.
We show in Lemma~\ref{} that if $T$ is the total number of time steps, then, the total number of episodes is bounded as a logarithmic of $T$.

Note that, by Assumptions~\ref{asmp:indexable} and \ref{asmp:unichain}, the sampled RMAB\footnote{Precisely, we sample the parameter of each arm} is unichain and indexable.
Hence, the Whittle index policy is meaningful and induces the average gain that is constant over all states in $\bar{\gS}$.

\subsection{Regret Bound}
\label{ssec:regret_bound}

Let $K_T$ be the total number of episodes over $T$ time steps.

\subsubsection{High Probability Event}
\label{sssec:high_event}

For any $i\in[n]$, any $(x_i,a_i,y_i)\in\gS_i\times\{0,1\}\times\gS_i$, let $${\Phat_{k-1}^{a_i}(x_i,y_i):=\frac1{N^+_{k-1}(x_i,a_i)}\sum_{t'=1}^{t_k-1}\sI{(X_{t',i},A_{t',i},X_{t'+1,i})=(x_i,a_i,y_i)}}$$ be the empirical estimator of $P^{a_i}(x_i,y_i)$.% and $\sigHat_{k-1}:=\Phat_{k-1}(1-\Phat_{k-1})$ be its population variance.
%The rigorous notation is $\Phat_{k-1}=\Phat_{N^+_{k-1}}$ and $\sigHat_{k-1}=\sigHat_{N^+_{k-1}}$.
If $T$ is the total number of time steps, consider the following event
\begin{align}
    \gE_1(T):=\left\{\forall i,x_i,a,y_i,k\ge0: |\Phat_k^a(x_i,y_i) {-}P^a(x_i,y_i)|\le \sqrt{\frac{2\Phat_k(1-\Phat_k)ln(20nS^2N_k^2T)}{N_k(x_i,a_i)}} +\frac{3ln(20nS^2N_k^2T)}{N_k(x_i,a_i)}\right\}. \label{eq:event_1}
\end{align}

\begin{lem}
    \label{lem:high_event1}
    For total number of time steps $T$, we have $\Proba{\gE_1(T)}\ge 1-\frac1{2T}$.
\end{lem}
\begin{proof}
The rigorous notation is $\Phat_{k-1}=\Phat_{N^+_{k-1}}$.
We slightly change notation $N^+_{k-1}$ to $\ell$ so that $\Phat_{N^+_{k-1}}=\Phat_\ell$.
From \cite[Theorem~1]{audibert2009exploration}, for any $\ell\ge1$ and $\varepsilon>0$,
\begin{align}
    \Proba{|\Phat_\ell-P|\le\sqrt{\frac{2\Phat_\ell(1-\Phat_\ell)\varepsilon}{\ell}} +\frac{3\varepsilon}{\ell}} \ge 1-3e^{-\varepsilon}. \label{eq:bern_concentration}
\end{align}
Now, consider the following event
\begin{align*}
    \gE'_1(T):=\left\{\exists i,x_i,a,y_i,\ell\ge1: |\Phat_\ell^a(x_i,y_i) -P^a(x_i,y_i)| > \sqrt{\frac{2\Phat_\ell(1-\Phat_\ell)ln(20nS^2\ell^2T)}\ell} +\frac{3ln(20nS^2\ell^2T)}{\ell}\right\}.
\end{align*}
We have that the complement of $\gE_1(T)$ is smaller then or equal to event $\gE'_1(T)$, $\bar{\gE}_1(T)\subseteq\gE'_1(T)$. To show that $\gE_1(T)$ occurs with high probability, we work on its complement: by union bound, we have
\begin{align*}
    \Proba{\bar{\gE}_1(T)}
    &\le \Proba{\gE'_1(T)} \\
    &\le \sum_{i=1}^{n} \sum_{x_i,a,y_i} \sum_{l=1}^{\infty} \Proba{|\Phat_\ell^a(x_i,y_i) -P^a(x_i,y_i)| > \sqrt{\frac{2\Phat_\ell(1-\Phat_\ell)ln(20nS^2\ell^2T)}\ell} +\frac{3ln(20nS^2\ell^2T)}{\ell}} \\
    &\le \sum_{i=1}^{n} \sum_{x_i,a,y_i} \sum_{l=1}^{\infty} \Proba{|\Phat_\ell^a(x_i,y_i) -P^a(x_i,y_i)| > \sqrt{\frac{2\Phat_\ell(1-\Phat_\ell)ln(2\pi^2nS^2\ell^2T)}\ell} +\frac{3ln(2\pi^2nS^2\ell^2T)}{\ell}} \\
    &\le \sum_{i=1}^{n} \sum_{x_i,a,y_i} \sum_{l=1}^{\infty} 3e^{-ln(2\pi^2nS^2\ell^2T)}
    =2nS^2\sum_{l=1}^{\infty}\frac{3}{2\pi^2nS^2\ell^2T} = \frac1{2T}
\end{align*}
\end{proof}

At the start of episode $k\ge1$, the estimator $\Phat_{k-1}$ is made prior and the learner samples $P_k$ from the posterior distribution.
Now, consider the following event
\begin{align}
    \gE_2(T):=\left\{\forall i,x_i,a,y_i,k\ge0: |\Phat_k^a(x_i,y_i) {-}P_{k+1}^a(x_i,y_i)|\le \sqrt{\frac{2\Phat_k(1-\Phat_k)ln(20nS^2N_k^2T)}{N_k(x_i,a_i)}} +\frac{3ln(20nS^2N_k^2T)}{N_k(x_i,a_i)}\right\}. \label{eq:event_2}
\end{align}
\begin{lem}
    \label{lem:high_event2}
    $\Proba{\gE_2(T)}\ge 1-\frac1{2T}$.
\end{lem}
\begin{proof}
For every episode $k\ge1$, $P_k$ and $P$ are identically distributed.
With a slightly abuse of notation, we have the following inequality that is similar to \eqref{eq:bern_concentration}
\begin{align*}
    \Proba{|\Phat_\ell-P_{\ell+1}|\le\sqrt{\frac{2\Phat_\ell(1-\Phat_\ell)\varepsilon}{\ell}} +\frac{3\varepsilon}{\ell}} \ge 1-3e^{-\varepsilon}.
\end{align*}
We follow the same process in the proof of Lemma~\ref{lem:high_event1}.
\end{proof}

Finally, we consider the following event
\begin{align}
    \gE(T){:=}\left\{\forall i,x_i,a,y_i,k\ge0: |P_{k+1}^a(x_i,y_i) {-}P^a(x_i,y_i)|\le \sqrt{\frac{2\Phat_{k}(1-\Phat_k)ln(20nS^2N_k^2T)}{N_k(x_i,a_i)}} +\frac{3ln(20nS^2N_k^2T)}{N_k(x_i,a_i)}\right\}. \label{eq:event}
\end{align}

\begin{lem}
    \label{lem:high_event}
    $\Proba{\gE(T)}\ge 1-\frac1{T}$.
\end{lem}
\begin{proof}
The complement of $\gE$ is smaller than the union of $\bar{\gE}_1$ and $\bar{\gE}_2$.
Then, by union bound, $\Proba{\bar{\gE}}\le \Proba{\bar{\gE}_1} +\Proba{\bar{\gE}_2}$.
We use Lemmas~\ref{lem:high_event1} and \ref{lem:high_event2} to conclude the proof.
\end{proof}

Note that this high probability does not depends on the episode $k$.

\begin{lem}
    \label{lem:global_conc}
    For any episode $k\ge1$, $\vx\in\bar{\gS}$, $\va\in\bar{\gA}$, we have
    \begin{align}
        \sum_{\vy\in\bar{\gS}}|\mP_k^{\va}(\vx,\vy) -\mP^{\va}(\vx,\vy)| 
        \le \sum_{i=1}^{n} \sum_{y_i\in\gS_i} |P_k^{a_i}(x_i,y_i) -P^{a_i}(x_i,y_i)|.
    \end{align}
\end{lem}
\begin{proof}
    To proof...
\end{proof}

\begin{lem}
    \label{lem:nb_episodes}
    The number of episodes $K_T$ is bounded as follows:
    \begin{align}
        K_T\le2\sqrt{nST\ln(T)}.
    \end{align}
\end{lem}
\begin{proof}
    To proof
\end{proof}


\section{Open questions}

\begin{itemize}
    \item Does the above properties have implications on regret? 
    \item What about Lazy MDPs? \emph{i.e.} chains such that $P(i | i,a)>\varepsilon$ for all states and actions.
    \item If RMAB problem is in stationary regime, what is the regime of each arm?
\end{itemize}

\endgroup
