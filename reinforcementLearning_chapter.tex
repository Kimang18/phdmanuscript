\begingroup

\let\clearpage\relax

\chapter{Reinforcement Learning}
\label{ch:rl}

\KK{change $n$ to $N$}

In the previous chapter, we used the formalism of MDPs to describe how a decision maker interacts with its environment.
Depending on the chosen optimality criterion, we explained how an optimal policy can be obtained when the parameters of the MDP are fully known.
In this chapter, we consider the case when the MDP's parameters are \textbf{unknown} and need to be learned by the decision maker via trial and error.
Also, we will use the term ``learner'' instead of\footnote{We use the term ``decision maker'' when the parameters are known and ``learner'' when the parameters are unknown.} ``decision maker''.

In Section~\ref{ch:rl:sec:overal}, we describe the existing settings in reinforcement learning (RL) framework.
Then, we present the learning setting considered in this thesis and performance measure, namely the regret. 
In Section~\ref{ch:rl:sec:baseline}, we give the regret lower bound for any learning algorithms.
We finish the chapter by presenting two learning approaches known as optimism in face of uncertainty principle and posterior sampling in Section~\ref{ch:rl:sec:opt_post}.
We discuss several algorithms with regret guarantee at the end of this chapter.

The reader can skip this chapter and go directly to Chapter~\ref{ch:mb} and Part~\ref{part:idx} and come back to this chapter before diving into Part~\ref{part:learning}.

\section{A brief summary of existing learning setups}
\label{ch:rl:sec:overal}

\subsection{Models and paradigms of learning}

Fundamentally, a RL problem consists of a learner interacting with an environment modeled by an MDP $M=\langle\gS,\gA,r,p\rangle$.
The state and action spaces $\gS$ and $\gA$ are known to the learner but the expected value of reward $r$ and state transition probabilities $p$ are \textbf{unknown}.
So, we say that the MDP $M$ is unknown to the learner.
The goal of the learner is to identify an optimal policy $\pi^*$ by interacting with the unknown MDP.

%From the previous chapter, an optimal policy $\pi^*$ can be computed using the planning methods such as policy iteration, value iteration or backward induction. However, these methods require the knowledge about $r$ and $p$.
%So, $\pi^*$ cannot be immediately obtained, and the learner needs to interact with the MDP in order to collect observations that are used to make inference about $\pi^*$.

%While $\pi^*$ is ultimately identified, two learning mechanisms are defined:
In the literature of RL, there are mainly two distinguishable learning models:
\begin{enumerate}[label=(\roman*)]
    \item \textbf{Generative model}: the learner chooses the state of the unknown MDP $M$ at desire
    \item \textbf{Navigating model}: the learner has no control over the state of $M$.
\end{enumerate}
With the generative model, the learner picks any state-action pair $(s,a)$ and the MDP $M$ incurs a random reward $u$ whose expected value is $r(s,a)$ and the next state $s'\in\gS$ with probability $p(s'\mid s,a)$.
The learner collects the sample $\{u,s'\}$ related to the pair $(s,a)$ and the process is repeated.
This mechanism is used in \textbf{offline learning paradigm} (see  \eg, \cite{lange2012batch, levine2020offline}) in which the learner starts by collecting samples until a budget resource fixed by the setting is exhausted (it can be a time resource, sample resource or approximation error). Then, the learner plans a policy based on the collected samples and follows the policy.
No more policy update is made thereafter.

With the navigating model, the learner observes the current state $s$ of the MDP $M$ and executes an action $a$ and the MDP $M$ incurs a random reward with expected value $r(s,a)$ and transitions to state $s'$ with probability $p(s'\mid s,a)$.
The learner cannot force the MDP to restart in any state.
If the learner wants the MDP to be in a certain state, it has to go along the trajectory that brings the MDP from its current state to the desired state.
In some settings like finite horizon or infinite horizon with discount, the MDP $M$ can restart, but the learner has no control over the restart such as when and in which state to restart.
This mechanism is used in \textbf{online learning paradigm} in which the learner keeps updating its policy based on the observations it collects via the interaction with the MDP $M$ (see \eg, \cite{jaksch2010near, osband2013more, azar2017minimax, ouyang2017learning,zanette2019tighter}).
The policy update can be done in every decision time --known as real-time update--, periodically or when some conditions are met --known as \emph{episodic update}.

\subsection{Classification of learning algorithms}
\label{ch:rl:ssec:class_algo}

%For both paradigms, an optimal policy $\pi^*$ is computed via two different ways
In either paradigm described above, there are two types of learning
\begin{itemize}
    \item \textbf{model-free}: the learner tries to infer the value functions in the MDP;
    \item \textbf{model-based}: the learner tries to infer the unknown parameters $r$ and $p$ of the MDP.
\end{itemize}
The typical algorithm for model-free methods is \textbf{Q-learning} \cite{watkins1989learning} that tries to infer the optimal state-action value function via stochastic approximation. 
Model-free method is very appealing when the MDP has a large or even continuous state and action spaces (see \eg, \cite{mnih2015human, bellemare2017distributional, dabney2018distributional}).
However, model-free methods are generally slower than model-based ones in learning an optimal policy.

Model-based algorithms input the estimates of $r$ and $p$ to a planning method described in the previous chapter such as backward induction, policy or value iterations and follows the policy output by the planning method (see \eg, \cite{jaksch2010near, osband2013more, azar2017minimax}).
The main difference within model-based algorithms is the way $r$ and $p$ are estimated at each planning phase.
Model-based algorithms are costly for MDPs with large or continuous state and action spaces but when applicable, they are usually faster than model-free algorithms.

Last but not least, for both model-free and model-based methods, statistical inference provides two perspectives on the unknown MDP:
\begin{itemize}
    \item \textbf{frequentist perspective}, in which all quantities related to the unknown MDP such as $r$, $p$, value function, etc, are seen as \textbf{unknown deterministic} quantities,
    \item \textbf{Bayesian perspective}, in which all quantities related to the unknown MDP such as $r$, $p$, value function, etc, are seen as realization of \textbf{random variables}.
\end{itemize}
In frequentist school, the planning is decided before the learning begins, and the observations collected during the learning are used to controlled \textbf{the probability that the plan is correct} (see \eg, \cite{jaksch2010near, azar2017minimax, jin2018q, shi2022pessimistic}).
In Bayesian school, the observations collected during the learning are used to \textbf{update the posterior of the random variables}.
The planning is then done based on the posterior belief (see \eg, \cite{osband2013more, ouyang2017learning, bellemare2017distributional, dabney2018distributional}).

%We will discuss two families of model-based algorithms in the below sections.

\section{Learning setup studied in this thesis}
\label{sec:intro_learning}

%We focus on \textbf{model-based} algorithms in online learning with \textbf{episodic policy update}.
In this thesis, the learner interacts with an MDP $M=\langle\gS,\gA,r,p\rangle$ where the state space $\gS$ of size $\abs{\gS}=S$ and action space $\gA$ of size $\abs{\gA}=A$ are known but the expected value of reward $r$ and state transition probabilities $p$ are unknown.
Also, for any state-action pair $(s,a)$, the expected reward is bounded\footnote{in general, the expected reward is bounded in $[0, r_{max}]$ where $r_{max}\ge1$ but the interval can be simply scaled down to $[0,1]$.} $r(s,a)\in[0,1]$.
%That is, the parameters $r$ and $p$ are unknown to the learner.
At time step $t\ge1$, the MDP is in state $s_t$ and the learner executes an action $a_t$.
The MDP incurs a random reward denoted by $r_t$ and transitions to next state denoted by $s_{t+1}$.
This mechanism is repeated, and the learner observes a sequence of the form $\{s_1,a_1,r_1,s_2,\dots,s_t,a_t,r_t,s_{t+1},\dots\}$ that is called \emph{observations} (also known as a ``trajectory'').
At time step $t$, the learner has in disposition the observations up to time $t$ denoted by $o_t:=\{s_1,a_1,r_1,s_2,\dots,s_{t-1},a_{t-1},r_{t-1},s_{t}\}$.
The objective of the learner is to maximize the expected cumulative reward $\ex{\sum_{t\ge1}r_t}$ incurred by the MDP by identifying an optimal policy $\pi^*$ as early as possible.
When the context is clear, we will use the term ``algorithm'' to mean the learner.

We focus on \textbf{model-based} algorithms in online learning with \textbf{episodic policy update}.
We present the fundamental structure of episodic learning algorithms in Algorithm~\ref{algo:generics}.
Basically, the algorithm takes the state space $\gS$ and action space $\gA$ of MDP $M$ as input.
The initial state $s_1$ of $M$ can be drawn from some categorical distribution over the state space $\gS$.
Then, state $s_1$ is revealed to the learning algorithm.
The algorithm computes a policy $\pi^1$ and episode $1$ begins.
During episode $k\ge1$, the algorithm executes action $a_t=\pi^k(s_t)$ and collects observations $\{r_t, s_{t+1}\}$.
This process repeats until some specific condition depending on the setting is met.
The episode $k$ then terminates.
The algorithm computes a new policy $\pi^{k+1}$ based on its observations and episode $k+1$ begins.

\begin{algorithm}[ht]
    \DontPrintSemicolon
    \SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
    \Input{The MDP $M$ with state space $\gS$ and action space $\gA$}
    \BlankLine
    Set $t=1$ and observe initial state $s_1$ \;
    %Set $t=1$ and $t^1=1$ \;
    %Observe initial state $s_1$ \;
    \For{episodes $k=1,2,\dots$}{
        Set $t^k=t$ \;
        Compute a new policy $\pi^k$ \;\label{ch:rl:line:new_policy} %(using optimism or posterior sampling) \;\label{ch:rl:line:new_policy}
        \While{terminal condition is not met}{ \label{ch:rl:line:end_cond}
            Execute action $a_t=\pi^k(s_t)$ \;
            %Observe $o_t=o_t\cup\{s_t,a_t,r_t,s_{t+1}\}$. \;
            Observe $r_t$ and next state $s_{t+1}$ \;
            $t\leftarrow t+1$.
        }
    }
    \caption{Episodic learning algorithms}
    \label{algo:generics}
\end{algorithm}


%Note that the expected value of reward is bounded as $r(s,a)\in[0,1]$ for all $s\in\gS, a\in\gA$.

%This leads to two different probabilistic approaches described in Table~\ref{tab:compare}.
%
%\begin{table}[ht]
%    \center
%    \begin{tabular}{|p{0.45\textwidth}|p{0.45\textwidth}|}
%        \hline
%        \textbf{Frequentist model} & \textbf{Bayesian model} \\ \hline
%        \begin{itemize}
%            \item $r$ and $p$ are unknown parameters
%        \end{itemize}
%        &
%        \begin{itemize}
%            \item $r$ and $p$ are drawn from a prior distribution
%        \end{itemize} \\ \hline
%    \end{tabular}
%    \caption{Comparison between frequentist and Bayesian modeling about the MDP ${M=\langle\gS,\gA,r,p\rangle}$}
%    \label{tab:compare}
%\end{table}

\section{Regret definition}

In online RL, there are at least two performance metrics: (1) probably approximately correct (PAC) bounds on the sample complexity (see \eg, \cite{brafman2002r, kearns2002near, kakade2003sample, dann2015sample, jiang2018open, wang2020long}) and (2) regret (see \eg, \cite{jaksch2010near, osband2013more, azar2017minimax, jin2018q, zanette2019tighter, zhang2019regret}).
However, it is shown that both metrics are ``equivalent'': PAC bounds imply regret bounds and regret bounds imply PAC bounds \cite{osband2016deep}.
Hence, the recent works often discuss both metrics (see \eg, \cite{he2021nearly, zhang2021reinforcement}).

In this thesis, we measure the performance of the learner using a notion of \emph{regret} that compares the expected cumulative reward of an optimal policy $\pi^*$ to the cumulative reward of the learner. 

If the learner $\gL$ interacts with the unknown MDP $M$ over $T\in\N^+$ time steps, we denote by $\Reg(\gL, M, T)$ the regret suffered by the learner.
So, maximizing the expected cumulative reward is equivalent to minimizing the expected regret, $\ex{\Reg(\gL, M, T)}$.

The formal definition of regret depends on the setting of learning problem.
In this section, we recall the definition in two settings: finite horizon and infinite horizon average reward criterion.

\subsection{Finite horizon setting}
In this setting, the learner interacts with the unknown MDP $M$ over $K\in\N^+$ episodes, each episode lasts in $H\in\N^+$ time steps (so the total time steps is $T=KH$) and the state of $M$ is reset according to a distribution $\rho$.
The terminal condition in Line~\ref{ch:rl:line:end_cond} of Algorithm~\ref{algo:generics} is simply $t-t^k=H$.
%That is, episode $k$ terminates after the learner interacts for $H$ time steps.
In such a setting, $H$ is called \emph{horizon} and it is bounded in $\N^+$.
At the beginning of each episode $k\ge1$, the initial state of the MDP $s_{t^k}\sim\rho$ is revealed to the learner who computes a policy $\pi^k$ based on its collected observations and follows the policy $\pi^k$ during the episode.
%After $H$ time steps, the state of the MDP is reset according to a distribution $\rho$ and revealed to the learner who computes a policy $\pi^k$ based on its collected observations and rollouts the policy $\pi^k$ during the episode.
Following the definition in Section~\ref{ch:mdp:sec:finite}, we denote by $w^{\pi^k}_{1:H}(s):=\E^{\pi^k}[\sum_{t=1}^{H}r_t \mid s_1=s]$ the expected cumulative reward from state $s$ over time steps $1$ to $H$.
The regret is defined by the following.
\begin{defn}
    If a learner $\gL$ interacts with an unknown MDP $M$ over $K$ episodes, each episode ends in $H\in\N^+$ time steps, then its regret is
    \begin{equation}
        \label{eq:rg_finite}
        \Reg(\gL, M, K):= \sum_{k=1}^K\sum_{s\in\gS}\rho(s)[w^*_{1:H}(s) -w^{\pi^k}_{1:H}(s)].
    \end{equation}
    \label{ch:rl:defn:rg_finite}
\end{defn}
Since $H$ is bounded, we derive from Definition~\ref{ch:rl:defn:rg_finite} that a ``good'' learner has an expected regret sublinear in the number of episodes $K$, $\ex{\Reg(\gL, M, K)}=\landauo(K)$ when $K\to+\infty$.

\subsection{Infinite horizon average reward criterion}
For this setting, there is no reset on MDP state but the unknown MDP $M$ is assumed to be \emph{weakly communicating}.
This assumption is required so that the optimal gain in $M$ is state independent.
The learner interacts with $M$ over $T$ time steps and collects a sequence of rewards $\{r_t\}_{1\le t\le T}$.
The terminal condition at Line~\ref{ch:rl:line:end_cond} of Algorithm~\ref{algo:generics} is specified by the learner depending on how often it wants to update its policy.
The regret is defined by the following.
\begin{defn}
    If a learner $\gL$ interacts with an unknown MDP $M$ which is weakly communicating, then its regret after $T$ time steps is
    \begin{equation}
        \label{eq:rg_infinite}
        \Reg(\gL, M, T):= Tg^* -\sum_{t=1}^Tr_t
    \end{equation}
    where $g^*$ is the optimal gain of $M$ (see its definition in Section~\ref{ch:mdp:sec:gain}).
    \label{ch:rl:defn:rg_infinite}
\end{defn}
So, a learner is ``good'' if its expected regret is sublinear in the total number of time steps $T$, \ie, $\E\Big[\Reg(\gL, M, T)\Big]= \landauo(T)$ when $T\to+\infty$.

%A notable difference between finite and infinite horizon settings is that in finite setting, the learner updates its policy right before the episode starts and never within the episode while in infinite setting, the learner decides by itself when to update its policy.

%\section{Regret benchmarks}
%\label{sec:rg_benchmark}

%In this section, we describe... 

\section{Minimax regret lower bound}
\label{ch:rl:sec:baseline}

%We compare the exploration-exploitation performance of good learners by how fast their regret tends to zero.
We compare the performance of good learners by how fast their regret tends to zero.
\cite{jaksch2010near} has proven that no learners can perform better than a certain baseline for all MDPs $M$.
We will give this baseline below, but first we need to introduce the notion of diameter of an MDP.
\begin{defn}
    The diameter of an MDP is defined by
    \begin{equation}
        \label{eq:diameter}
        D := \max_{s,s'\in\gS}\min_{\pi:\gS\mapsto\gA} \E^\pi[\tau(s') \mid s_1=s] -1
    \end{equation}
    where $\tau(s'):=\inf\{t\ge2 : s_t=s'\}$ is the first time step when $s'$ is reached.
    \label{ch:rl:defn:diameter}
\end{defn}
So, the diameter of an MDP is the length of the \textbf{longest shortest path} in the MDP where the distance between any pair of vertices is $2$.
In other words, it is the expected number of vertices along the shortest path between two states that are the \textbf{most distant} from each other.
From Definition~\ref{ch:mdp:defn:mdp_class} and \cite[Proposition~8.3.1]{puterman2014markov}, it is clear that the diameter $D$ is finite if and only if the MDP is communicating.

Given any learner $\gL$, there always exists an unknown MDP $M$ that slows down the learner $\gL$ as the following.
\begin{prop}[{\cite[Theorem~5]{jaksch2010near}}]
    \label{prop:minimax_rg_lb}
    In infinite horizon setting, for any learner $\gL$, any integers $S$, $A\ge10$, $D\ge 20\log_{A}(S)$, and $T\ge DSA$, there is an MDP $M=\langle\gS,\gA,r,p\rangle$ whose diameter is $D$ such that for any initial state $s_1$, the expected regret of $\gL$ after $T$ time steps is lower bounded as
    \begin{equation}
        \label{eq:minimax_rg_lb}
        \ex{\Reg(\gL, M, T)} \ge 0.015\sqrt{DSAT}.
    \end{equation}
\end{prop}
This proposition means that no matter how ``good'' a learner is, it is always possible to construct a worst-case MDP $M$ having $S$ states, $A$ actions and diameter $D$ such that the learner suffers a regret $\landauOmega(\sqrt{DSAT})$ after $T$ time steps in $M$.
This bound on worst-case regret is often referred as ``minimax'' bound.

The minimax bound is different from the bounds given in \cite{ok2018exploration, burnetas1997optimal} which are problem-dependent and asymptotic bounds.
Minimax bounds usually scale as $\sqrt{T}$ while problem-dependent bounds scale logarithmically\footnote{In the bandit literature, “problem-dependent” bounds are said to be distribution-dependent, as opposed to minimax bounds which are said to be distribution-free \cite{garivier2019explore}} with $T$.
We refer to \cite{ok2018exploration, burnetas1997optimal} for more detail about these problem-dependent bounds.
%We do not recall those problem-dependent bounds because they are not necessary to understand the thesis.

Minimax bound is often expressed in terms of span (or range) of the optimal bias function as well.
In fact, the specific worst-case MDP constructed by \cite{jaksch2010near} to prove the lower bound in Proposition~\ref{prop:minimax_rg_lb} satisfies $D=2sp(\vh^*)$ where $\vh^*$ is the optimal bias function (see Section~\ref{ch:mdp:sec:gain} for its definition).
If $H$ is an upper bound on $sp(\vh^*)$, then the minimax bound is also expressed as $\landauOmega(\sqrt{HSAT})$.
In recent work of \cite{zhang2019regret}, the proposed algorithm, albeit no efficient implementations are given, achieves a regret upper bounded by $\tilde{\landauO}(\sqrt{HSAT})$ (see \cite[Theorem~1]{zhang2019regret}).
This result suggests that the minimax bound in Proposition~\ref{prop:minimax_rg_lb} cannot be improved.

For finite horizon setting, the diameter of the MDP or the upper bound on span of optimal bias is replaced with the horizon of an episode.
So, the minimax bound can be immediately deduced from Proposition~\ref{prop:minimax_rg_lb}: for any learner, it is always possible to construct a worst-case MDP with $S$ states and $A$ actions such that after $K$ episodes, each of horizon\footnote{For stochastic bandit, we have $H=1, S=1$ and $K=T$. So, the minimax bound is $\landauOmega(\sqrt{AT})$ as given in \cite{bubeck2012regret}} $H$, the learner suffers a regret\footnote{In time-inhomogeneous finite horizon setting, the minimax bound is $\landauOmega(H\sqrt{HSAK})$ (see, \eg, \cite{jin2018q, domingues2021episodic}).} $\landauOmega(H\sqrt{SAK})$.


\section{Algorithms with regret guarantee}
%\section{Optimism and posterior sampling}
\label{ch:rl:sec:opt_post}

To get enough information for deriving $\pi^*$, the learner needs to \emph{explore} the dynamic of the MDP as much as possible.
However, too much exploration equalizes the performance of the learner with one that blindly chooses action at random.
So, the regret is linear in the total number of time steps.
A good learner should then \emph{exploit} the gathered information as soon as possible.
Unfortunately, untimely exploitation leads to suboptimal policy, thus a regret that is linear in $T$. This is the famous “exploration versus exploitation dilemma”.

To manage exploration-exploitation dilemma, the two perspectives from statistical inference that we mentioned in Section~\ref{ch:rl:ssec:class_algo} are adopted. % in the following way:
%\begin{enumerate}
%    \item \textbf{frequentist} point of view, in which $r$ and $p$ are seen as unknown \textbf{deterministic parameters} of the MDP $M$,
%    \item \textbf{Bayesian} point of view, in which $r$ and $p$ of $M$ are \textbf{random variables}, drawn from some prior distribution.
%\end{enumerate}
In frequentist perspective, the learner $\gL$ that maximizes the expected cumulative reward over $T$ time steps equivalently minimizes its expected regret $\E\bigg[\Reg(\gL, M, T)\bigg]$.
To achieve this goal, a common strategy in frequentist paradigm is to apply \emph{optimism in face of uncertainty} (OFU) principle: the learner maintains a confidence set for the unknown MDP $M$ and executes an optimal policy of the
“best” MDP in the confidence set (it is the best in terms of gain, or value function, etc), \eg, \cite{jaksch2010near, filippi2010optimism, bartlett2012regal, azar2017minimax, fruit2017regret, jin2018q, fruit2018efficient, fruit2018near, zanette2019tighter, zhang2019regret, bourel2020tightening, ortner2020regret}.

In Bayesian perspective, we say the unknown MDP $M$ is drawn from a probability distribution (prior or posterior) $\phi$ to mean that $r$ is drawn from a dedicated probability distribution (prior or posterior) and $p$ is drawn from another dedicated probability distribution (prior or posterior).
The learner minimizes a similar notion of regret known as \emph{Bayesian regret} (or Bayes risk) defined by
\begin{equation}
    \label{eq:bayes_rg}
    \BayReg(\gL, \phi, T):=\ex{\E\bigg[\Reg(\gL, M, T) \mid M\bigg]}
\end{equation}
where $\phi$ is the prior distribution of the unknown MDP $M$.
%So, the learner $\gL$ maximizes the expected cumulative rewards from $M$ over $T$ time steps by minimizing its Bayesian regret $\BayReg(\gL, \phi, T)$.
In Bayesian paradigm, an efficient exploration-exploitation trade-off can be done by posterior sampling introduced by \cite{thompson1933likelihood}: the learner keeps a posterior distribution over possible MDPs (precisely, the support of the prior distribution) and executes an optimal policy of a sampled MDP, see \eg, \cite{osband2013more, gopalan2015thompson, ouyang2017learning}.

We summarize several existing results about minimizing the regret in Table~\ref{ch:rl:tab:finite} for finite horizon setting and Table~\ref{ch:rl:tab:infinite} for infinite horizon average reward criterion.
%The results in Table~\ref{ch:rl:tab:finite} are fewer than those in Table~\ref{ch:rl:tab:infinite} because 
The results in Table~\ref{ch:rl:tab:finite} are few because the minimax lower bound in the presented finite horizon setting is achieved quite early (ignoring the logarithmic terms).
However, there are a lot of works for finite horizon model such as  \cite{jin2018q, domingues2021episodic, li2021breaking} in which the reward and state transition also depend on the time step (also known as time-inhomogeneous, or non-stationary MDP) or \cite{zanette2019tighter, zhang2021reinforcement} in which the total reward over $H$ time steps is bounded by $1$.

The quest to minimax regret lower bound for infinite horizon is longer compared to finite horizon setting.
With average reward criterion, the structure of the unknown MDP is vital.
So, the works in Table~\ref{ch:rl:tab:infinite} make different assumption on the MDP in function of the nature of the proposed algorithms.
In addition, policy computation is also an important aspect for infinite horizon setting.
Some algorithms in Table~\ref{ch:rl:tab:infinite} only have theoretical regret guarantee and cannot be implemented efficiently.
That is, either the implementation of those algorithms takes unreasonably long time to run or there is no possible implementations.
So, we say that they are intractable.
However, the algorithms that use value iteration, extended value iteration or modified extended value iteration are implementable, and their computational complexity is $\landauO(S^2A)$ \cite{jaksch2010near}.
Last but not least, Bayesian algorithms like PSRL and TSDE use planning methods described in Chapter~\ref{ch:mdp} for policy computation.
It might seem that these algorithms are computationally ``efficient'' but they are ``easily implementable'' if the conjugate prior has a closed-form expression.
Otherwise, a sophisticated implementation might be required.

\begin{table}[ht]
    \centering
\begin{tabular}{|l|l|}
\hline
Algorithm & Regret \\ \hline
PSRL \cite{osband2013more}  & $\tilde{\landauO}(HS\sqrt{AT})$ \\ 
UCBVI-BF \cite{azar2017minimax}  & $\tilde{\landauO}(\sqrt{HSAT})$ \\ 
EULER \cite{zanette2019tighter}        & $\tilde{\landauO}(\sqrt{HSAT})$ \\ \hline
Lower bound & $\landauOmega(\sqrt{HSAT})$ \cite{jaksch2010near} \\ \hline
\end{tabular}
\caption{The quest to minimax regret lower bound for model-based RL algorithms in finite horizon setting with $S$ states, $A$ actions, and $T=KH$ steps.
$K$ is the total number of episodes and $H$ is horizon of each episode.
}
\label{ch:rl:tab:finite}
\end{table}

\begin{table}[ht]
\begin{tabular}{|l|l|l|l|}
\hline
Algorithm & Regret & Assump. on $M$ & Policy comp.            \\ \hline
UCRL2 \cite{jaksch2010near}       & $\tilde{\landauO}(DS\sqrt{AT})$ & comm.         & EVI \\ 
REGAL \cite{bartlett2012regal}      & $\tilde{\landauO}(HS\sqrt{AT})$  & weakly comm. & Intractable   \\ 
TSDE \cite{ouyang2017learning}        & $\tilde{\landauO}(HS\sqrt{AT})$ & weakly comm. & VI       \\ 
SCAL \cite{fruit2018efficient}        & $\tilde{\landauO}(HS\sqrt{AT})$ & weakly comm. & modified EVI           \\
OSP \cite{ortner2020regret}         & $\tilde{\landauO}(\sqrt{t_{mix}SAT})$ & ergodic           & Intractable   \\ 
UCRL2B \cite{fruit2020improved}      & $\tilde{\landauO}(\sqrt{\Gamma DSAT})$ & comm.      & EVI                           \\
%KL-UCRL     & $\tilde{O}$(\sqrt{S\sum_{x,a}\sV^*_{x,a}T}) & ergodic                     & modified EVI                  \\ \hline
EBF \cite{zhang2019regret}         & $\tilde{\landauO}(\sqrt{HSAT})$ & weakly comm.     & Intractable   \\
UCRL-V \cite{tossou2019near}         & $\tilde{\landauO}(\sqrt{DSAT})$ & comm.     & modified EVI   \\ \hline
Lower bound & $\landauOmega(\sqrt{DSAT})$ \cite{jaksch2010near} &                              &                               \\ \hline
\end{tabular}
\caption{The quest to minimax regret lower bound for model-based RL algorithms in infinite horizon average reward model with $S$ states, $A$ actions, and $T$ steps.
$D$ is the diameter of the MDP, $H\ge sp(\vh^*)$ is the upper bound on the span of the optimal bias, $t_{mix}$ is the mixing time of the MDP and $\Gamma$ is the upper bound on the number of next possible states.
EVI stands for Extended value iteration \cite{jaksch2010near}, VI for value iteration, assump. for assumption, comm. for communicating, and comp. for computation.
Intractable here means that there is no efficient implementations.
}
\label{ch:rl:tab:infinite}
\end{table}

% No need to add table of summary because finite horizon is not the real setting of our learning problem
% Only infinite horizon is really related

\section{Overview of regret analysis}

%From now on, a learner is an algorithm for the learning problem.
As mentioned in Section~\ref{sec:intro_learning}, we consider \textbf{model-based} algorithms with \textbf{episodic policy update}.
Algorithm~\ref{algo:generics} shows how episodic algorithms work during the learning.
%The two approaches compute an optimal policy of an ``imagined MDP'' that they believe to be the unknown MDP $M=\langle\gS,\gA,r,p\rangle$.
%Precisely, for both approaches, Line~\ref{line:new_policy} of Algorithm~\ref{algo:generics} can be broken down into two steps
For both optimism and posterior sampling methods, computing a new policy at Line~\ref{ch:rl:line:new_policy} of Algorithm~\ref{algo:generics} can be broken down into two steps
\begin{enumerate}[label=(\roman*)]
    \item \label{it:new_mdp} compute an estimate $M^k=\langle\gS,\gA,r^k,p^k\rangle$ of the unknown MDP $M$;
    \item compute an optimal policy $\pi^k$ of $M^k$.
\end{enumerate}
We will say that $M^k$ is the imagined version of the unknown $M$ for episode $k$.
%at the beginning of episode $k\ge1$, the learner computes a new policy denote by $\pi^k$ that is optimal in an imagined MDP denote by $M^k=\langle\gS,\gA,r^k,p^k\rangle$.
%The learning process is summarized in Algorithm~\ref{algo:generics}.

Since an optimal policy can be computed in a deterministic manner given the MDP, the ``high-level'' difference between both approaches lies in Step~\ref{it:new_mdp} at which the imagined MDP $M^k$ is constructed: chosen ``optimistically'' by OFU method or chosen randomly by posterior sampling according to its belief (or the posterior distribution).

%\begin{algorithm}[ht]
%    \DontPrintSemicolon
%    \SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
%    \Input{The unknown MDP $M=\langle\gS,\gA,r,p\rangle$}
%    \BlankLine
%    Observe initial state $s_1$ \;
%    \For{episodes $k=1,2,\dots$}{
%        Compute a new policy $\pi^k$ (using optimism or posterior sampling) \;\label{ch:rl:line:new_policy}
%        \While{terminal condition is not met}{
%            Execute action $a_t=\pi^k(s_t)$ \;
%            %Observe $o_t=o_t\cup\{s_t,a_t,r_t,s_{t+1}\}$. \;
%            Observe $r_t$ and next state $s_{t+1}$. \;
%        }
%    }
%    \caption{Episodic learning algorithms}
%    \label{algo:generics}
%\end{algorithm}

In the following, we provide an overview of regret analysis in the finite horizon setting. 
The analysis will involve value function associated to different MDPs.
So, we extend the notation in Chapter~\ref{ch:mdp} as the following
\begin{itemize}
    \item the expected cumulative reward from the unknown MDP $M$ between time steps $h$ and $H$ is denoted by $w^{\pi^*}_{M,h:H}$ respectively where $\pi^*$ is an optimal policy in $M$;
    \item the expected cumulative reward from the imagined MDP $M^k$ between time steps $h$ and $H$ is denoted by $w^{\pi^k}_{M^k,h:H}$ respectively where $\pi^k$ is an optimal policy in $M^k$.
\end{itemize}

By Definition~\ref{ch:rl:defn:rg_finite}, the regret of a learner $\gL$ after $K$ episodes in an unknown MDP $M$ can be written
\begin{align*}
    \Reg(\gL, M, K) &= \sum_{k=1}^K\sum_{s\in\gS}\rho(s)[w^{\pi^*}_{M,1:H}(s) -w^{\pi^k}_{M,1:H}(s)].
                    %&= \sum_{k=1}^K \sum_{s\in\gS}\rho(s) [w^{\pi^*}_{M,1:H}(s) -w^{\pi^k}_{M^k,1:H}(s)] +\sum_{k=1}^K \sum_{s\in\gS}\rho(s) [w^{\pi^k}_{M^k,1:H}(s) -w^{\pi^k}_{M,1:H}(s)]
\end{align*}
The term $\vw^{\pi^*}_M -\vw^{\pi^k}_M$ can be rewritten as
\begin{equation}
    \label{eq:rg_model_conc}
    \vw^{\pi^*}_M -\vw^{\pi^k}_M = \underbrace{(\vw^{\pi^*}_M -\vw^{\pi^k}_{M^k})}_{=:\Delta_{model}^k} +\underbrace{(\vw^{\pi^k}_{M^k} -\vw^{\pi^k}_M)}_{=:\Delta_{conc}^k}
\end{equation}
The first term $\Delta_{model}^k$ is the difference between the real but \textbf{unknown} optimal value function and the \textbf{imagined} optimal value function.
It is also understood as the error due to model misspecification.
Since $M$ and $\pi^*$ are unknown, this first term is hard to be analyzed.
However, we will see in the following that the term is non-positive for \textbf{optimism} or zero in expectation for \textbf{posterior sampling}.

The second term $\Delta_{conc}^k$ can be interpreted as the ``dissimilarity'' between the \textbf{unknown} MDP $M$ and the \textbf{imagined} MDP $M^k$ along the trajectory induced by the policy $\pi^k$.
In other words, it is the discordance between the value from $M^k$ and the value from $M$ for policy $\pi^k$.
Since $M^k$ and $\pi^k$ are chosen by the learner, this second term can be analyzed and both approaches upper bound this term using concentration inequalities.

\section{Sketch of proof on regret bound of three algorithms}

In this section, we revisit three algorithms: UCRL2 \cite{jaksch2010near} and UCBVI \cite{azar2017minimax}, which rely on OFU method, and PSRL \cite{osband2013more}, which is a posterior sampling algorithm.
We give the sketch of proof for their regret upper bound in finite horizon setting although UCRL2 is originally design for infinite horizon average reward criterion.
We will give a detail proof in Chapter~\ref{ch:learning_rested} when we derive the three algorithms for Markovian bandit problem.

\subsection{Upper confidence bound reinforcement learning (UCRL2) \texorpdfstring{\cite{jaksch2010near}}{[JOA10]}.}
As mentioned above, UCRL2 was originally design for infinite horizon setting.
%However, it still proceeds in episodic manner: the algorithm decides to update its policy when the number of visits to a state-action pair has doubled since the end of the previous episode and the new episode begins.
%The number of visits to state-action pair $(s,a)$ means how many time steps so far that the action $a$ has been executed when the unknown MDP is in state $s$.
%When updating the policy, UCRL2 compute the maximum-likelihood estimates of $r$ and $p$ as well as confidence sets 
Here, we adapt UCRL2 to finite horizon setting.
When updating the policy, UCRL2 constructs confidence sets $\sB_r$ and $\sB_p$ for $r$ and $p$ respectively based on high probability confidence bounds.
We denote by $\sM$ the set of plausible MDPs whose reward function lives in $\sB_r$ and state transition lives in $\sB_p$.
UCRL2 chooses the MDP that incurs the highest optimal value among the MDPs in $\sM$ and computes an optimal policy of the chosen MDP.
That is, at Line~\ref{ch:rl:line:new_policy} of Algorithm~\ref{algo:generics}, UCRL2 computes a new policy $\pi^k$ such that
\begin{equation}
    \label{ch:rl:eq:optim}
    \vw_{M^k}^{\pi^k} \ge \max_{\pi\in\Pi}\max_{M'\in\sM^k} \vw_{M'}^\pi
\end{equation}
where
\begin{align*}
    \sM^k:=\Big\{\langle\gS,\gA,r',p'\rangle :
    \text{ for all }(s,a), r'(s,a)\in\sB_r^k(s,a) \text{ and } \vp'(\cdot\mid s,a)\in\sB_p^k(s,a)\Big\}
\end{align*}
is the set of plausible MDPs compatible with the confidence sets for episode $k$.
To do so, UCRL2 uses extended value iteration (EVI).
This is because EVI is designed such that \eqref{ch:rl:eq:optim} holds.
We refer to \cite{jaksch2010near} for the detail about EVI and its convergence proof.

Let $t^k$ be the time step when the episode $k$ begins and $t^1:=1$ (see Algorithm~\ref{algo:generics}).
The observations up to time $t^k$ is denoted by $o_{t^k}{:=}\{s_1,a_1,r_1,\dots,s_{t^k-1},a_{t^k-1},r_{t^k-1},s_{t^k}\}$.
For all state-action pair $(s,a)$, let $n^k(s,a)$ be the number of times up to $t^k$ that the algorithm executes action $a$ when the MDP is in state $s$.
At time step $t^k$, UCRL2 uses the observations $o_{t^k}$ to compute $\hat{r}^k$ and $\hat{p}^k$, the empirical means of $r$ and $p$ respectively.
The confidence sets for reward and transition are then computed: for all state-action pair $(s,a)$,
\begin{align*}
    \sB_r^k(s,a)&:=\Bigl\{u\in[0,1] : \abs{u-\hat{r}^k(s,a)}\le\beta^k_r(s,a)\Bigr\}, \\
    \sB_p^k(s,a)&:=\Bigl\{\vq\in[0,1]^{\abs{\gS}}: \norm{\vq-\hat{\vp}^k(\cdot\mid s,a)}_{\ell_1}\le \beta^k_p(s,a)
    \text{ and }\norm{\vq}_{\ell_1}=1\Bigr\}
\end{align*}
where $\beta_r^k(s,a)=\displaystyle\sqrt{\frac{c_r\ln(SAt^k)}{2\max\Bigl(1,n^k(s,a)\Bigr)}}$ and $\beta_p^k(s,a)=\displaystyle\sqrt{\frac{c_pS\ln(At^k)}{\max\Bigl(1,n^k(s,a)\Bigr)}}$ are the confidence bonuses, and $c_r$ and $c_p$ are constants to be chosen accordingly before the learning.

The set of plausible MDPs $\sM^k$ is constructed to contain the unknown $M$ with high probability.
Hence, by \eqref{ch:rl:eq:optim}, $\vw^{\pi^k}_{M^k}\ge \vw^{\pi^*}_M$ with high probability. 
%This means that with high probability, $\vw^{\pi^*}_M- \vw^{\pi^k}_{M^k}\le \vzero$.
In consequence, \Eqref{eq:rg_model_conc} implies that the regret of UCRL2 is smaller than ${\sum_{k=1}^K\sum_{s\in\gS}\rho(s)[\Delta_{conc}^k(s)]}$ with high probability.
Using Bellman evaluation equations, the term $\Delta_{conc}^k(s)$ can be rewritten as
\begin{align}
    w^{\pi^k}_{M^k,1:H}(s_1) -w^{\pi^k}_{M,1:H}(s_1)
    &= r^k(s_1,a_1)+\sum_{s'\in\gS}p^k(s'\mid s_1,a_1)w^{\pi^k}_{M^k,2:H}(s') \nonumber\\
    &\qquad -r(s_1,a_1)-\sum_{s'\in\gS}p(s'\mid s_1,a_1)w^{\pi^k}_{M,2:H}(s') \label{eq:decom}
\end{align}
where $a_1=\pi^k_1(s_1)$ and $a_t=\pi^k_t(s_t)$ for any $1\le t\le H$.
The term $\vp^k\vw^{\pi^k}_{M^k}-\vp\vw^{\pi^k}_M$ equals $(\vp^k-\vp)\vw^{\pi^k}_{M^k} +\vp(\vw^{\pi^k}_{M^k}-\vw^{\pi^k}_M)$.
Since $w^{\pi^k}_{M^k,h:H}(s)\in [0,H]$ for any $h\le H$ and $s\in\gS$ is not deterministic, the term $(\vp^k-\vp)\vw^{\pi^k}_{M^k}$ is bounded using Hölder's inequality.
So, we get
\begin{equation}
    \label{ch:rl:eq:delta_conc}
    \Delta_{conc}^k(s_1)\le\sum_{t=1}^{H} \abs{r^k(s_t,a_t)-r(s_t,a_t)}{+}H\norm{\vp^k(\cdot\mid s_t,a_t)-\vp(\cdot\mid s_t,a_t)}_{\ell_1} +d_t
\end{equation}
where $d_{t-1}=\vp(\cdot\mid s_{t{-}1},a_{t{-}1})\Bigl(\vw^{\pi^k}_{M^k,t:H} {-}\vw^{\pi^k}_{M,t:H}\Bigr) {-}\Bigl(w^{\pi^k}_{M^k,t:H}(s_{t}) -w^{\pi^k}_{M,t:H}(s_{t})\Bigr)$.
So, $\{d_t\}_{1\le t\le H}$ is a martingale difference sequence, each term upper bounded by $H$.
The sum $\sum_{k=1}^K\sum_{t=1}^Hd_t$ can be bounded using Azuma-Hoeffding's inequality.
Moreover, if the unknown $M$ belongs to the plausible set $\sM^k$, then $\abs{r^k-r}$ and $\norm{\vp^k-\vp}_{\ell_1}$ can be bounded by the confidence bonuses $\beta_r^k$ and $\beta_p^k$ respectively.
So, the regret of UCRL2 is bounded.

%If $M$ is communicating with diameter $D$, then any MDP in $\sM$ is communicating with a diameter bounded by $D$ (see \cite[Section~4.3.1]{jaksch2010near}).
For the sake of completeness, we recall the result of \cite{jaksch2010near} for infinite horizon setting below.
\begin{prop}[{\cite[Theorem~2]{jaksch2010near}}]
    \label{ch:rl:prop:uclr2}
    For any communicating MDP $M$ with $S$ states, $A$ actions and diameter $D$, with probability at least $1-\delta$, it holds that for any $T>1$, the regret of UCRL2 is bounded by
    \begin{align*}
        \Reg(\textnormal{UCRL2}, M, T) \le 34DS\sqrt{AT\ln\Bigl(\frac{T}{\delta}\Bigr)}.
    \end{align*}
\end{prop}
Compare to Proposition~\ref{prop:minimax_rg_lb}, the upper bound in Proposition~\ref{ch:rl:prop:uclr2} is loose by a factor $\sqrt{DS}$ ignoring the logarithmic term.

\subsection{Upper confidence bound value iteration (UCBVI) \texorpdfstring{\cite{azar2017minimax}}{[AOM17]}.}
UCBVI is an optimistic algorithm designed for finite horizon setting.
The algorithm keeps track of the emprical mean $\hat{p}$ of $p$ and only constructs confidence set $\sB_r$ for $r$ based on high probability confidence bound.
Differently from UCRL2 that chooses only one copy of expected reward, UCBVI executes Line~\ref{ch:rl:line:new_policy} of Algorithm~\ref{algo:generics} by choosing $H$ copies in the following manner:
\begin{itemize}
    \item set $w^{\pi^k}_{H+1:H,M^k}(s)=0$ for all $s$,
    \item for $h$ from $H$ to $1$
        \begin{itemize}
            \item choose $r'(s,a)\in\sB^k_r(s,a,h)$ for all $(s,a)$ such that for all $s$,
            \begin{align}
                w^{\pi^k}_{h:H,M^k}(s) &=\max_{a\in\gA}\Big(r'(s,a)+\sum_{s'\in\gS}\hat{p}^k(s'\mid s,a)w_{h+1:H,M^k}^{\pi^k}(s')\Big) \label{ch:rl:eq:ucbvi_max}
                %\pi^k_h(s) &=\argmax_{a\in\gA}\Big(r'(s,a)+\sum_{s'\in\gS}\hat{p}^k(s'\mid s,a)w_{h+1:H,M^k}^{\pi^k}(s')\Big)
            \end{align}
            \item set $\pi^k_h(s)=a_h$ where $a_h$ is one of the actions that achieve the maximum of \eqref{ch:rl:eq:ucbvi_max}.
        \end{itemize}
\end{itemize}
The confidence sets for reward are defined by: for all state-action pair $(s,a)$ and all time step $1\le h\le H$,
\begin{align*}
    \sB_r^k(s,a,h)&:=\Bigl\{u\in[0,h] : \abs{u-\hat{r}^k(s,a)}\le\beta^k_r(s,a,h)\Bigr\}
\end{align*}
where $\beta_r^k(s,a,h)=\displaystyle h\sqrt{\frac{c_r\ln(SAt^k)}{2\max\Bigl(1,n^k(s,a)\Bigr)}}$ is the confidence bonus, and $c_r$ is a constant to be chosen accordingly before the learning.

%That is, at the start of episode $k$, UCBVI compute $M^k$ and $\pi^k$ such that \eqref{ch:rl:eq:optim} holds where
%\begin{align*}
%    \sM^k:=\{\langle\gS,\gA,r',\hat{p}^k\rangle : r'(s,a)\in\sB_r^k(s,a) \text{ for all }s\in\gS, a\in\gA_s\}
%\end{align*}
%is the set of plausible MDPs compatible with the confidence set.
%The confidence bonus for reward is computed by $\beta_r^k(s,a)=\displaystyle H\sqrt{\frac{c_r\ln(\abs{\gS}\abs{\gA}t^k)}{2\max\Bigl(1,n^k(s,a)\Bigr)}}$ where $c_r$ is a constant to be chosen accordingly before the learning.
%The set $\sM^k$ contains the unknown $M$ with high probability. So, $\vw^{\pi^*}_M- \vw^{\pi^k}_{M^k}\le \vzero$ holds with high probability.
It is shown in \cite{azar2017minimax} that $\vw^{\pi^*}_M- \vw^{\pi^k}_{M^k}\le \vzero$ holds with high probability.
By \Eqref{eq:rg_model_conc}, the regret of UCBVI is bounded by ${\sum_{k=1}^K\sum_{s\in\gS}\rho(s)[\Delta_{conc}^k(s)]}$ with high probability.
The theoretical novelty in UCBVI is to efficiently deal with the term $(\hat{\vp}^k-\vp)\vw^{\pi^k}_{M^k}$ when rewriting the right term of \eqref{eq:decom} as shown in UCRL2 technique.
Indeed, $(\hat{\vp}^k-\vp)\vw^{\pi^k}_{M^k}$ is also rewritten as $(\hat{\vp}^k-\vp)(\vw^{\pi^k}_{M^k}-\vw^{\pi^*}_{M}) +\vw^{\pi^*}_{M}(\hat{\vp}^k-\vp)$.
Since $\vw^{\pi^*}_M$ is deterministic, the term $\vw^{\pi^*}_{M}(\hat{\vp}^k-\vp)$ can be bounded using Chernoff-Hoeffding's inequality on individual component $w^{\pi^*}_{M}(s')\Bigl(\hat{p}^k(s')-p(s')\Bigr)$.
Moreover, thanks to the optimism, $w^{\pi^k}_{M^k}(s')-w^{\pi^*}_{M}(s')$ is non-negative for any $s'\in\gS$.
So, the empirical Bernstein's inequality can be used to upper bound each individual component $\Bigl(\hat{p}^k(s'){-}p(s')\Bigr)\Bigl(w^{\pi^k}_{M^k}(s'){-}w^{\pi^*}_{M}(s')\Bigr)$.
%The term $(\hat{p}^k-p)(w^{\pi^k}_{M^k}-w^{\pi^*}_{M})$ is upper bounded thanks to the optimism and Bernstein's inequality.

UCBVI enjoys the following worst-case regret guarantee.
\begin{prop}[{\cite[Theorem~1]{azar2017minimax}}]
    \label{ch:rl:prop:ucbvi}
    In finite horizon setting with horizon $H$, for any unknown MDP $M$ with $S$ states and $A$ actions,
    with probability at least $1-\delta$, it holds that the regret of UCBVI is bounded by
    \begin{align*}
        \Reg(\textnormal{UCBVI}, M, K) \le 20H^{3/2}\sqrt{SAK}L +250H^2S^2AL^2
    \end{align*}
    where $L:=\ln\Bigl(\frac{5SAKH^2}{\delta}\Bigr)$.
\end{prop}
So, for $T\ge HS^3A$ and $SA\ge H$, this bound is of order $\tilde{\landauO}(H\sqrt{SAT})$ where $T=KH$ and $\tilde{\landauO}(\cdot)$ hides the logarithmic terms.
Compare to the lower bound $\landauOmega(\sqrt{HSAT})$, the bound given in Proposition~\ref{ch:rl:prop:ucbvi} is loose by a factor $\sqrt{H}$ ignoring the logarithmic terms.
The advanced version UCBVI-BF that enjoys a regret bound $\tilde{\landauO}(\sqrt{HSAT})$ is also given in \cite{azar2017minimax}.% but it is not necessary to understand this thesis.

\subsection{Posterior sampling for reinforcement learning (PSRL) \texorpdfstring{\cite{osband2013more}}{[ORV13]}.}

PSRL designed for finite horizon setting is a posterior sampling algorithm which starts by choosing prior distributions $\phi_r$ and $\phi_p$ such that the unknown $r$ is assumed to be drawn from $\phi_r$ and the unknown $p$ from $\phi_p$.
To ease the exposition, we will say that a MDP $M'=\langle\gS,\gA,r',p'\rangle$ is sampled from $\phi$ when $r'$ is sampled from $\phi_r$ and $p'$ is sampled from $\phi_p$.
When updating its policy, PSRL use the collected observations and Bayes' theorem to derive the posterior distribution of the unknown MDP.
After that, an MDP is sampled from the posterior and PSRL computes an optimal policy in the sampled MDP.
That is, at time step $t^k$, PSRL uses the observations $o_{t^k}$ to update the posterior $\phi\bigl(\cdot\mid o_{t^k}\bigr)$.
Then, Line~\ref{ch:rl:line:new_policy} of Algorithm~\ref{algo:generics} is performed by sampling an MDP $M^k$ from $\phi\bigl(\cdot\mid o_{t^k}\bigr)$ and computing an optimal policy $\pi^k$ in $M^k$.
The policy $\pi^k$ is then used during the episode $k$ to collect more observations.

The performance of PSRL is measured by the Bayesian regret given in \eqref{eq:bayes_rg}.
Broadly speaking, we bound the Bayesian regret by bounding the regret with terms that are deterministic and the expected value of those terms are then themselves.
To do so, we will first show that the expected value of $\Delta^k_{model}$ defined in \eqref{eq:rg_model_conc} is zero.
Then, the term $\Delta^k_{conc}$ is bounded in exactly the same manner as UCRL2 does.
\begin{lem}
    \label{lem:expected_identity}
    For any $k\ge1$, let $t^k$ be the time step that episode $k$ starts and $o_{t^k}$ be the observations collected right before $t^k$.
    Assume that the unknown MDP $M$ is drawn according to the prior $\phi$ and that $M^k$ is sampled according to the posterior $\phi\bigl(\cdot\mid o_{t^k}\bigr)$. Then, for any $o_{t^k}$-measurable function $f$, one has
    \begin{equation}
        \label{eq:exp_iden}
        \E[f(M)] =\E[f(M^k)].
    \end{equation}
\end{lem}
\begin{proof}
    At the start of each episode $k$, $M$ and $M^k$ are identically distributed conditioned on $o_{t^k}$.
    In consequence, if $f$ is $o_{t^k}$-measurable function, one has:
    \begin{align*}    
        \E[f(M) \mid o_{t^k}] =\E[f(M^k) \mid o_{t^k}].
    \end{align*}
    \Eqref{eq:exp_iden} then follows from the tower rule.
\end{proof}
This lemma implies that $\ex{\vw^{\pi^k}_{M^k}}=\ex{\vw^{\pi^*}_M}$ because $M^k$ and $\pi^k$ are $o_{t^k}$-measurable.
Consequently, $\ex{\Delta^k_{model}}=\vzero$ for PSRL.
The term $\Delta^k_{conc}$ can be rewritten as in \eqref{ch:rl:eq:delta_conc}.
Since $d_t$ is a martingale difference term, its expected value is zero.
The term $\abs{r^k-r}$ and $\norm{\vp^k-\vp}_{\ell_1}$ can be bounded using Hoeffding's and Weissman's inequalities.

PSRL enjoys the following Bayesian regret guarantee.
\begin{prop}[{\cite[Theorem~1]{osband2013more}}]
    \label{prop:brg_psrl}
    In finite horizon setting with horizon $H$, if $\phi$ is the prior distribution of the unknown MDP $M$ with $S$ states and $A$ actions, then
    \begin{equation}
        \label{eq:brg_psrl}
        \BayReg(\textnormal{PSRL}, \phi, T) = \landauO\left(HS\sqrt{AT\ln\Bigl(SAT\Bigr)}\right)
    \end{equation}
    where $T=KH$.
\end{prop}
So, this bound is of order $\tilde{\landauO}(HS\sqrt{AT})$ where $T=KH$ and $\tilde{\landauO}(\cdot)$ hides the logarithmic terms.
It is loose by a factor $\sqrt{HS}$ compared to the lower bound $\landauOmega(\sqrt{HSAT})$.
Note that this bound is for Bayesian regret which means that this bound is a weaker guarantee compared to the regret bound with high probability of UCRL2 and UCBVI.

%    In general, in MDP $M=\langle\mSpace,\aSpace,r,P,H\rangle$, the agent starts to play from state $x$ with a certain probability given by the distribution $\rho(x)$. When using the algorithm $\Algo$ to play for $T$ time steps, the incurring regret is given by
%    \begin{align}
%        \Reg(T,\Algo,M)=\sum_{k=1}^{\lceil T/H\rceil}\sum_{x\in\mcal{E}}\rho(x)\big(V_{M,1}^*(x) -V_{M,1}^{\pik}(x)\big) =\sum_{k=1}^{\lceil T/H\rceil}\sum_{x\in\mcal{E}}\rho(x)\Delta_k(x)
%    \end{align}
%    where, during $k$th episode, $\Algo$ follows policy $\pik$ which is optimal in the imaginary MDP $\Mk$, $V^*_{M,1}(x)$ and $V_{M,1}^{\pik}(x)$ are the value of state $x$ under the optimal policy and the policy $\pik$ starting from time step $1$ to $H$ respectively, and $H$ is the horizon of each episode.
%    Note that we write $V_M$ as the value evaluated using the real MDP $M$ and $V_{\Mk}$ as the one evaluated using the imaginary MDP $\Mk$. It is convenient to seperate the regret gap into two gaps as the following:
%    \begin{enumerate}
%        \item the regret gap at $k$th episode:
%        \begin{align*}
%        \Delta_k &=V_{M,1}^* -V_{M,1}^{\pik} \\
%        &=\big(V^*_{M,1} -V^{\pik}_{\Mk,1}\big) +\big(V^{\pik}_{\Mk,1} -V_{M,1}^{\pik}\big) \\
%        &=\Delta_k^{opt}+\Delta_k^{conc}
%        \end{align*}
%    \item the gap from optimism $\Delta_k^{opt}(x)=V^*_{M,1}(x) -V^{\pik}_{\Mk,1}(x)$, where $V^{\pik}_{\Mk,1}(x)$ is the value of state $x$ under the policy $\pik$ from time step $1$ to $H$ and evaluated in the imagined MDP $\Mk$
%    \item the gap from concentration $\Delta_k^{conc}(x)=V^{\pik}_{\Mk,1}(x) -V_{M,1}^{\pik}(x)$.
%    \end{enumerate}
%    So,
%    \begin{equation}
%    \label{eq:eq_6}
%    \Reg(T,\Algo,M)= \sum_{k=1}^{\lceil T/H\rceil}\sum_{x\in\mcal{E}}\rho(x)(\Delta_k^{opt}(x)+\Delta_k^{conc}(x))
%    \end{equation}
%
%\section{UCRL2}
%\label{sec:ucrl2}
%
%    \subsection{Optimism in Face of Uncertainty(OFU)}
%    \label{subsec:OFU}
%    
%    This algorithm uses the principle of "Optimism in Face of Uncertainty": when we are \textbf{uncertain} about the outcome, we consider the \textit{best possible world} and choose the \textit{best decision} in imaginary world. Let $\mcal{O}_k$ be the set of observations collected until the beginning of episode $k$. UCRL2 does
%        \begin{enumerate}
%            \item \label{it:ucrl2_1} compute a set of plausible MDPs, $\mcal{M}_k$, based on the observations $\mcal{O}_k$ at the beginning of episode $k$. This set contains the unknown MDP $M$ with high probability (this is equivalent to compute the sets $\mcal{R}_k$ and $\mcal{P}_k$ containing $r$ and $P$ respectively with high probability)
%            \item compute $\pik$ such that
%                \begin{align}
%                    V_{\Mk,1}^{\pik} \ge \max_{\pi}\max_{\Mbar\in\mcal{M}_k} V_{\Mbar,1}^\pi
%                \end{align}
%            \item follow $\pik$ during the episode $k$
%            \item after finishing episode $k$, update $\mcal{O}_{k+1}$ and go back to \ref{it:ucrl2_1}
%        \end{enumerate}
%    
%    \subsection{High Probability Event}
%    \label{subsec:high_prob_event}
%        In order to build the sets $\mcal{R}$ and $\mcal{P}$, we should review some inequalities that are vital to our construction.
%    
%        \begin{thm}[Hoeffding inequality]
%        \label{thm:hoeffding}
%            Let $\{R_i\}_{i\le n}$ be independent and identically distributed random variable with mean $r$ bounded in $[0,1]$.
%            By Hoeffding inequality,
%            \begin{align*}
%                \Proba{\Big\vert\frac{1}{n}\sum_{i=1}^nR_i-r\Big\vert>\ep}\le2\exp\left(-2n\ep^2\right)
%            \end{align*}
%            The random variables $\{R_i\}_{i\le n}$ bounded in $[0,1]$ are $(1/2)$-sub-Gaussian.
%        \end{thm}
%        \begin{thm}[Weissman inequality (Inequalities for L1 deviation of the empirical distribution)]
%        \label{thm:weissman}
%            Let $\{Y_i\}_{i\le n}$ be $n$ i.i.d sample from a distribution $P$ over a finite set $\mSpace=\{1,\dots,\mSize\}$. Define the empirical distribution $\Pbar(y)=\frac{1}{n}\sum_{i=1}^{n}\mathbb{I}_{Y_i=y}, \forall y\in\mSpace$. Then, for any $\ep>0$,
%            \begin{align*}
%                \Proba{\Vert\Pbar-P\Vert_1> \ep} \le 2^\mSize\exp\left(-\frac{n\ep^2}{2}\right)
%            \end{align*}
%        \end{thm}
%        
%        At time $t$, let $\rbar_t$ and $\Pbar_t$ be the empirical estimators of $r$ and $P$ respectively.
%        Let $N_t(x,a)$ be the number of visits to state-action pair $(x,a)$ up to time $t$.
%        We have
%        \begin{align*}
%            \rbar_t(x,a) =\frac1{N_t(x,a)}\sum_{h=1}^{t}R_h\I_{\{X_h=x\wedge A_h=a\}} \text{ and } \Pbar_t(x,a,y) =\frac1{N_t(x,a)}\sum_{h=1}^{t}\I_{\{X_h=x\wedge A_h=a\wedge X_{h+1}=y\}}
%        \end{align*}
%
%        Now, we prove a high probability event:
%        \begin{lem}
%        \label{lem:high_prob_event}
%            Let $L_t:=\sqrt{2\log(4EAt/\delta)}$. Then, the event
%            \begin{align}
%            \event_t:=\bigg\{\forall a\in\mcal{A}, \forall x\in\mcal{E}, t'\le t: &\vert\rbar_{t'}(x,a)-r(x,a)\vert\le \frac{L_t}{2\sqrt{N_{t'}(x,a)}}\\
%            \text{ and } &\Vert\Pbar_{t'}(x,a)-P(x,a)\Vert_1\le \frac{L_t+1.5\sqrt{E}}{\sqrt{N_{t'}(x,a)}}\bigg\}
%            \end{align}
%            is true with probability $1-\delta$.
%        \end{lem}
%        \begin{proof}
%            By Theorem~\ref{thm:hoeffding}, for any $\ep>0$, one has
%            \begin{align*}
%            \Proba{\vert\rbar_{t}(x,a)-r(x,a)\vert>\ep \mid N_{t}(x,a)} \le 2\exp\left(-2N_{t}(x,a)\ep^2\right)
%            \end{align*}
%            This holds for $\ep=\sqrt{\frac{\log(4EAt/\delta)}{2N_t(x,a)}}$ where $0<\delta<1$.
%            As $N_t(x,a)\le t$, by using the union-bound, this implies that
%            \begin{align}
%            &\Proba{\exists x,a, t'\le t: \vert\rbar_{t'}(x,a)-r(x,a)\vert>\sqrt{\frac{\log(4EAt/\delta)}{2N_{t'}(x,a)}}} \label{eq:conc_r}\\
%            &\qquad \le \sum_{t'=1}^{t}\sum_{x,a} \Proba{\vert\rbar_{t'}(x,a)-r(x,a)\vert>\sqrt{\frac{\log(4EAt/\delta)}{2N_{t'}(x,a)}}} \nonumber \\
%            &\qquad \le \sum_{t'=1}^{t}\sum_{x,a} 2\exp\left(-2N_{t'}(x,a)\times\frac{\log(4EAt/\delta)}{2N_{t'}(x,a)}\right) =2EAt \frac{\delta}{4EAt}=\frac{\delta}2 \nonumber
%            \end{align}
%            By Theorem~\ref{thm:weissman}, for any $\ep>0$, one has
%            \begin{align*}
%            \Proba{\Vert\Pbar_t(x,a)-P(x,a)\Vert_1 > \ep \mid N_t(x,a)} \le 2^E\exp\left(-\frac{N_t(x,a)\ep^2}{2}\right)
%            \end{align*}
%            Similarly, with $\ep=\sqrt{2\log(2EAt2^E/\delta)/N_t(x,a)}$, we have
%            \begin{align*}
%            &\Proba{\exists x,a, t'\le t: \Vert\Pbar_t(x,a)-P(x,a)\Vert_1 >\sqrt{\frac{2\log(2EAt2^E/\delta)}{N_{t'}(x,a)}}} \\
%            &\qquad \le 2^EEAt\exp\left(-\frac{N_t(x,a)}{2}\times\frac{2\log(2EAt2^E/\delta)}{N_{t'}(x,a)}\right) =\frac{\delta}2
%            \end{align*}
%            Since $L_t=\sqrt{2\log(4EAt/\delta)}$ and $\sqrt{x+y}\le\sqrt{x}+\sqrt{y}$, we have $\sqrt{2\log(2EAt2^E/\delta)}=\sqrt{2\log(4EAt/\delta) +2(E-1)\log2}\le L_t + \sqrt{2(E-1)\log2} \le L_t +1.5\sqrt{E}$.
%            Hence,
%            \begin{align}
%            \label{eq:conc_p}
%            \Proba{\exists x,a, t'\le t: \Vert\Pbar_t(x,a)-P(x,a)\Vert_1 >\frac{L_t+1.5\sqrt{E}}{\sqrt{N_{t'}(x,a)}}} \le \frac{\delta}2
%            \end{align}
%            The complement of event $\event_t$ defined in the statement of Lemma~\ref{lem:high_prob_event} is the union of \eqref{eq:conc_r} and \eqref{eq:conc_p}.
%            Hence, the union-bound concludes the proof of the lemma.
%        \end{proof}
%
%\subsection{Regret bound}
%\label{subsec:regret_ucrl2}
%
%%TOCONTINUE
%    Let $t_k:=(k-1)H+1$ and $L_{t_k}=\sqrt{2\log(4EAt_k/\delta)}$.
%    Define the set of plausible MDPs by
%    \begin{align*}
%        \mcal{M}_k :=\left\{(r_k,P_k) \colon \forall x,a, \vert r_k(x,a)-\rbar_k(x,a)\vert \le \frac{L_{t_k}}{2\sqrt{N_k(x,a)}} \text{ and }\right. \\
%        \left. \Vert P_k(x,a)-\Pbar_k(x,a)\Vert_1\le \frac{L_{t_k}+1.5\sqrt{E}}{\sqrt{N_k(x,a)}}\right\}.
%    \end{align*}
%    So, under event $\event_k$, the unknown MDP $M$ belongs to $\mcal{M}_k$.
%    By the definition of $\pik$, $\Delta_k^{opt}=V^*_{M,1}(x) -V^{\pik}_{\Mk,1}(x)\le0$.
%    Then, under event $\event_k$,
%    \begin{align*}
%        \sum_{k=1}^{\lceil T/H\rceil}\Delta_k \le \sum_{k=1}^{\lceil T/H\rceil}\Delta_k^{conc}.
%    \end{align*}
%    
%    \subsubsection{Gap from concentration}
%    \label{subsubsec:sec_gap_conc}
%    
%        Let $a_{kh}=\pik(x_{kh})$ be the action that policy $\pik$ takes when at state $x_{kh}$ at time step $h$ in episode $k$. From Bellman's equation, we have
%        \begin{align*}
%            V^{\pik}_{M,H}(x_{k1})
%            &= r^{\pik}(x_{k1}) +\sum_{y\in\mSpace}P^{\pik}(x_{k1},y)V^{\pik}_{M,H-1}(y)
%        \end{align*}
%        In vector form, we have
%        \begin{align*}
%            V^{\pik}_{M,H} &= r^{\pik} +P^{\pik}V^{\pik}_{M,H-1} \\
%            V^{\pik}_{M_k,H} &= r^{\pik}_k +P^{\pik}_kV^{\pik}_{M_k,H-1}.
%        \end{align*}
%        Then,
%        \begin{align*}
%            \Delta_k^{conc}
%            &= V^{\pik}_{M_k,1} -V^{\pik}_{M,1} \\
%            &= r^{\pik}_k-r^{\pik} +(P^{\pik}_k -P^{\pik})V^{\pik}_{M_k,2} +P^{\pik}(V^{\pik}_{M,2}-V^{\pik}_{M_k,2}).
%        \end{align*}
%        So, at state $x_{k1}$
%        \begin{align*}
%            \Delta_k^{conc}(x_{k1})
%            &= r^{\pik}_k(x_{k1})-r^{\pik}(x_{k1}) +[(P^{\pik}_k(x_{k1},\cdot) -P^{\pik}(x_{k1},\cdot))V^{\pik}_{M_k,2}] \\
%            &\qquad +\ex{V^{\pik}_{M,2}(Y)-V^{\pik}_{M_k,2}(Y) \mid Y\sim P^{\pik}(x_{k1},\cdot)} \\
%            &\le \vert r^{\pik}_k(x_{k1}) -r^{\pik}(x_{k1})\vert +(H-1)\Vert P^{\pik}_k(x_{k1},\cdot) -P^{\pik}(x_{k1},\cdot)\Vert_1 \\
%            &\qquad +\ex{V^{\pik}_{M,2}(Y)-V^{\pik}_{M_k,2}(Y) \mid Y\sim P^{\pik}(x_{k1},\cdot)}.
%        \end{align*}
%        Continue to roll out Bellman's equation for $V^{\pik}_{M_k,h}-V^{\pik}_{M,h}$ with $3\le h\le H$, we get
%        %TOCONTINUE
%        \begin{align*}
%            \ex{\Delta_k^{conc}} \le \sum_{h=1}^{H}\vert r_k-r\vert(x_{kh},a_{kh}) +(H-1)\Vert P_k -P\Vert_1(x_{kh},a_{kh}).
%        \end{align*}
%        Now,
%        \begin{align*}
%            \sum_{k=1}^{\lceil T/H\rceil}\ex{\Delta_k^{conc}}
%            &\le \sum_{k=1}^{\lceil T/H\rceil} \sum_{h=1}^{H}\vert r_k-r\vert(x_{kh},a_{kh}) +(H-1)\Vert P_k -P\Vert_1(x_{kh},a_{kh}) \\
%            &=\sum_{k=1}^{\lceil T/H\rceil} \sum_{t=t_k}^{t_{k+1}-1}\vert r_k(x_t,a_t)-r(x_t,a_t)\vert +(H-1)\Vert P_k(x_t,a_t) -P(x_t,a_t)\Vert_1 \\
%            &\overset{(a)}\le \sum_{k=1}^{\lceil T/H\rceil} \sum_{t=t_k}^{t_{k+1}-1} \frac{L_{t_k}}{2\sqrt{N_k(x_t,a_t)}} +(H-1)\frac{L_{t_k}+1.5\sqrt{E}}{\sqrt{N_k(x_t,a_t)}} \le\sum_{k=1}^{\lceil T/H\rceil} \sum_{t=t_k}^{t_{k+1}-1}H\frac{L_T+1.5\sqrt{E}}{\sqrt{N_k(x_t,a_t)}}
%        \end{align*}
%        where $(a)$ is true under event $\event_k$.
%        For $t\le t_k$, let $\tilde{N}_t(x,a)$ be the real-time counter for state-action pair $(x,a)$. We have $\forall t\le T, \forall (x,a)\in \mdpStateSpace\times\ActionSpace$,
%        
%        \begin{align*}
%            N_{\lfloor t/H\rfloor+1}(x,a) +\horizon \ge \tilde{N}_t(x,a) &\ge N_{\lfloor t/H\rfloor+1}(x,a) \ge 1 \\
%            N_{\lfloor t/H\rfloor+1}(x,a) &\ge \max\{1, \tilde{N}_t(x,a) -\horizon\} \\
%            \sum_{k=1}^{\lceil T/\horizon\rceil} \sum_{t=t_k}^{t_{k+1}-1} \frac{1}{\sqrt{N_{\lfloor t/H\rfloor+1}(x_t,a_t)}} &\le \sum_{k=1}^{\lceil T/\horizon\rceil} \sum_{t=t_k}^{t_{k+1}-1} \min\left\{1, \frac{1}{\sqrt{\tilde{N}_t(x_t,a_t) -\horizon}}\right\} \\
%            \sum_{k=1}^{\lceil T/\horizon\rceil} \sum_{t=t_k}^{t_{k+1}-1} \frac{1}{\sqrt{N_k(x_t,a_t)}} &\le \sum_{t=1}^{T} \min\left\{1, \frac{1}{\sqrt{\tilde{N}_t(x_t,a_t) -\horizon}}\right\} \\
%            &= \sum_{x,a} \sum_{t=1}^{\tilde{N}_T(x,a)} \min\left\{1, \frac{1}{\sqrt{t -\horizon}}\right\} \\
%            &= \sum_{x,a} \Big(H +\sum_{t=H+1}^{\tilde{N}_T(x,a)} \frac{1}{\sqrt{t -H}}\Big) \\
%            &= EAH +\sum_{x,a} \sum_{t=1}^{\tilde{N}_T(x,a) -\horizon} \frac{1}{\sqrt{t}} \\
%            &\le EAH +\sum_{x,a} \int_0^{\tilde{N}_T(x,a) -\horizon} \frac{dt}{\sqrt{t}} \\
%            &\le EAH +\sum_{x,a} \int_0^{\tilde{N}_T(x,a)} \frac{dt}{\sqrt{t}} \\
%            &= EAH +2\sum_{x,a} \sqrt{\tilde{N}_T(x,a)} \\
%            &\overset{(a)}\le EAH +2\sqrt{EA \sum_{x,a}\tilde{N}_T(x,a)} \\
%            &= EAH +2\sqrt{EAT}
%        \end{align*}
%        where inequality $(a)$ is true by Jensen's inequality. Indeed, using Jensen's inequality, we have
%        \begin{align*}
%            \frac{1}{EA}\sum_{x,a}\sqrt{\tilde{N}_T(x,a)} \le\sqrt{\frac{1}{EA}\sum_{x,a}\tilde{N}_T(x,a)}.
%        \end{align*}
%        Thus,
%        \begin{align*}
%            \sum_{k=1}^{\lceil T/\horizon\rceil}\ex{\Delta_k^{conc}}
%            &\le H\Big(L_T+1.5\sqrt{E}\Big) \Big(EAH +2\sqrt{EAT}\Big) \\
%            &= H\Big(\sqrt{2\log(4EAT/\delta)} +1.5\sqrt{E}\Big) \Big(EAH +2\sqrt{EAT}\Big)
%        \end{align*}
%        Now,
%        \begin{align*}
%            \ex{\Delta_k}
%            &=\ex{\Delta_k\I_{\event_k} +\Delta_k\I_{\neg\event_k}} \\
%            &\le \ex{\Delta_k\I_{\event_k}} +H\Proba{\neg\event_k} =\ex{\Delta_k\I_{\event_k}} +H\delta.
%        \end{align*}
%        Finally, the expected regret UCRL2 is bounded by
%        \begin{align*}
%            \ex{\Reg(T,\mathrm{UCRL2},M)} 
%            &\le H\Big(\sqrt{2\log(4EAT/\delta)} +1.5\sqrt{E}\Big) \Big(EAH +2\sqrt{EAT}\Big) +\sum_{k=1}^{\lceil T/H\rceil}H\delta \\
%            &\le H\Big(\sqrt{2\log(4EAT/\delta)} +1.5\sqrt{E}\Big) \Big(EAH +2\sqrt{EAT}\Big) +T\delta
%        \end{align*}
%        
%        This result is a significant statistical guarantee as its scaling is close to the fundamental lower bound $\Omega(\sqrt{HEAT})$ \cite{jaksch2010near, osband2016lower}.
%
%\section{UCBVI}
%\label{sec:ucbvi}
%    OFU algorithm presented above applies the confidence sets on the parameters of the model, namely $(r,P)$. Another approach from \cite{azar2017minimax} is to directly apply the confidence set on the value function. One known algorithm is UCBVI that works as the following
%    
%    At the start of episode $k$,
%    \begin{enumerate}
%        \item compute the empirical reward and transition $(\rbar_k,\Pbar_k)$ for all state-action pairs
%        \item set $V_{k,H+1}=0$
%        \item for each time step $h=H,\dots,1$, compute the value and quality function for each $(x,a)$ state-action pairs:
%        \begin{align*}
%            Q_{k,h}(x,a) &=\min\left\{Q_{k-1,h}(x,a), H, \rbar_k(x,a) +\Pbar_k(x,a)V_{k,h+1} +b_k(x,a)\right\} \\
%            V_{k,h}(x) &= \max_{a\in\ActionSpace}Q_{k,h}(x,a)
%        \end{align*}
%        \item during episode $k$, for each time step $h=1,\dots,H$, 
%        \begin{align*}
%            \pol_{k}(x_{kh}) =\arg\max_{a\in\ActionSpace}Q_{k,h}(x_{kh},a)=\Action_{kh}
%        \end{align*}
%    \end{enumerate}
%    
%    \subsection{High Probability Event}
%    \label{subsec:high_prob_event1}
%    
%        Let $L_t:=\log(4E^2At/\delta)$. For any deterministic mappings $f\colon \mcal{E}\mapsto[0,H-1]$, and any $g\colon\mcal{E}\mapsto[0,H]$, we define the following events:
%        \begin{align}
%            &\event_1:=\left\{\forall x, a, t'\le t,|\rhat_{t'}(x,a)-r(x,a)+[(\Phat_{t'}-P)f](x,a)| \le H\sqrt{\frac{L_t}{2N_{t'}(x,a)}}\right\} \\
%            &\event_2:=\left\{\forall x, a, y, t'\le t,|\Phat_{t'}(x,a,y)-P(x,a,y)|g(y) \le\sqrt{\frac{2P(x,a,y)[1-P(x,a,y)]g^2(y)L_t}{N_{t'}(x,a)}} +\frac{2HL_t}{3N_{t'}(x,a)}\right\} \\
%            &\event:=\event_1\cap\event_2.
%        \end{align}
%        
%        \begin{lem}
%        \label{lem:high_prob}
%            For any deterministic mappings $f\colon \mcal{E}\mapsto[0,H-1]$ and $g\colon \mcal{E}\mapsto[0,H]$, we have $\Proba{\event}\ge1-\delta$.
%        \end{lem}
%        \begin{proof}
%        Given $\{U_i\}_{1\le i\le n}$ independent samples of random variable $U\in[0, c]$ where $c\in\R$, Theorem~\ref{thm:hoeffding} gives, for $\ep>0$,
%        \begin{align*}
%        \Proba{|\bar{U}-\ex{U}|\ge \varepsilon} \le 2e^{-2\frac{n}{c^2}\varepsilon^2}
%        \end{align*}
%        Consider a fixed $(x,a)$.
%        For any deterministic mapping $f\colon\mcal{E}\mapsto[0,H-1]$, let $U:=R+\beta f(Y)$ be a random variable where $R\in[0,1]$ and $Y\in\mcal{E}$ are random variables such that $\ex{R}=r(x,a)$ and $\Proba{Y=y}=P(x,a,y)$.
%        Hence, the expectation of $U$ is given by 
%        \begin{align*}
%        \ex{U}=r(x,a) +\sum_{y\in\mcal{E}}P(x,a,y)f(y)=r(x,a) +P(x,a)f.
%        \end{align*}
%        Also, $U\in [0,H]$.
%        After $n$ of activations, we will have collected $\{U_i\}_{1\le i\le n}$.
%        The empirical estimate of $U$ is
%        $$\bar{U}:=\frac1n\sum_{i=1}^{n} U_i=\rbar(x,a) +\Pbar(x,a)f.$$
%        Then,
%        \begin{align*}
%        &\Proba{\vert\bar{U}-\ex{U}\vert >\varepsilon} \le2e^{-2\frac{n\varepsilon^2}{H^2}} \\
%        &\Proba{\vert \rbar(x,a)-r(x,a) +(\Pbar(x,a) -P(x,a))f\vert >\varepsilon} \le2e^{-2\frac{n\varepsilon^2}{H^2}}.
%        \end{align*}
%        Using the same approach above,
%        \begin{align*}
%        \Proba{\neg\event_1} 
%        &=\Proba{\exists x,a, t'\le t,|\rbar_{t'}(x,a)-r(x,a)+(\Pbar_{t'}(x,a)-P(x,a))f| >H\sqrt{\frac{L_t}{2N_{t'}(x_a)}}} \\
%        &\le2\sum_{x,a} \sum_{t'=1}^{t}e^{-\log(4E^2At/\delta)} =2EAt\frac{\delta}{4E^2At}=\frac{\delta}{2E}
%        \end{align*}
%        Given $\{U_i\}_{1\le i\le n}$ independent samples of random variable $U\in[0, c]$ where $c\in\R$ and $\var{U}=\sigma^2$, Bernstein inequality gives
%        \begin{align*}
%        \Proba{|\bar{U}-\ex{U}|> \varepsilon} \le 2e^{-\frac{n\varepsilon^2}{2\sigma^2+\frac{2c}3\varepsilon}}
%        \end{align*}
%        Consider a fixed $(x,a,y)$. For any deterministic mapping $g\colon\mcal{E}\mapsto[0,H]$, let $U:=g(Y)$ be a random variable and $Y\in\mcal{E}$ where $\Proba{Y=y}=P(x,a,y)$.
%        So, we have
%        \begin{align*}
%        \ex{U}=P(x,a,y)g(y),\ \var{U}=P(x,a,y)(1-P(x,a,y))g^2(y):=\sigma^2(y), \text{ and } U\in[0,H].
%        \end{align*}
%        Then,
%        \begin{align*}
%        \Proba{\neg\event_2}
%        &=\Proba{\exists x,a,y,t'\le t,|\Pbar_{t'}(x,a,y)-P(x,a,y)|g(y) >\sqrt{\frac{2\sigma^2(y)L_t}{N_{t'}(x,a)}} +\frac{2HL_t}{3N_{t'}(x,a)}} \\
%        &\le\sum_{x,a,y} \sum_{t'=1}^{t} \Proba{|\Pbar_{t'}(x,a,y)-P(x,a,y)|g(y) \le\sqrt{\frac{2\sigma^2(y)L_t}{N_{t'}(x,a)}} +\frac{2HL_t}{3N_{t'}(x_a)}\mid N_{t'}(x_a)=n} \\
%        &\le E^2At 2\exp\left(-\frac{n\left(\sqrt{\frac{2\sigma^2(y)L_t}{n}} +\frac{2HL_t}{3n}\right)^2}{2\sigma^2(y)+\frac{2H}{3}\left(\sqrt{\frac{2\sigma^2(y)L_t}{n}} +\frac{2HL_t}{3n}\right)}\right) \\
%        &\le 2E^2At\exp\left(-\frac{n\left[\frac{2\sigma^2(y)L_t}{n} +\frac{2HL_t}{3n}\left(\sqrt{\frac{2\sigma^2(y)L_t}{n}} +\frac{2HL_t}{3n}\right)\right]}{2\sigma^2(y)+\frac{2H}{3}\left(\sqrt{\frac{2\sigma^2(y)L_t}{n}} +\frac{2HL_t}{3n}\right)}\right) \\
%        &=2E^2At\exp(-\log(4E^2At/\delta)) = \frac{\delta}2
%        \end{align*}
%        Now,
%        \begin{align*}
%        \Proba{\neg\event} 
%        &\le\Proba{\neg\event_1} +\Proba{\neg\event_2}\le\frac{\delta}{2E} +\frac{\delta}2\le \delta\frac{1+E}{2E} \le \delta.
%        \end{align*}
%        \end{proof}
%        
%        \begin{lem}
%        \label{lem:bern}
%        Under event $\event$, for any $g\colon\mcal{E}\mapsto[0,H]$ and any $x,a$,
%        \begin{align*}
%        \vert (\Pbar_t(x,a)-P(x,a))g \vert
%        \le \frac1H\ex{g(Y) \mid Y\sim P(x,a)} +\frac{SH^2L_t}{N_t(x,a)}
%        \end{align*}
%        \end{lem}
%        \begin{proof}
%        \begin{align*}
%        \vert (\Pbar_t(x,a)-P(x,a))g \vert
%        &= \vert\sum_{y\in\mcal{E}}\big(\Pbar_t(x,a,y) -P(x,a,y)\big)g(y)\vert \\
%        &\le \sum_{y\in\mcal{E}}|\Pbar_t(x,a,y) -P(x,a,y)|g(y) \\
%        &\overset{(a)}{\le} \sum_{y\in\mcal{E}} \sqrt{\frac{2P(x,a,y)[1-P(x,a,y)]g^2(y)L_t}{N_t(x,a)}} +\sum_{y\in\mcal{E}}\frac{2HL_t}{3N_t(x,a)} \\
%        &\le \sum_{y\in\mcal{E}} \sqrt{\frac{2P(x,a,y)g^2(y)L_t}{N_t(x,a)}} +\frac{2EHL_t}{3N_t(x,a)} \\
%        &\overset{(b)}{\le}\sqrt{E}\sqrt{\frac{2L_t}{N_t(x,a)} \sum_{y\in\mcal{E}}P(x,a,y)g^2(y)} +\frac{2EHL_t}{3N_t(x,a)} \\
%        &=\sqrt{\frac{2EH^2L_t}{N_t(x,a)} \sum_{y\in\mcal{E}}P(x,a,y)\frac{g^2(y)}{H^2}} +\frac{2EHL_t}{3N_t(x,a)} \\
%        &\overset{(c)}{\le}\frac{EH^2L_t}{N_t(x,a)} +\sum_{y\in\mcal{E}}P(x,a,y)\frac{g^2(y)}{H^2} +\frac{2EHL_t}{3N_t(x,a)} \\
%        &\overset{(d)}{\le} \sum_{y\in\mcal{E}}P(x,a,y)\frac{g(y)}{H} +\frac{EH^2L_t}{N_t(x,a)}\\
%        &=\frac1H\ex{g(Y) \mid Y\sim P(x,a)} +\frac{EH^2L_t}{N_t(x,a)} 
%        \end{align*}
%        where $(a)$ is true under the event $\xi$, $(b)$ is true due to Cauchy-Schwartz inequality, $(c)$ is true due to $ab\le(a^2+b^2)/2$ and $(d)$ is true due to $g\le H$.
%        \end{proof}
%        
%        \begin{lem}
%        \label{lem:bonus}
%        Let $L_t:=\log(4E^2At/\delta)$. For any $x,a$, define $b(x,a):=H\sqrt{\frac{L_t}{2N_t(x,a)}}$.
%        Let $\mTilde:=\langle\mcal{E}, [n], \rhat+b, \Phat\rangle$ be a MDP and $V_{\mTilde}^{*}$ be the optimal value function in $\mTilde$.
%        Under event $\event$, $$V_{\mTilde}^{*} \ge V_M^*.$$
%        \end{lem}
%        \begin{proof}
%        Let $\pik$ be the policy used during episode $k$. By Bellman, we have
%        \begin{align*}
%        V_{k,h}(x)
%        &= \rbar(x,\pik(x)) +b(x,\pik(x)) +\Pbar(x,\pik(x)) V_{k,h+1} \\
%        &\ge \rbar(x,\pi_*(x)) +b(x,\pi_*(x)) +\Pbar(x,\pi_*(x)) V_{k,h+1}.
%        \end{align*}
%        Then,
%        \begin{align*}
%        V_{k,h}(x) -V_{M,h}^{*}(x)
%        &= \rbar(x,\pik(x)) +b(x,\pik(x)) +\Pbar(x,\pik(x)) V_{k,h+1} 
%        -r(x,\pi_*(x)) -P(x,\pi_*(x))V_{M,h+1}^{*} \\
%        &\ge \rbar(x,\pi_*(x)) +b(x,\pi_*(x)) +(\Pbar(x,\pi_*(x)) V_{k,h+1}
%        -r(x,\pi_*(x)) -P(x,\pi_*(x))V_{M,h+1}^{*} \\
%        &= b(x,\pi_*(x)) +(\rbar-r)(x,\pi_*(x)) +[(\Pbar-P)V_{M,h+1}^{*}](x,\pi_*(x)) +\Pbar(x,\pi_*(x))(V_{k,h+1} -V_{M,h+1}^{*}).
%        \end{align*}
%        For $h=H$, we have $V_{k,H}(x) -V_{M,h}^*(x) \ge b(x,\pi_*(x)) +(\rbar-r)(x,\pi_*(x)) \ge0$ where the last inequality is true under event $\event_k$.
%        Suppose that $V_{k,h}\ge V_{M,h}^*$ is true for $h=\{h+1,\dots,H\}$.
%        We prove that $V_{k,l}\ge V_{M,l}^*$ is also true for $l=h$.
%        We have
%        \begin{align*}
%        V_{k,h}(x) -V_{M,h}^{*}(x)
%        &\ge b(x,\pi_*(x)) +(\rbar-r)(x,\pi_*(x)) +[(\Pbar-P)V_{M,h+1}^{*}](x,\pi_*(x)) +\Pbar(x,\pi_*(x))(V_{k,h+1} -V_{M,h+1}^{*}) \\
%        &\ge b(x,\pi_*(x)) +(\rbar-r)(x,\pi_*(x)) +[(\Pbar-P)V_{M,h+1}^{*}](x,\pi_*(x)) \ge0.
%        \end{align*}
%        where the last inequality is true, again, under event $\event_k$.
%        \end{proof}
%    
%    \subsection{Regret bound}
%    \label{subsec:regret_ucbvi}
%    
%    Let $b_k:=H\sqrt{\log(4E^2At_k/\delta)/(2N_k)}$. Under $\event_k$, Lemma~\ref{lem:bonus} implies that $V_M^*-V_M^{\pik} \le V_{k}-V_M^{\pik}$.
%    Then,
%    \begin{align*}
%    V_{k,1}(x_{k1}) -V_{M,1}^{\pik}(x_{k1})
%    &=\rbar_k(x_{k1},a_{k1}) +\Pbar_k(x_{k1},a_{k1})V_{k,2} +b_k(x_{k1},a_{k1}) -r(x_{k1},a_{k1}) -P(x_{k1},a_{k1})V_{M,2}^{\pik} \\
%    &=\rbar_k(x_{k1},a_{k1}) -r(x_{k1},a_{k1}) +(\Pbar_k(x_{k1},a_{k1}) -P(x_{k1},a_{k1}))V_{M,2}^*
%    \end{align*}
%    Since $\Pbar V_k -PV_M^{\pik}=(\Pbar-P)V_M^* +P(V_k-V_M^{\pik}) +(\Pbar-P)(V_k-V_M^*)$, we have
%    \begin{align*}
%    V_{k,1}(x_{k1}) -V_{M,1}^{\pik}(x_{k1})
%    &=\underbrace{\rbar_k(x_{k1},a_{k1}) -r(x_{k1},a_{k1}) +(\Pbar_k(x_{k1},a_{k1}) -P(x_{k1},a_{k1}))V_{M,2}^*}_{I_1} +b_k(x_{k1},a_{k1}) \\
%    &\qquad +P(x_{k1},a_{k1})(V_{k,2}-V_{M,2}^{\pik}) +\underbrace{(\Pbar_k(x_{k1},a_{k1})-P(x_{k1},a_{k1}))(V_{k,2}-V_{M,2}^*)}_{I_2}.
%    \end{align*}
%    Under $\event_k$, we have
%    \begin{align*}
%    I_1&\le H\sqrt{\frac{L_{t_k}}{2N_k(x_{k1},a_{k1})}}
%    \end{align*}
%    by when taking $f=V_{M,2}^{*}$ under event $\event_k$.
%    Also, by Lemma~\ref{lem:bern} with $g=V_{k,2} -V_{M,2}^{*}$,
%    \begin{align*}
%    I_2
%    &\le\frac1{H}\ex{V_{k,2}(Y) -V_{M,2}^{*}(Y) \mid Y\sim P(x_{k1}, a_{k1})} +\frac{EH^2L_{t_k}}{N_k(x,a)} \\
%    &\le\frac1{H}\ex{V_{k,2}(Y) -V_{M,2}^{\pik}(Y) \mid Y\sim P(x_{k1}, a_{k1})} +\frac{EH^2L_{t_k}}{N_k(x,a)}
%    \end{align*}
%    where the last inequality is true due to $V^*_M\ge V^{\pik}_{M}$.
%    Then,
%    \begin{align*}
%    V_{k,1}(x_{k1}) -V_{M,1}^{\pik}(x_{k1})
%    &\le b_k(x_{k1},a_{k1}) +H\sqrt{\frac{L_{t_k}}{2N_k(x_{k1},a_{k1})}} +\frac{EH^2L_{t_k}}{N_k(x_{k1},a_{k1})} \\
%    &\qquad +\left(1+\frac1H\right)\ex{V_{k,2}(Y) -V_{M,2}^{\pik}(Y) \mid Y\sim P(x_{k1}, a_{k1})}
%    \end{align*}
%    Continue to roll out $V_{k,h}-V_{M,h}^{\pik}$, we get
%    \begin{align*}
%    \ex{V_{k,1}(x_{k1})-V_{M,1}^{\pik}(x_{k1})}
%    &\le\sum_{t=t_k}^{t_{k+1}-1}\underbrace{\left(1+\frac1H\right)^{t-t_k}}_{\le e} \left(2H\sqrt{\frac{L_{t_k}}{2N_k(x_t,a_t)}} +\frac{EH^2L_{t_k}}{N_k(x_t,a_t)}\right).
%    \end{align*}
%    Then, the upper bound of expected regret of UCBVI is
%    \begin{align*}
%    \ex{\Reg(T,\mathrm{UCBVI},M)}
%    &\le e\sum_{k=1}^{\lceil T/H\rceil}\sum_{t=t_k}^{t_{k+1}-1}  \left(H\sqrt{\frac{2L_{t_k}}{N_k(x_t,a_t)}} +\frac{EH^2L_{t_k}}{N_k(x_t,a_t)}\right) +T\delta \\
%    &\le e\sum_{k=1}^{\lceil T/H\rceil}\sum_{t=t_k}^{t_{k+1}-1}  \left(H\sqrt{\frac{2L_T}{N_k(x_t,a_t)}} +\frac{EH^2L_T}{N_k(x_t,a_t)}\right) +T\delta
%    \end{align*}
%    Using the same approach to bound $\sum_{k=1}^{\lceil T/H\rceil}\sum_{t=t_k}^{t_{k+1}-1}[1/\sqrt{N_k(x_t,a_t)} +1/N_k(x_t,a_t)]$,
%    the expected regret is roughly upper bounded by $\tilde{O}\left(H\sqrt{EAT}\right)$.
%
%\section{PSRL}
%\label{sec:psrl}
%
%\subsection{Statistical Inference}
%\label{subsec:statistical_inf}
%
%Before presenting the algorithms, we want to review some interesting points in Statistical Inference that are related to our problem.
%
%Consider a statistical parametric model $\mathfrak{M}=\{p(\cdot|\theta);\theta\in\Theta\}$ where $\theta$ is a parameter of the model living in the parameter space $\Theta$, and $p(\cdot|\theta)$ is the probability measure of an event in function of the parameter $\theta$.
%
%- for discrete random variable X, we write $p(x|\theta)=\mathbb{P}_{\theta}\{X=x\}$
%- for continuous random variable X, we write $p(x|\theta)=f(x;\theta)$ where $f(x;\theta)$ is the (probability) density function
%
%We want to estimate the parameter of the model, $\theta$, using the observations. Suppose that we have
%
%- $(X_1,\dots,X_n)\rightarrow(x_1,\dots,x_n)$: $n$ observations sampled from the model
%- each observation is independent of one another
%
%To infer the parameter, there are two perspectives: frequentist and bayesian.
%
%\subsubsection{Frequentist perspective}
%\label{subsubsec:freq_perspective}
%
%One idea in frequentist perspective is to write a likelihood or log-likelihood in function of $\theta$ and observations $(x_1,\dots,x_n)$, then choose $\hat{\theta}$ that maximizes the likelihood:
%
%\begin{align*}
%l_{(x_1,\dots,x_n)}(\theta)&=\prod_{i=1}^{n}p(x_i|\theta) \\
%L_{(x_1,\dots,x_n)}(\theta)&=\sum_{i=1}^{n}\log p(x_i|\theta)
%\end{align*}
%
%and $$\hat{\theta}=\arg\max_{\theta\in\Theta}l_{(x_1,\dots,x_n)}(\theta)=\arg\max_{\theta\in\Theta}L_{(x_1,\dots,x_n)}(\theta)$$
%
%\subsubsection{Bayesian perspective}
%\label{subsubsec:bayes_perspective}
%
%One idea in this perspective is to derive the distribution of the parameter $\theta$ then draw the parameter from that distribution when needed. Concretely, we put a Prior distribution on the parameter $\theta$, use the observations to deduce the Posterior distribution, and sample the parameter $\hat{\theta}$ from the Posterior.
%
%- we make a hypothesis that the parameter has a certain distribution $p(\theta)$
%- the Likelihood of the observations: $l_{(x_1,\dots,x_n)}(\theta)=\prod_{i=1}^{n}p(x_i|\theta)$
%- from Bayes's theorem: $$p(\theta|x_1,\dots,x_n) =\frac{l_{(x_1,\dots,x_n)}(\theta)p(\theta)}{p(x_1,\dots,x_n)} \propto l_{(x_1,\dots,x_n)}(\theta)p(\theta)$$
%- when needed, we draw $\hat{\theta}\sim l_{(x_1,\dots,x_n)}(\theta)p(\theta)$
%
%\subsection{Application of Posterior Sampling}
%
%We make a hypothesis that the unknown $\mdpModel$ is drawn from a distribution $\phi(\mdpModel)$. It is our \textbf{prior distribution} about the MDP. So, we draw an MDP from that distribution, compute the respective optimal policy and play that policy in $\mdpModel$ for a whole episode to collect observations. Then, we refine our prior distribution based on Bayes' theorem and the observations collected. The refined distribution is called \textbf{posterior distribution}.
%
%Concretely, let $\mcal{O}_k$ be the observations collected overall episodes until time step $1$ of $k$th episode. So, the algorithm $\mathrm{PSRL}$ draws $M_k$ from the posterior distribution $\phi(\cdot|\mcal{O}_k)$, computes the optimal policy $\pik$ in that MDP and uses $\pik$ during the $k$th episode.
%
%\subsection{Bayesian Regret}
%\label{subsec:regret_psrl}
%
%In this section, we study the bayesian regret in posterior sampling approach and prove its bound when using PSRL algorithm. Suppose that the unknown MDP $M$ is drawn from $\phi$. The bayesian regret is defined by
%
%$$\BayReg(T,\mathrm{PSRL})=\mathbb{E}\left[\Reg(T,\mathrm{PSRL},M) |M\sim\phi\right]$$
%
%where $\Reg(T,\mathrm{PSRL},M)$ is defined in Equation$~\eqref{eq:eq_6}$. After playing $(k-1)$ episodes and collecting $\mcal{O}_{k}$, we believe that $M$ is drawn from the posterior distribution $\phi(\cdot|\mcal{O}_{k})$. So, $M$ and $M_k$ have the same distribution and for any $\sigma(\mcal{O}_{k})$-measurable function $f$, we have
%\begin{align*}
%\mathbb{E}[f(M)|\mcal{O}_{k}] =\mathbb{E}[f(M_k)|\mcal{O}_{k}]
%\end{align*}
%and applying the law of total expectation, we get $\mathbb{E}[f(M)] =\mathbb{E}[f(M_k)]$. This implies that $\mathbb{E}[\Delta_k^{opt}]=0$. Effectively, $\Delta_k^{opt}=V^*_{M,1}(x) -V^{\pik}_{M_k,1}(x)$ where the optimal value $V^*_M$ is a $\sigma(\mcal{O}_{k})$-measurable function of $M$ and the optimal value $V^{\pik}_{M_k}$ is a $\sigma(\mathcal{O}_{k})$-measurable function of $M_k$.
%
%So, we get
%\begin{align*}
%\BayReg(T,\mathrm{PSRL}) =\ex{\sum_{k=1}^{\lceil T/H\rceil}\Delta_k^{conc} |M\sim\phi}
%\end{align*}
%Following the same approach in the proof of UCRL3, we get the upper bound of Bayesian regret of PSRL as $O(HE\sqrt{AT})$.
%    
\endgroup
