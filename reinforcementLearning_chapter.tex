\begingroup

\let\clearpage\relax

\chapter{Reinforcement Learning}
\label{ch:rl}

In the previous chapter, we used the formalism of MDPs to describe how a decision maker interacts with its environment.
Depending on the chosen optimality criterion, we explained how an optimal policy can be obtained when the parameters of the MDP are fully known.
In this chapter, we consider the case when the MDP's parameters are \textbf{unknown} and need to be learned by the decision maker via trial and error.


In Section~\ref{ch:rl:sec:blabla}, we introduce...

The reader can skip this chapter and go directly to Part~\ref{part:idx} and come back before starting Part~\ref{part:learning}.

\section{The learning problem and regret definition}

\subsection{Learning problem and learning approaches}

We consider the learning problem in which the learner\footnote{we use the term learner instead of decision maker} interacts with a MDP $M=\langle\gS,\gA,r,p\rangle$ where the state and action spaces $\gS$ and $\gA$ are known but the expected value of reward $r$ and state transition probabilities $p$ are \textbf{unknown} to the learner.
At time step $t\ge1$, the MDP is in state $s_t$ and the decision maker executes an action $a_t$.
The MDP incurs a (random) reward denoted by $r_t$ and transitions to next state denoted by $s_{t+1}$.
This mechanism is repeated and one obtains a sequence of the form $\{s_1,a_1,r_1,s_2,\dots,s_t,a_t,r_t,s_{t+1},\dots\}$ that is called \emph{observations} (also known as a ``trajectory'')
Note that the expected value of reward is bounded as $r(s,a)\in[0,1]$ for all $s\in\gS, a\in\gA$.
The planning methods given previous in chapter such as value iteration or backward induction, which require $r$ and $p$, cannot be immediately used and the learner needs to collect observations about $r$ and $p$.
To get enough information, the learner needs to \emph{explore} the dynamic of the MDP as much as possible.
However, too much exploration equalizes the performance of the learner with one that blindly chooses action at random.
So, a good learner should also \emph{exploit} the gathered information.
Unfortunately, untimely exploitation leads to suboptimal policy, thus unsatisfied performance. This is the famous “exploration versus exploitation dilemma”.

%This leads to two different probabilistic approaches described in Table~\ref{tab:compare}.
%
%\begin{table}[ht]
%    \center
%    \begin{tabular}{|p{0.45\textwidth}|p{0.45\textwidth}|}
%        \hline
%        \textbf{Frequentist model} & \textbf{Bayesian model} \\ \hline
%        \begin{itemize}
%            \item $r$ and $p$ are unknown parameters
%        \end{itemize}
%        &
%        \begin{itemize}
%            \item $r$ and $p$ are drawn from a prior distribution
%        \end{itemize} \\ \hline
%    \end{tabular}
%    \caption{Comparison between frequentist and Bayesian modeling about the MDP ${M=\langle\gS,\gA,r,p\rangle}$}
%    \label{tab:compare}
%\end{table}

\subsection{Regret definition}

To measure the exploration-exploitation performance, we use the notion of \emph{regret} which compares the cumulative rewards of the optimal policy and the learner. 
If the learner $\gL$ interacts with the unknown MDP $M$ over $T\in\N^+$ time steps, we denote by $\Reg(\gL, M, T)$ the regret incurred by the learner.

The formal definition of regret depends on the setting of MDP.
In this thesis, we recall the definition in two settings: finite horizon and infinite horizon average reward.
%However, good learners are compared by how fast 

%over any finite number of steps $T$ (i.e., the difference gap between the cumulative reward of the optimal policy and the one of the learning algorithm over $T$ steps)
%(or the \emph{optimism in face Uncertainty})

\paragraph{Finite horizon model.}
With this model, the learner interacts with the unknown MDP $M$ over $K\in\N^+$ episodes, each episode lasts in $H\in\N^+$ time steps (so the total time steps is $T=KH$) and the state of $M$ is reset according to a distribution $\rho$.
In such a setting, $H$ is called \emph{horizon} and it is bounded in $\N^+$.
At the beginning of each episode $k\ge1$, the initial state of the MDP $s_1\sim\rho$ is revealed to the learner who computes a policy $\pi^k$ based on its collected observations and rollouts the policy $\pi^k$ during the episode.
%After $H$ time steps, the state of the MDP is reset according to a distribution $\rho$ and revealed to the learner who computes a policy $\pi^k$ based on its collected observations and rollouts the policy $\pi^k$ during the episode.
Following the definition in Section~\ref{ch:mdp:sec:finite}, we denote by $w^{\pi^k}_{1:H}(s):=\E^{\pi^k}[\sum_{t=1}^{H}r_t \mid s_1=s]$ the expected cumulative reward from state $s$ over time steps $1$ to $H$.
The regret is defined by the following.
\begin{defn}
    \label{defn:rg_finite}
    If a learner $\gL$ interacts with an unknown MDP $M$ over $K$ episodes, each episode ends in $H\in\N^+$, then its regret is
    \begin{equation}
        \label{eq:rg_finite}
        \Reg(\gL, M, K):= \sum_{k=1}^K\sum_{s\in\gS}\rho(s)[w^*_{1:H}(s) -w^{\pi^k}_{1:H}(s)].
    \end{equation}
\end{defn}
Since $H$ is bounded, we derive from the above that a \emph{good} learner has an expected regret sublinear in the number of episodes $K$, $\ex{\Reg(\gL, M, K)}=\landauo(K)$ when $K\to+\infty$.

\paragraph{Infinite horizon average reward model.}
For this setting, there is no reset but the unknown MDP $M$ is assumed to be \emph{weakly communicating}.
The learner interacts with $M$ over $T$ time steps and collects a sequence of rewards $\{r_t\}_{1\le t\le T}$.
The regret is defined by the following.
\begin{defn}
    \label{defn:rg_infinite}
    If a learner $\gL$ interacts with an unknown MDP $M$ which is weakly communicating, then its regret after $T$ time steps is
    \begin{equation}
        \label{eq:rg_finite}
        \Reg(\gL, M, T):= Tg^* -\sum_{t=1}^Tr_t
    \end{equation}
    where $g^*$ is the optimal gain of $M$ (see its definition in Section~\ref{ch:mdp:sec:gain}).
\end{defn}
A notable difference between finite and infinite horizon settings is that in finite setting, the learner updates its policy right before the episode starts and never within the episode while in infinite setting, the learner decides by itself when to update its policy.

%TODO:continue
In such dilemma, two different points of view about the MDP $M$ are adopted: the frequentist point of view, in which $r$ and $p$ of the MDP $M$ are seen as unknown \textbf{deterministic parameters}, and the Bayesian point of view, in which $r$ and $p$ of $M$ are \textbf{random variables}, drawn from some prior distribution.
To simplify the exposition, in Bayesian approach, we will say the unknown MDP $M$ is drawn from a probability distribution (prior or posterior) $\phi$ to mean that $r$ is drawn from a dedicated probability distribution (prior or posterior) and $p$ is drawn from another dedicated probability distribution (prior or posterior).

In frequentist point of view about $M$, the learner $\gL$ that maximizes the expected cumulative rewards over $T$ time steps equivalently minimizes its expected regret $\E\bigg[\Reg(\gL, M, T)\bigg]$.

For Bayesian point of view, we use similar notion of regret known as \emph{Bayesian regret} (or Bayes risk) defined by
\begin{equation}
    \label{eq:bayes_rg}
    \BayReg(\gL, \phi, T):=\ex{\E\bigg[\Reg(\gL, M, T) \mid M\bigg]}
\end{equation}
where $\phi$ is the prior distribution of the unknown MDP $M$.
So, the learner $\gL$ maximizes the expected cumulative rewards from $M$ over $T$ time steps by minimizing its Bayesian regret $\BayReg(\gL, \phi, T)$.

In both approaches, a learner is \emph{good} if its expected regret is sublinear in the total number of time steps $T$, \ie, $\E\Big[\Reg(\gL, M, T)\Big]= \landauo(T)$ when $T\to+\infty$.


For both settings, the Bayesian regret is defined by \eqref{eq:bayes_rg}.






%\section{Regret benchmarks}
%\label{sec:rg_benchmark}

%In this section, we describe... 

\section{Minimax regret lower bound}

We compare the exploration-exploitation performance of good learners by how fast their regret tends to zero.
\cite{jaksch2010near} has proven that no learners can perform better than a certain baseline for all MDPs $M$.
We will give this baseline below but first we need to introduce the notion of diameter of a MDP.
\begin{defn}
    The diameter of a MDP is defined by
    \begin{equation}
        \label{eq:diameter}
        D := \max_{s,s'\in\gS}\min_{\pi:\gS\mapsto\gA} \E^\pi[\tau(s') \mid s_1=s] -1
    \end{equation}
    where $\tau(s'):=\inf\{t\ge2 : s_t=s'\}$ is the first time step when $s'$ is reached.
\end{defn}
So, the diameter of a MDP is the length of the \textbf{longest shortest path} in the MDP where the distance between any pair of vertices is $1$.
In other words, it is the expected number of vertices along the shortest path between two states that are the \textbf{most distant} from each other.
From Definition~\ref{ch:mdp:defn:mdp_class} and \cite[Proposition~8.3.1]{puterman2014markov}, it is clear that the diameter $D$ is finite if and only if the MDP is communicating.

Given any learner $\gL$, there always exists an unknown MDP $M$ that slows down the learner $\gL$ as the following.
\begin{prop}[{\cite[Theorem~5]{jaksch2010near}}]
    \label{prop:minimax_rg_lb}
    In infinite horizon setting, for any learner $\gL$, any integers $\abs{\gS}$, $\abs{\gA}\ge10$, $D\ge 20\log_{\abs{\gA}}(\abs{\gS})$, and $T\ge DSA$, there is a MDP $M=\langle\gS,\gA,r,p\rangle$ whose diameter is $D$ such that for any initial state $s_1$, the expected regret of $\gL$ after $T$ time steps is lower bounded as
    \begin{equation}
        \label{eq:minimax_rg_lb}
        \ex{\Reg(\gL, M, T)} \ge 0.015\sqrt{D\abs{\gS}\abs{\gA}T}.
    \end{equation}
\end{prop}
This proposition means that no matter how ``good'' a learner is, it is always possible to construct a worst case MDP $M$ having $\abs{\gS}$ states, $\abs{\gA}$ actions and diameter $D$ such that the learner suffers a regret $\landauOmega(\sqrt{D\abs{\gS}\abs{\gA}T})$ after $T$ time steps in $M$.
This bound on worst case regret is often referred as ``minimax'' bound.

The minimax bound is different from the bounds given in \cite{ok2018exploration, burnetas1997optimal} which are problem-dependent and asymptotic bounds.
Minimax bounds usually scale as $\sqrt{T}$ while problem-dependent bounds scale logarithmically\footnote{In the bandit literature, “problem-dependent” bounds are said to be distribution-dependent, as opposed to minimax bounds which are said to be distribution-free \cite{garivier2019explore}} with $T$.
We do not recall those problem-dependent bounds because they are not necessary to understand the thesis.

Recent work of \cite{zhang2019regret} suggests that the minimax bound in Proposition~\ref{prop:minimax_rg_lb} cannot be improved.
Minimax bound is often expressed in term of span (or range) of the optimal bias function as well.
In fact, the specific worst-case MDP constructed by \cite{jaksch2010near} to prove the lower bound in Proposition~\ref{prop:minimax_rg_lb} satisfies $D=2sp(\vh^*)$ where $\vh^*$ is the optimal bias function (see Section~\ref{ch:mdp:sec:gain} for its definition).

For finite horizon setting, the diameter of the MDP is replaced with the horizon of an episode.
So, the minimax bound can be immediately deduced from Proposition~\ref{prop:minimax_rg_lb}: for any learner, it is always possible to construct a worst case MDP with $\abs{\gS}$ states and $\abs{\gA}$ actions such that after $K$ episodes, each of horizon\footnote{For stochastic bandit, we have $H=1, S=1$ and $K=T$. So, the minimax bound is $\landauOmega(\sqrt{AT})$ as given in \cite{bubeck2012regret}} $H$, the learner suffers a regret $\landauOmega(\sqrt{HSAK})$.

\section{(Near) Optimal algorithms}

To achieve an efficient trade-off between exploration and exploitation, a common strategy in frequentist point of view is to apply \emph{optimism in face of uncertainty} (OFU) principle: the learner maintains a confidence set for the unknown MDP and execute an optimal policy of the
“best” (in term of gain, value function, etc) MDP in the confidence set, \eg, \cite{ortner2020regret, bourel2020tightening, zhang2019regret, zanette2019tighter, fruit2018near, fruit2018efficient, jin2018q, fruit2017regret,azar2017minimax, bartlett2012regal, filippi2010optimism, jaksch2010near}.

In Bayesian point of view, an efficient trade-off can be done by posterior sampling introduced by \cite{thompson1933likelihood}: the learner keeps a posterior distribution over possible MDPs (precisely, the support of the prior distribution) and execute the optimal policy of a sampled MDP, see \eg, \cite{osband2013more, gopalan2015thompson, ouyang2017learning}.

The common point between the two approaches is that they compute an optimal policy in an imagined MDP that they believe to be the unknown MDP $M=\langle\gS,\gA,r,p\rangle$.
Indeed, in both approaches, when the learner update its policy for the $k$th time, the learner compute a policy denote by $\pi^k$ that is optimal in an imagined MDP denote by $M^k=\langle\gS,\gA,r^k,p^k\rangle$.
Since an optimal policy can be computed in a deterministic manner given the MDP, the ``high-level'' difference between both approaches lies in how this imagined MDP $M^k$ is constructed: chosen ``optimistically'' by OFU method or chosen randomly by posterior sampling according to its belief (or the posterior distribution).

From now on, a learner is an algorithm for the learning problem.
To understand this thesis, we revisit three algorithms for the end this chapter: UCRL2 \cite{jaksch2010near} and UCBVI \cite{azar2017minimax}, which rely on OFU method, and PSRL \cite{osband2013more}, which is a posterior sampling algorithm.
We give the sketch of proof for their regret upper bound in finite horizon setting although UCRL2 is originally design for infinite horizon average reward.
We will give a detail proof in Chapter~\ref{ch:learning_rested} when we derive the three algorithms for Markovian bandit problem.

We extend the notation of incentive quantities such as gain and value function in Chapter~\ref{ch:mdp} as the following
\begin{itemize}
    \item the optimal gain, discounted value function and expected cumulative reward from the unknown MDP $M$ is denoted by $g^{\pi^*}_M$, $v^{\pi^*}_M$ and $w^{\pi^*}_{M,h:H}$ respectively where $\pi^*$ is an optimal policy in $M$;
    \item the optimal gain, discounted value function and expected cumulative reward from the imagined MDP $M^k$ is denoted by $g^{\pi^k}_{M^k}$, $v^{\pi^k}_{M^k}$ and $w^{\pi^k}_{M^k,h:H}$ respectively where $\pi^k$ is an optimal policy in $M^k$.
\end{itemize}

By Definition~\ref{defn:rg_finite}, the regret of an algorithm $\gL$ after $K$ episodes in an unknown MDP $M$ can be written
\begin{align*}
    \Reg(\gL, M, K) &= \sum_{k=1}^K\sum_{s\in\gS}\rho(s)[w^{\pi^*}_{M,1:H}(s) -w^{\pi^k}_{M,1:H}(s)].
                    %&= \sum_{k=1}^K \sum_{s\in\gS}\rho(s) [w^{\pi^*}_{M,1:H}(s) -w^{\pi^k}_{M^k,1:H}(s)] +\sum_{k=1}^K \sum_{s\in\gS}\rho(s) [w^{\pi^k}_{M^k,1:H}(s) -w^{\pi^k}_{M,1:H}(s)]
\end{align*}
The term $\vw^{\pi^*}_M -\vw^{\pi^k}_M$ can be rewritten as
\begin{equation}
    \label{eq:rg_model_conc}
    \vw^{\pi^*}_M -\vw^{\pi^k}_M = \underbrace{(\vw^{\pi^*}_M -\vw^{\pi^k}_{M^k})}_{=:\Delta_{model}^k} +\underbrace{(\vw^{\pi^k}_{M^k} -\vw^{\pi^k}_M)}_{=:\Delta_{conc}^k}
\end{equation}
The first term $\Delta_{model}^k$ is the difference between the real but \textbf{unknown} optimal incentive and the \textbf{imagined} optimal incentive.
It is also understood as the error due to model misspecification.
Since $M$ and $\pi^*$ are unknown, this first term is hard to analyse.
However, we will see in the following that the term is non-positive for \textbf{optimism} or zero in expectation for \textbf{posterior sampling}.

The second term $\Delta_{conc}^k$ can be interpreted as the ``dissimilarity'' between the \textbf{unknown} MDP $M$ and the \textbf{imagined} MDP $M^k$ along the trajectory induced by policy $\pi^k$.
In other words, it is the discordance between the incentive from $M^k$ and the one from $M$ for policy $\pi^k$.
Since $M^k$ and $\pi^k$ are chosen by the learner, this second term can be analyzed and both approaches upper bound the term using concentration inequalities.

\paragraph{UCRL2 \texorpdfstring{\cite{jaksch2010near}}{[JAO10]}.}
As mentionned above, UCRL2 was originally design for infinite horizon setting.
%However, it still proceeds in episodic manner: the algorithm decides to update its policy when the number of visits to a state-action pair has doubled since the end of the previous episode and the new episode begins.
%The number of visits to state-action pair $(s,a)$ means how many time steps so far that the action $a$ has been executed when the unknown MDP is in state $s$.
%When updating the policy, UCRL2 compute the maximum-likelihood estimates of $r$ and $p$ as well as confidence sets 
Here, we adapt UCRL2 to finite horizon setting.
When updating the policy, UCRL2 constructs confidence sets $\sB_r$ and $\sB_p$ for $r$ and $p$ respectively based on high probability confidence bounds.
We denote by $\sM$ the set of plausible MDPs whose reward function lives in $\sB_r$ and state transition lives in $\sB_p$.
UCRL2 chooses the MDP that incurs the highest optimal incentive among the MDPs in $\sM$ and computes an optimal policy of the chosen MDP.
That is, at the start of episode $k$, UCRL2 computes $M^k$ and $\pi^k$ such that
\begin{equation}
    \label{ch:rl:eq:optim}
    \vw_{M^k}^{\pi^k} \ge \max_{\pi\in\Pi}\max_{M'\in\sM^k} \vw_{M'}^\pi
\end{equation}
where
\begin{align*}
    \sM^k:=\{\langle\gS,\gA,r',p'\rangle : r'(s,a)\in\sB_r^k(s,a)\text{ and } \vp'(\cdot\mid s,a)\in\sB_p^k(s,a) \text{ for all }s\in\gS, a\in\gA_s\}
\end{align*}
is the set of plausible MDPs compatible with the confidence sets.
The set of plausible MDPs is constructed to contains the unknown $M$ with probability.
Hence, $\pi^k$ is optimistic in the sense that $\vw^{\pi^k}_{M^k}\ge \vw^{\pi^*}_M$ with high probability. 
This means that with high probability, $\vw^{\pi^*}_M- \vw^{\pi^k}_{M^k}\le \vzero$.

So, by \Eqref{eq:rg_model_conc}, with high probability the regret of UCRL2 is smaller than ${\sum_{k=1}^K\sum_{s\in\gS}\rho(s)[\Delta_{conc}^k(s)]}$.
Using Bellman evaluation equations, the term $\Delta_{conc}^k(s)$ can be rewritten as
\begin{align}
    w^{\pi^k}_{M^k,1:H}(s_1) -w^{\pi^k}_{M,1:H}(s_1)
    &= r^k(s_1,a_1)+\sum_{s'\in\gS}p^k(s'\mid s_1,a_1)w^{\pi^k}_{M^k,2:H}(s') \nonumber\\
    &\qquad -r(s_1,a_1)-\sum_{s'\in\gS}p(s'\mid s_1,a_1)w^{\pi^k}_{M,2:H}(s'). \label{eq:decom}
\end{align}
The term $\vp^k\vw^{\pi^k}_{M^k}-\vp\vw^{\pi^k}_M$ equals $(\vp^k-\vp)\vw^{\pi^k}_{M^k} +\vp(\vw^{\pi^k}_{M^k}-\vw^{\pi^k}_M)$.
Since $w^{\pi^k}_{M^k,h:H}(s)\in [0,H]$ for any $h\le H$ and $s\in\gS$ is not deterministic, the term $(\vp^k-\vp)\vw^{\pi^k}_{M^k}$ is bounded using Hölder's inequality.
So, we get
\begin{equation}
    \label{ch:rl:eq:delta_conc}
    \Delta_{conc}^k(s_1)\le\sum_{t=1}^{H} \abs{r^k(s_t,a_t)-r(s_t,a_t)}{+}H\norm{\vp^k(\cdot\mid s_t,a_t)-\vp(\cdot\mid s_t,a_t)}_{\ell_1} +d_t
\end{equation}
where $\{d_t\}_{1\le t\le H}$ is a martingale difference sequence, each term upper bounded by $H$.
So, the sum $\sum_{k=1}^K\sum_{t=1}^Hd_t$ can be bounded using Azuma-Hoeffding's inequality.
Moreover, if the unknown $M$ belongs to the plausible set $\sM$, then $\abs{r^k-r}$ and $\norm{\vp^k-\vp}_{\ell_1}$ can be bounded by the size of $\sB_r^k$ and $\sB_p^k$ respectively.
So, the regret of UCRL2 is bounded.
%If $M$ is communicating with diameter $D$, then any MDP in $\sM$ is communicating with a diameter bounded by $D$ (see \cite[Section~4.3.1]{jaksch2010near}).
For the sake of completeness, we recall the result of \cite{jaksch2010near} for infinite horizon setting below.
\begin{prop}[{\cite[Theorem~2]{puterman2014markov}}]
    \label{ch:rl:prop:uclr2}
    For any communicating MDP $M$ with $\abs{\gS}$ states, $\abs{\gA}$ actions and diameter $D$, with probability at least $1-\delta$, it holds that for any $T>1$, the regret of UCRL2 is bounded by
    \begin{align*}
        \Reg(\mathrm{UCRL2}, M, T) \le 34D\abs{\gS}\sqrt{\abs{\gA}T\ln\Bigl(\frac{T}{\delta}\Bigr)}.
    \end{align*}
\end{prop}
Compare to Proposition~\ref{prop:minimax_rg_lb}, the upper bound in Proposition~\ref{ch:rl:prop:uclr2} is loose by a factor $\sqrt{D\abs{\gS}}$ ignoring the logarithmic term.

\paragraph{UCBVI \texorpdfstring{\cite{azar2017minimax}}{[AOM17]}.}
UCBVI is an optimistic algorithm designed for finite horizon setting.
The algorithm keeps track of the maximum-likelihood estimate of $p$ and constructs only a confidence set $\sB_r$ for $r$ based on high probability confidence bounds.
That is, at the start of episode $k$, UCBVI compute $M^k$ and $\pi^k$ such that \eqref{ch:rl:eq:optim} is satisfied where
\begin{align*}
    \sM^k:=\{\langle\gS,\gA,r',\hat{p}^k\rangle : r'(s,a)\in\sB_r^k(s,a) \text{ for all }s\in\gS, a\in\gA_s\}
\end{align*}
is the set of plausible MDPs compatible with the confidence set and the estimate.
Hence, the policy $\pi^k$ is optimistic and with high probability $\vw^{\pi^*}_M- \vw^{\pi^k}_{M^k}\le \vzero$ holds.

So, by \Eqref{eq:rg_model_conc}, with high probability, the regret of UCBVI is bounded by ${\sum_{k=1}^K\sum_{s\in\gS}\rho(s)[\Delta_{conc}^k(s)]}$.
The theoretical novelty in UCBVI is to deal efficiently with the term $(\hat{\vp}^k-\vp)\vw^{\pi^k}_{M^k}$ when rewritting the right term of \eqref{eq:decom} as shown in UCRL2 technique.
Indeed, $(\hat{\vp}^k-\vp)\vw^{\pi^k}_{M^k}$ is also rewritten as $(\hat{\vp}^k-\vp)(\vw^{\pi^k}_{M^k}-\vw^{\pi^*}_{M}) +\vw^{\pi^*}_{M}(\hat{\vp}^k-\vp)$.
Since $\vw^{\pi^*}_M$ is deterministic, the term $\vw^{\pi^*}_{M}(\hat{\vp}^k-\vp)$ can be bounded using Chernoff-Hoeffding's inequality on individual component $w^{\pi^*}_{M}(s')\Bigl(\hat{p}^k(s')-p(s')\Bigr)$.
Moreover, thank to the optimism, $w^{\pi^k}_{M^k}(s')-w^{\pi^*}_{M}(s')$ is non-negative for any $s'\in\gS$.
So, the empirical Bernstein's inequality can be used to upper bound each individual component $\Bigl(\hat{p}^k(s'){-}p(s')\Bigr)\Bigl(w^{\pi^k}_{M^k}(s'){-}w^{\pi^*}_{M}(s')\Bigr)$.
%The term $(\hat{p}^k-p)(w^{\pi^k}_{M^k}-w^{\pi^*}_{M})$ is upper bounded thank to the optimism and Bernstein's inequality.

UCBVI enjoys the following minimax regret guarantee.
\begin{prop}[{\cite[Theorem~1]{azar2017minimax}}]
    \label{ch:rl:prop:ucbvi}
    In finite horizon setting with horizon $H$, for any unknown MDP $M$ with $\abs{\gS}$ states and $\abs{\gA}$ actions,
    with probability at least $1-\delta$, it holds that the regret of UCBVI is bounded by
    \begin{align*}
        \Reg(\mathrm{UCBVI}, M, K) \le 20H^{3/2}\sqrt{\abs{\gS}\abs{\gA}K}L +250H^2\abs{\gS}^2\abs{\gA}L^2
    \end{align*}
    where $L:=\ln\Bigl(\frac{5\abs{\gS}\abs{\gA}KH^2}{\delta}\Bigr)$.
\end{prop}
So, for $T\ge H\abs{\gS}^3\abs{\gA}$ and $\abs{\gS}\abs{\gA}\ge H$, this bound is of order $\tilde{\landauO}(H\sqrt{\abs{\gS}\abs{\gA}T})$ where $T=KH$ and $\tilde{\landauO}(\cdot)$ hides the logarithmic terms.
Compare to the lower bound $\landauOmega(\sqrt{H\abs{\gS}\abs{\gA}T})$, the bound given in Proposition~\ref{ch:rl:prop:ucbvi} is loose by a factor $\sqrt{H}$ ignoring the logarithmic terms.
Another advanced version of UCBVI that enjoys a regret bound $\tilde{\landauO}(\sqrt{H\abs{\gS}\abs{\gA}T})$ is also given in \cite{azar2017minimax} but it is not necessary to understand this thesis.

\paragraph{PSRL \texorpdfstring{\cite{osband2013more}}{[ORV13]}.}
PSRL designed for finite horizon setting is a posterior sampling algorithm which starts by choosing the prior distributions $\phi_r$ and $\phi_p$ such that the unknown $r$ is assumed to be drawn from $\phi_r$ and the unknown $p$ from $\phi_p$.
To ease the exposition, we will say that a MDP $M'=\langle\gS,\gA,r',p'\rangle$ is sampled from $\phi$ when $r'$ is sampled from $\phi_r$ and $p'$ is sampled from $\phi_p$.
When updating its policy, PSRL use the collected observations and Bayes' theorem to derive the posterior distribution of the unknown MDP.
After that, a MDP is sampled from the posterior and PSRL computes an optimal policy in the sampled MDP.
That is, at time step $t^k$, the start of episode $k$, PSRL has collected the observations denoted by $o_{t^k}:=\{s_1,a_1,r_1,\dots,s_{t^k-1},a_{t^k-1},r_{t^k-1}\}$ and updates the posterior $\phi\bigl(\cdot\mid o_{t^k}\bigr)$.
Then, it samples a MDP $M^k$ from $\phi\bigl(\cdot\mid o_{t^k}\bigr)$ and computes an optimal policy $\pi^k$ in $M^k$.
The policy $\pi^k$ is then used to rollout episode $k$ and collect more observations.
The performance of PSRL is measured by the Bayesian regret given in \eqref{eq:bayes_rg}.
In broad terms, we bound the Bayesian regret by bounding the regret with terms that are deterministic and the expected value of deterministic term is itself.
To do so, we will first show that the expected value of $\Delta^k_{model}$ defined in \eqref{eq:rg_model_conc} is zero.
Then, the term $\Delta^k_{conc}$ is bounded in exactly the same manner as what we did with UCRL2.
\begin{lem}
    \label{lem:expected_identity}
    For any $k\ge1$, let $t^k$ be the time step that episode $k$ starts and $o_{t^k}$ be the observations collected right before $t^k$.
    Assume that the unknown MDP $M$ is drawn according to the prior $\phi$ and that $M^k$ is sampled according to the posterior $\phi\bigl(\cdot\mid o_{t^k}\bigr)$. Then, for any $o_{t^k}$-measurable function $f$, one has
    \begin{equation}
        \label{eq:exp_iden}
        \E[f(M)] =\E[f(M^k)].
    \end{equation}
\end{lem}
\begin{proof}
    At the start of each episode $k$, $M$ and $M^k$ are identically distributed conditioned on $o_{t^k}$.
    In consequence, if $f$ is $o_{t^k}$-measurable function, one has:
    \begin{align*}    
        \E[f(M) \mid o_{t^k}] =\E[f(M^k) \mid o_{t^k}].
    \end{align*}
    \Eqref{eq:exp_iden} then follows from the tower rule.
\end{proof}
This lemma implies that $\ex{w^{\pi^k}_{M^k}}=\ex{w^{\pi^*}_M}$ because $M^k$ and $\pi^k$ are $o_{t^k}$-measurable.
Consequently, $\ex{\Delta^k_{model}}=\vzero$ for PSRL.
The term $\Delta^k_{conc}$ can be rewritten as in \eqref{ch:rl:eq:delta_conc}.
Since $d_t$ is a martingale difference term, its expected value is zero.
The term $\abs{r^k-r}$ and $\norm{\vp^k-\vp}_{\ell_1}$ can be bounded using Hoeffding's and Weissman's inequalities.

PSRL enjoys the following Bayesian regret guarantee.
\begin{prop}[{\cite[Theorem~1]{osband2013more}}]
    \label{prop:brg_psrl}
    In finite horizon setting with horizon $H$, if $\phi$ is the prior distribution of the unknown MDP $M$ with $\abs{\gS}$ states and $\abs{\gA}$ actions, then
    \begin{equation}
        \label{eq:brg_psrl}
        \BayReg(\mathrm{PSRL}, \phi, K) = \landauO\left(H\abs{\gS}\sqrt{\abs{\gA}KH\ln\Bigl(\abs{\gS}\abs{\gA}KH\Bigr)}\right).
    \end{equation}
\end{prop}
So, this bound is of order $\tilde{\landauO}(H\abs{\gS}\sqrt{\abs{\gA}T})$ where $T=KH$ and $\tilde{\landauO}(\cdot)$ hides the logarithmic terms.
It is loose by a factor $\sqrt{H\abs{\gS}}$ compared to the lower bound $\landauOmega(\sqrt{H\abs{\gS}\abs{\gA}T})$.
Note that this bound is for Bayesian regret which means that this bound is a weaker guarantee compared to the regret bound with high probability of UCRL2 and UCBVI.

%    In general, in MDP $M=\langle\mSpace,\aSpace,r,P,H\rangle$, the agent starts to play from state $x$ with a certain probability given by the distribution $\rho(x)$. When using the algorithm $\Algo$ to play for $T$ time steps, the incurring regret is given by
%    \begin{align}
%        \Reg(T,\Algo,M)=\sum_{k=1}^{\lceil T/H\rceil}\sum_{x\in\mcal{E}}\rho(x)\big(V_{M,1}^*(x) -V_{M,1}^{\pik}(x)\big) =\sum_{k=1}^{\lceil T/H\rceil}\sum_{x\in\mcal{E}}\rho(x)\Delta_k(x)
%    \end{align}
%    where, during $k$th episode, $\Algo$ follows policy $\pik$ which is optimal in the imaginary MDP $\Mk$, $V^*_{M,1}(x)$ and $V_{M,1}^{\pik}(x)$ are the value of state $x$ under the optimal policy and the policy $\pik$ starting from time step $1$ to $H$ respectively, and $H$ is the horizon of each episode.
%    Note that we write $V_M$ as the value evaluated using the real MDP $M$ and $V_{\Mk}$ as the one evaluated using the imaginary MDP $\Mk$. It is convenient to seperate the regret gap into two gaps as the following:
%    \begin{enumerate}
%        \item the regret gap at $k$th episode:
%        \begin{align*}
%        \Delta_k &=V_{M,1}^* -V_{M,1}^{\pik} \\
%        &=\big(V^*_{M,1} -V^{\pik}_{\Mk,1}\big) +\big(V^{\pik}_{\Mk,1} -V_{M,1}^{\pik}\big) \\
%        &=\Delta_k^{opt}+\Delta_k^{conc}
%        \end{align*}
%    \item the gap from optimism $\Delta_k^{opt}(x)=V^*_{M,1}(x) -V^{\pik}_{\Mk,1}(x)$, where $V^{\pik}_{\Mk,1}(x)$ is the value of state $x$ under the policy $\pik$ from time step $1$ to $H$ and evaluated in the imagined MDP $\Mk$
%    \item the gap from concentration $\Delta_k^{conc}(x)=V^{\pik}_{\Mk,1}(x) -V_{M,1}^{\pik}(x)$.
%    \end{enumerate}
%    So,
%    \begin{equation}
%    \label{eq:eq_6}
%    \Reg(T,\Algo,M)= \sum_{k=1}^{\lceil T/H\rceil}\sum_{x\in\mcal{E}}\rho(x)(\Delta_k^{opt}(x)+\Delta_k^{conc}(x))
%    \end{equation}
%
%\section{UCRL2}
%\label{sec:ucrl2}
%
%    \subsection{Optimism in Face of Uncertainty(OFU)}
%    \label{subsec:OFU}
%    
%    This algorithm uses the principle of "Optimism in Face of Uncertainty": when we are \textbf{uncertain} about the outcome, we consider the \textit{best possible world} and choose the \textit{best decision} in imaginary world. Let $\mcal{O}_k$ be the set of observations collected until the beginning of episode $k$. UCRL2 does
%        \begin{enumerate}
%            \item \label{it:ucrl2_1} compute a set of plausible MDPs, $\mcal{M}_k$, based on the observations $\mcal{O}_k$ at the beginning of episode $k$. This set contains the unknown MDP $M$ with high probability (this is equivalent to compute the sets $\mcal{R}_k$ and $\mcal{P}_k$ containing $r$ and $P$ respectively with high probability)
%            \item compute $\pik$ such that
%                \begin{align}
%                    V_{\Mk,1}^{\pik} \ge \max_{\pi}\max_{\Mbar\in\mcal{M}_k} V_{\Mbar,1}^\pi
%                \end{align}
%            \item follow $\pik$ during the episode $k$
%            \item after finishing episode $k$, update $\mcal{O}_{k+1}$ and go back to \ref{it:ucrl2_1}
%        \end{enumerate}
%    
%    \subsection{High Probability Event}
%    \label{subsec:high_prob_event}
%        In order to build the sets $\mcal{R}$ and $\mcal{P}$, we should review some inequalities that are vital to our construction.
%    
%        \begin{thm}[Hoeffding inequality]
%        \label{thm:hoeffding}
%            Let $\{R_i\}_{i\le n}$ be independent and identically distributed random variable with mean $r$ bounded in $[0,1]$.
%            By Hoeffding inequality,
%            \begin{align*}
%                \Proba{\Big\vert\frac{1}{n}\sum_{i=1}^nR_i-r\Big\vert>\ep}\le2\exp\left(-2n\ep^2\right)
%            \end{align*}
%            The random variables $\{R_i\}_{i\le n}$ bounded in $[0,1]$ are $(1/2)$-sub-Gaussian.
%        \end{thm}
%        \begin{thm}[Weissman inequality (Inequalities for L1 deviation of the empirical distribution)]
%        \label{thm:weissman}
%            Let $\{Y_i\}_{i\le n}$ be $n$ i.i.d sample from a distribution $P$ over a finite set $\mSpace=\{1,\dots,\mSize\}$. Define the empirical distribution $\Pbar(y)=\frac{1}{n}\sum_{i=1}^{n}\mathbb{I}_{Y_i=y}, \forall y\in\mSpace$. Then, for any $\ep>0$,
%            \begin{align*}
%                \Proba{\Vert\Pbar-P\Vert_1> \ep} \le 2^\mSize\exp\left(-\frac{n\ep^2}{2}\right)
%            \end{align*}
%        \end{thm}
%        
%        At time $t$, let $\rbar_t$ and $\Pbar_t$ be the empirical estimators of $r$ and $P$ respectively.
%        Let $N_t(x,a)$ be the number of visits to state-action pair $(x,a)$ up to time $t$.
%        We have
%        \begin{align*}
%            \rbar_t(x,a) =\frac1{N_t(x,a)}\sum_{h=1}^{t}R_h\I_{\{X_h=x\wedge A_h=a\}} \text{ and } \Pbar_t(x,a,y) =\frac1{N_t(x,a)}\sum_{h=1}^{t}\I_{\{X_h=x\wedge A_h=a\wedge X_{h+1}=y\}}
%        \end{align*}
%
%        Now, we prove a high probability event:
%        \begin{lem}
%        \label{lem:high_prob_event}
%            Let $L_t:=\sqrt{2\log(4EAt/\delta)}$. Then, the event
%            \begin{align}
%            \event_t:=\bigg\{\forall a\in\mcal{A}, \forall x\in\mcal{E}, t'\le t: &\vert\rbar_{t'}(x,a)-r(x,a)\vert\le \frac{L_t}{2\sqrt{N_{t'}(x,a)}}\\
%            \text{ and } &\Vert\Pbar_{t'}(x,a)-P(x,a)\Vert_1\le \frac{L_t+1.5\sqrt{E}}{\sqrt{N_{t'}(x,a)}}\bigg\}
%            \end{align}
%            is true with probability $1-\delta$.
%        \end{lem}
%        \begin{proof}
%            By Theorem~\ref{thm:hoeffding}, for any $\ep>0$, one has
%            \begin{align*}
%            \Proba{\vert\rbar_{t}(x,a)-r(x,a)\vert>\ep \mid N_{t}(x,a)} \le 2\exp\left(-2N_{t}(x,a)\ep^2\right)
%            \end{align*}
%            This holds for $\ep=\sqrt{\frac{\log(4EAt/\delta)}{2N_t(x,a)}}$ where $0<\delta<1$.
%            As $N_t(x,a)\le t$, by using the union-bound, this implies that
%            \begin{align}
%            &\Proba{\exists x,a, t'\le t: \vert\rbar_{t'}(x,a)-r(x,a)\vert>\sqrt{\frac{\log(4EAt/\delta)}{2N_{t'}(x,a)}}} \label{eq:conc_r}\\
%            &\qquad \le \sum_{t'=1}^{t}\sum_{x,a} \Proba{\vert\rbar_{t'}(x,a)-r(x,a)\vert>\sqrt{\frac{\log(4EAt/\delta)}{2N_{t'}(x,a)}}} \nonumber \\
%            &\qquad \le \sum_{t'=1}^{t}\sum_{x,a} 2\exp\left(-2N_{t'}(x,a)\times\frac{\log(4EAt/\delta)}{2N_{t'}(x,a)}\right) =2EAt \frac{\delta}{4EAt}=\frac{\delta}2 \nonumber
%            \end{align}
%            By Theorem~\ref{thm:weissman}, for any $\ep>0$, one has
%            \begin{align*}
%            \Proba{\Vert\Pbar_t(x,a)-P(x,a)\Vert_1 > \ep \mid N_t(x,a)} \le 2^E\exp\left(-\frac{N_t(x,a)\ep^2}{2}\right)
%            \end{align*}
%            Similarly, with $\ep=\sqrt{2\log(2EAt2^E/\delta)/N_t(x,a)}$, we have
%            \begin{align*}
%            &\Proba{\exists x,a, t'\le t: \Vert\Pbar_t(x,a)-P(x,a)\Vert_1 >\sqrt{\frac{2\log(2EAt2^E/\delta)}{N_{t'}(x,a)}}} \\
%            &\qquad \le 2^EEAt\exp\left(-\frac{N_t(x,a)}{2}\times\frac{2\log(2EAt2^E/\delta)}{N_{t'}(x,a)}\right) =\frac{\delta}2
%            \end{align*}
%            Since $L_t=\sqrt{2\log(4EAt/\delta)}$ and $\sqrt{x+y}\le\sqrt{x}+\sqrt{y}$, we have $\sqrt{2\log(2EAt2^E/\delta)}=\sqrt{2\log(4EAt/\delta) +2(E-1)\log2}\le L_t + \sqrt{2(E-1)\log2} \le L_t +1.5\sqrt{E}$.
%            Hence,
%            \begin{align}
%            \label{eq:conc_p}
%            \Proba{\exists x,a, t'\le t: \Vert\Pbar_t(x,a)-P(x,a)\Vert_1 >\frac{L_t+1.5\sqrt{E}}{\sqrt{N_{t'}(x,a)}}} \le \frac{\delta}2
%            \end{align}
%            The complement of event $\event_t$ defined in the statement of Lemma~\ref{lem:high_prob_event} is the union of \eqref{eq:conc_r} and \eqref{eq:conc_p}.
%            Hence, the union-bound concludes the proof of the lemma.
%        \end{proof}
%
%\subsection{Regret bound}
%\label{subsec:regret_ucrl2}
%
%%TOCONTINUE
%    Let $t_k:=(k-1)H+1$ and $L_{t_k}=\sqrt{2\log(4EAt_k/\delta)}$.
%    Define the set of plausible MDPs by
%    \begin{align*}
%        \mcal{M}_k :=\left\{(r_k,P_k) \colon \forall x,a, \vert r_k(x,a)-\rbar_k(x,a)\vert \le \frac{L_{t_k}}{2\sqrt{N_k(x,a)}} \text{ and }\right. \\
%        \left. \Vert P_k(x,a)-\Pbar_k(x,a)\Vert_1\le \frac{L_{t_k}+1.5\sqrt{E}}{\sqrt{N_k(x,a)}}\right\}.
%    \end{align*}
%    So, under event $\event_k$, the unknown MDP $M$ belongs to $\mcal{M}_k$.
%    By the definition of $\pik$, $\Delta_k^{opt}=V^*_{M,1}(x) -V^{\pik}_{\Mk,1}(x)\le0$.
%    Then, under event $\event_k$,
%    \begin{align*}
%        \sum_{k=1}^{\lceil T/H\rceil}\Delta_k \le \sum_{k=1}^{\lceil T/H\rceil}\Delta_k^{conc}.
%    \end{align*}
%    
%    \subsubsection{Gap from concentration}
%    \label{subsubsec:sec_gap_conc}
%    
%        Let $a_{kh}=\pik(x_{kh})$ be the action that policy $\pik$ takes when at state $x_{kh}$ at time step $h$ in episode $k$. From Bellman's equation, we have
%        \begin{align*}
%            V^{\pik}_{M,H}(x_{k1})
%            &= r^{\pik}(x_{k1}) +\sum_{y\in\mSpace}P^{\pik}(x_{k1},y)V^{\pik}_{M,H-1}(y)
%        \end{align*}
%        In vector form, we have
%        \begin{align*}
%            V^{\pik}_{M,H} &= r^{\pik} +P^{\pik}V^{\pik}_{M,H-1} \\
%            V^{\pik}_{M_k,H} &= r^{\pik}_k +P^{\pik}_kV^{\pik}_{M_k,H-1}.
%        \end{align*}
%        Then,
%        \begin{align*}
%            \Delta_k^{conc}
%            &= V^{\pik}_{M_k,1} -V^{\pik}_{M,1} \\
%            &= r^{\pik}_k-r^{\pik} +(P^{\pik}_k -P^{\pik})V^{\pik}_{M_k,2} +P^{\pik}(V^{\pik}_{M,2}-V^{\pik}_{M_k,2}).
%        \end{align*}
%        So, at state $x_{k1}$
%        \begin{align*}
%            \Delta_k^{conc}(x_{k1})
%            &= r^{\pik}_k(x_{k1})-r^{\pik}(x_{k1}) +[(P^{\pik}_k(x_{k1},\cdot) -P^{\pik}(x_{k1},\cdot))V^{\pik}_{M_k,2}] \\
%            &\qquad +\ex{V^{\pik}_{M,2}(Y)-V^{\pik}_{M_k,2}(Y) \mid Y\sim P^{\pik}(x_{k1},\cdot)} \\
%            &\le \vert r^{\pik}_k(x_{k1}) -r^{\pik}(x_{k1})\vert +(H-1)\Vert P^{\pik}_k(x_{k1},\cdot) -P^{\pik}(x_{k1},\cdot)\Vert_1 \\
%            &\qquad +\ex{V^{\pik}_{M,2}(Y)-V^{\pik}_{M_k,2}(Y) \mid Y\sim P^{\pik}(x_{k1},\cdot)}.
%        \end{align*}
%        Continue to rollout Bellman's equation for $V^{\pik}_{M_k,h}-V^{\pik}_{M,h}$ with $2\le h\le H$, we get
%        %TOCONTINUE
%        \begin{align*}
%            \ex{\Delta_k^{conc}} \le \sum_{h=1}^{H}\vert r_k-r\vert(x_{kh},a_{kh}) +(H-1)\Vert P_k -P\Vert_1(x_{kh},a_{kh}).
%        \end{align*}
%        Now,
%        \begin{align*}
%            \sum_{k=1}^{\lceil T/H\rceil}\ex{\Delta_k^{conc}}
%            &\le \sum_{k=1}^{\lceil T/H\rceil} \sum_{h=1}^{H}\vert r_k-r\vert(x_{kh},a_{kh}) +(H-1)\Vert P_k -P\Vert_1(x_{kh},a_{kh}) \\
%            &=\sum_{k=1}^{\lceil T/H\rceil} \sum_{t=t_k}^{t_{k+1}-1}\vert r_k(x_t,a_t)-r(x_t,a_t)\vert +(H-1)\Vert P_k(x_t,a_t) -P(x_t,a_t)\Vert_1 \\
%            &\overset{(a)}\le \sum_{k=1}^{\lceil T/H\rceil} \sum_{t=t_k}^{t_{k+1}-1} \frac{L_{t_k}}{2\sqrt{N_k(x_t,a_t)}} +(H-1)\frac{L_{t_k}+1.5\sqrt{E}}{\sqrt{N_k(x_t,a_t)}} \le\sum_{k=1}^{\lceil T/H\rceil} \sum_{t=t_k}^{t_{k+1}-1}H\frac{L_T+1.5\sqrt{E}}{\sqrt{N_k(x_t,a_t)}}
%        \end{align*}
%        where $(a)$ is true under event $\event_k$.
%        For $t\le t_k$, let $\tilde{N}_t(x,a)$ be the real-time counter for state-action pair $(x,a)$. We have $\forall t\le T, \forall (x,a)\in \mdpStateSpace\times\ActionSpace$,
%        
%        \begin{align*}
%            N_{\lfloor t/H\rfloor+1}(x,a) +\horizon \ge \tilde{N}_t(x,a) &\ge N_{\lfloor t/H\rfloor+1}(x,a) \ge 1 \\
%            N_{\lfloor t/H\rfloor+1}(x,a) &\ge \max\{1, \tilde{N}_t(x,a) -\horizon\} \\
%            \sum_{k=1}^{\lceil T/\horizon\rceil} \sum_{t=t_k}^{t_{k+1}-1} \frac{1}{\sqrt{N_{\lfloor t/H\rfloor+1}(x_t,a_t)}} &\le \sum_{k=1}^{\lceil T/\horizon\rceil} \sum_{t=t_k}^{t_{k+1}-1} \min\left\{1, \frac{1}{\sqrt{\tilde{N}_t(x_t,a_t) -\horizon}}\right\} \\
%            \sum_{k=1}^{\lceil T/\horizon\rceil} \sum_{t=t_k}^{t_{k+1}-1} \frac{1}{\sqrt{N_k(x_t,a_t)}} &\le \sum_{t=1}^{T} \min\left\{1, \frac{1}{\sqrt{\tilde{N}_t(x_t,a_t) -\horizon}}\right\} \\
%            &= \sum_{x,a} \sum_{t=1}^{\tilde{N}_T(x,a)} \min\left\{1, \frac{1}{\sqrt{t -\horizon}}\right\} \\
%            &= \sum_{x,a} \Big(H +\sum_{t=H+1}^{\tilde{N}_T(x,a)} \frac{1}{\sqrt{t -H}}\Big) \\
%            &= EAH +\sum_{x,a} \sum_{t=1}^{\tilde{N}_T(x,a) -\horizon} \frac{1}{\sqrt{t}} \\
%            &\le EAH +\sum_{x,a} \int_0^{\tilde{N}_T(x,a) -\horizon} \frac{dt}{\sqrt{t}} \\
%            &\le EAH +\sum_{x,a} \int_0^{\tilde{N}_T(x,a)} \frac{dt}{\sqrt{t}} \\
%            &= EAH +2\sum_{x,a} \sqrt{\tilde{N}_T(x,a)} \\
%            &\overset{(a)}\le EAH +2\sqrt{EA \sum_{x,a}\tilde{N}_T(x,a)} \\
%            &= EAH +2\sqrt{EAT}
%        \end{align*}
%        where inequality $(a)$ is true by Jensen's inequality. Indeed, using Jensen's inequality, we have
%        \begin{align*}
%            \frac{1}{EA}\sum_{x,a}\sqrt{\tilde{N}_T(x,a)} \le\sqrt{\frac{1}{EA}\sum_{x,a}\tilde{N}_T(x,a)}.
%        \end{align*}
%        Thus,
%        \begin{align*}
%            \sum_{k=1}^{\lceil T/\horizon\rceil}\ex{\Delta_k^{conc}}
%            &\le H\Big(L_T+1.5\sqrt{E}\Big) \Big(EAH +2\sqrt{EAT}\Big) \\
%            &= H\Big(\sqrt{2\log(4EAT/\delta)} +1.5\sqrt{E}\Big) \Big(EAH +2\sqrt{EAT}\Big)
%        \end{align*}
%        Now,
%        \begin{align*}
%            \ex{\Delta_k}
%            &=\ex{\Delta_k\I_{\event_k} +\Delta_k\I_{\neg\event_k}} \\
%            &\le \ex{\Delta_k\I_{\event_k}} +H\Proba{\neg\event_k} =\ex{\Delta_k\I_{\event_k}} +H\delta.
%        \end{align*}
%        Finally, the expected regret UCRL2 is bounded by
%        \begin{align*}
%            \ex{\Reg(T,\mathrm{UCRL2},M)} 
%            &\le H\Big(\sqrt{2\log(4EAT/\delta)} +1.5\sqrt{E}\Big) \Big(EAH +2\sqrt{EAT}\Big) +\sum_{k=1}^{\lceil T/H\rceil}H\delta \\
%            &\le H\Big(\sqrt{2\log(4EAT/\delta)} +1.5\sqrt{E}\Big) \Big(EAH +2\sqrt{EAT}\Big) +T\delta
%        \end{align*}
%        
%        This result is a significant statistical guarantee as its scaling is close to the fundamental lower bound $\Omega(\sqrt{HEAT})$ \cite{jaksch2010near, osband2016lower}.
%
%\section{UCBVI}
%\label{sec:ucbvi}
%    OFU algorithm presented above applies the confidence sets on the parameters of the model, namely $(r,P)$. Another approach from \cite{azar2017minimax} is to directly apply the confidence set on the value function. One known algorithm is UCBVI that works as the following
%    
%    At the start of episode $k$,
%    \begin{enumerate}
%        \item compute the empirical reward and transition $(\rbar_k,\Pbar_k)$ for all state-action pairs
%        \item set $V_{k,H+1}=0$
%        \item for each time step $h=H,\dots,1$, compute the value and quality function for each $(x,a)$ state-action pairs:
%        \begin{align*}
%            Q_{k,h}(x,a) &=\min\left\{Q_{k-1,h}(x,a), H, \rbar_k(x,a) +\Pbar_k(x,a)V_{k,h+1} +b_k(x,a)\right\} \\
%            V_{k,h}(x) &= \max_{a\in\ActionSpace}Q_{k,h}(x,a)
%        \end{align*}
%        \item during episode $k$, for each time step $h=1,\dots,H$, 
%        \begin{align*}
%            \pol_{k}(x_{kh}) =\arg\max_{a\in\ActionSpace}Q_{k,h}(x_{kh},a)=\Action_{kh}
%        \end{align*}
%    \end{enumerate}
%    
%    \subsection{High Probability Event}
%    \label{subsec:high_prob_event1}
%    
%        Let $L_t:=\log(4E^2At/\delta)$. For any deterministic mappings $f\colon \mcal{E}\mapsto[0,H-1]$, and any $g\colon\mcal{E}\mapsto[0,H]$, we define the following events:
%        \begin{align}
%            &\event_1:=\left\{\forall x, a, t'\le t,|\rhat_{t'}(x,a)-r(x,a)+[(\Phat_{t'}-P)f](x,a)| \le H\sqrt{\frac{L_t}{2N_{t'}(x,a)}}\right\} \\
%            &\event_2:=\left\{\forall x, a, y, t'\le t,|\Phat_{t'}(x,a,y)-P(x,a,y)|g(y) \le\sqrt{\frac{2P(x,a,y)[1-P(x,a,y)]g^2(y)L_t}{N_{t'}(x,a)}} +\frac{2HL_t}{3N_{t'}(x,a)}\right\} \\
%            &\event:=\event_1\cap\event_2.
%        \end{align}
%        
%        \begin{lem}
%        \label{lem:high_prob}
%            For any deterministic mappings $f\colon \mcal{E}\mapsto[0,H-1]$ and $g\colon \mcal{E}\mapsto[0,H]$, we have $\Proba{\event}\ge1-\delta$.
%        \end{lem}
%        \begin{proof}
%        Given $\{U_i\}_{1\le i\le n}$ independent samples of random variable $U\in[0, c]$ where $c\in\R$, Theorem~\ref{thm:hoeffding} gives, for $\ep>0$,
%        \begin{align*}
%        \Proba{|\bar{U}-\ex{U}|\ge \varepsilon} \le 2e^{-2\frac{n}{c^2}\varepsilon^2}
%        \end{align*}
%        Consider a fixed $(x,a)$.
%        For any deterministic mapping $f\colon\mcal{E}\mapsto[0,H-1]$, let $U:=R+\beta f(Y)$ be a random variable where $R\in[0,1]$ and $Y\in\mcal{E}$ are random variables such that $\ex{R}=r(x,a)$ and $\Proba{Y=y}=P(x,a,y)$.
%        Hence, the expectation of $U$ is given by 
%        \begin{align*}
%        \ex{U}=r(x,a) +\sum_{y\in\mcal{E}}P(x,a,y)f(y)=r(x,a) +P(x,a)f.
%        \end{align*}
%        Also, $U\in [0,H]$.
%        After $n$ of activations, we will have collected $\{U_i\}_{1\le i\le n}$.
%        The empirical estimate of $U$ is
%        $$\bar{U}:=\frac1n\sum_{i=1}^{n} U_i=\rbar(x,a) +\Pbar(x,a)f.$$
%        Then,
%        \begin{align*}
%        &\Proba{\vert\bar{U}-\ex{U}\vert >\varepsilon} \le2e^{-2\frac{n\varepsilon^2}{H^2}} \\
%        &\Proba{\vert \rbar(x,a)-r(x,a) +(\Pbar(x,a) -P(x,a))f\vert >\varepsilon} \le2e^{-2\frac{n\varepsilon^2}{H^2}}.
%        \end{align*}
%        Using the same approach above,
%        \begin{align*}
%        \Proba{\neg\event_1} 
%        &=\Proba{\exists x,a, t'\le t,|\rbar_{t'}(x,a)-r(x,a)+(\Pbar_{t'}(x,a)-P(x,a))f| >H\sqrt{\frac{L_t}{2N_{t'}(x_a)}}} \\
%        &\le2\sum_{x,a} \sum_{t'=1}^{t}e^{-\log(4E^2At/\delta)} =2EAt\frac{\delta}{4E^2At}=\frac{\delta}{2E}
%        \end{align*}
%        Given $\{U_i\}_{1\le i\le n}$ independent samples of random variable $U\in[0, c]$ where $c\in\R$ and $\var{U}=\sigma^2$, Bernstein inequality gives
%        \begin{align*}
%        \Proba{|\bar{U}-\ex{U}|> \varepsilon} \le 2e^{-\frac{n\varepsilon^2}{2\sigma^2+\frac{2c}3\varepsilon}}
%        \end{align*}
%        Consider a fixed $(x,a,y)$. For any deterministic mapping $g\colon\mcal{E}\mapsto[0,H]$, let $U:=g(Y)$ be a random variable and $Y\in\mcal{E}$ where $\Proba{Y=y}=P(x,a,y)$.
%        So, we have
%        \begin{align*}
%        \ex{U}=P(x,a,y)g(y),\ \var{U}=P(x,a,y)(1-P(x,a,y))g^2(y):=\sigma^2(y), \text{ and } U\in[0,H].
%        \end{align*}
%        Then,
%        \begin{align*}
%        \Proba{\neg\event_2}
%        &=\Proba{\exists x,a,y,t'\le t,|\Pbar_{t'}(x,a,y)-P(x,a,y)|g(y) >\sqrt{\frac{2\sigma^2(y)L_t}{N_{t'}(x,a)}} +\frac{2HL_t}{3N_{t'}(x,a)}} \\
%        &\le\sum_{x,a,y} \sum_{t'=1}^{t} \Proba{|\Pbar_{t'}(x,a,y)-P(x,a,y)|g(y) \le\sqrt{\frac{2\sigma^2(y)L_t}{N_{t'}(x,a)}} +\frac{2HL_t}{3N_{t'}(x_a)}\mid N_{t'}(x_a)=n} \\
%        &\le E^2At 2\exp\left(-\frac{n\left(\sqrt{\frac{2\sigma^2(y)L_t}{n}} +\frac{2HL_t}{3n}\right)^2}{2\sigma^2(y)+\frac{2H}{3}\left(\sqrt{\frac{2\sigma^2(y)L_t}{n}} +\frac{2HL_t}{3n}\right)}\right) \\
%        &\le 2E^2At\exp\left(-\frac{n\left[\frac{2\sigma^2(y)L_t}{n} +\frac{2HL_t}{3n}\left(\sqrt{\frac{2\sigma^2(y)L_t}{n}} +\frac{2HL_t}{3n}\right)\right]}{2\sigma^2(y)+\frac{2H}{3}\left(\sqrt{\frac{2\sigma^2(y)L_t}{n}} +\frac{2HL_t}{3n}\right)}\right) \\
%        &=2E^2At\exp(-\log(4E^2At/\delta)) = \frac{\delta}2
%        \end{align*}
%        Now,
%        \begin{align*}
%        \Proba{\neg\event} 
%        &\le\Proba{\neg\event_1} +\Proba{\neg\event_2}\le\frac{\delta}{2E} +\frac{\delta}2\le \delta\frac{1+E}{2E} \le \delta.
%        \end{align*}
%        \end{proof}
%        
%        \begin{lem}
%        \label{lem:bern}
%        Under event $\event$, for any $g\colon\mcal{E}\mapsto[0,H]$ and any $x,a$,
%        \begin{align*}
%        \vert (\Pbar_t(x,a)-P(x,a))g \vert
%        \le \frac1H\ex{g(Y) \mid Y\sim P(x,a)} +\frac{SH^2L_t}{N_t(x,a)}
%        \end{align*}
%        \end{lem}
%        \begin{proof}
%        \begin{align*}
%        \vert (\Pbar_t(x,a)-P(x,a))g \vert
%        &= \vert\sum_{y\in\mcal{E}}\big(\Pbar_t(x,a,y) -P(x,a,y)\big)g(y)\vert \\
%        &\le \sum_{y\in\mcal{E}}|\Pbar_t(x,a,y) -P(x,a,y)|g(y) \\
%        &\overset{(a)}{\le} \sum_{y\in\mcal{E}} \sqrt{\frac{2P(x,a,y)[1-P(x,a,y)]g^2(y)L_t}{N_t(x,a)}} +\sum_{y\in\mcal{E}}\frac{2HL_t}{3N_t(x,a)} \\
%        &\le \sum_{y\in\mcal{E}} \sqrt{\frac{2P(x,a,y)g^2(y)L_t}{N_t(x,a)}} +\frac{2EHL_t}{3N_t(x,a)} \\
%        &\overset{(b)}{\le}\sqrt{E}\sqrt{\frac{2L_t}{N_t(x,a)} \sum_{y\in\mcal{E}}P(x,a,y)g^2(y)} +\frac{2EHL_t}{3N_t(x,a)} \\
%        &=\sqrt{\frac{2EH^2L_t}{N_t(x,a)} \sum_{y\in\mcal{E}}P(x,a,y)\frac{g^2(y)}{H^2}} +\frac{2EHL_t}{3N_t(x,a)} \\
%        &\overset{(c)}{\le}\frac{EH^2L_t}{N_t(x,a)} +\sum_{y\in\mcal{E}}P(x,a,y)\frac{g^2(y)}{H^2} +\frac{2EHL_t}{3N_t(x,a)} \\
%        &\overset{(d)}{\le} \sum_{y\in\mcal{E}}P(x,a,y)\frac{g(y)}{H} +\frac{EH^2L_t}{N_t(x,a)}\\
%        &=\frac1H\ex{g(Y) \mid Y\sim P(x,a)} +\frac{EH^2L_t}{N_t(x,a)} 
%        \end{align*}
%        where $(a)$ is true under the event $\xi$, $(b)$ is true due to Cauchy-Schwartz inequality, $(c)$ is true due to $ab\le(a^2+b^2)/2$ and $(d)$ is true due to $g\le H$.
%        \end{proof}
%        
%        \begin{lem}
%        \label{lem:bonus}
%        Let $L_t:=\log(4E^2At/\delta)$. For any $x,a$, define $b(x,a):=H\sqrt{\frac{L_t}{2N_t(x,a)}}$.
%        Let $\mTilde:=\langle\mcal{E}, [n], \rhat+b, \Phat\rangle$ be a MDP and $V_{\mTilde}^{*}$ be the optimal value function in $\mTilde$.
%        Under event $\event$, $$V_{\mTilde}^{*} \ge V_M^*.$$
%        \end{lem}
%        \begin{proof}
%        Let $\pik$ be the policy used during episode $k$. By Bellman, we have
%        \begin{align*}
%        V_{k,h}(x)
%        &= \rbar(x,\pik(x)) +b(x,\pik(x)) +\Pbar(x,\pik(x)) V_{k,h+1} \\
%        &\ge \rbar(x,\pi_*(x)) +b(x,\pi_*(x)) +\Pbar(x,\pi_*(x)) V_{k,h+1}.
%        \end{align*}
%        Then,
%        \begin{align*}
%        V_{k,h}(x) -V_{M,h}^{*}(x)
%        &= \rbar(x,\pik(x)) +b(x,\pik(x)) +\Pbar(x,\pik(x)) V_{k,h+1} 
%        -r(x,\pi_*(x)) -P(x,\pi_*(x))V_{M,h+1}^{*} \\
%        &\ge \rbar(x,\pi_*(x)) +b(x,\pi_*(x)) +(\Pbar(x,\pi_*(x)) V_{k,h+1}
%        -r(x,\pi_*(x)) -P(x,\pi_*(x))V_{M,h+1}^{*} \\
%        &= b(x,\pi_*(x)) +(\rbar-r)(x,\pi_*(x)) +[(\Pbar-P)V_{M,h+1}^{*}](x,\pi_*(x)) +\Pbar(x,\pi_*(x))(V_{k,h+1} -V_{M,h+1}^{*}).
%        \end{align*}
%        For $h=H$, we have $V_{k,H}(x) -V_{M,h}^*(x) \ge b(x,\pi_*(x)) +(\rbar-r)(x,\pi_*(x)) \ge0$ where the last inequality is true under event $\event_k$.
%        Suppose that $V_{k,h}\ge V_{M,h}^*$ is true for $h=\{h+1,\dots,H\}$.
%        We prove that $V_{k,l}\ge V_{M,l}^*$ is also true for $l=h$.
%        We have
%        \begin{align*}
%        V_{k,h}(x) -V_{M,h}^{*}(x)
%        &\ge b(x,\pi_*(x)) +(\rbar-r)(x,\pi_*(x)) +[(\Pbar-P)V_{M,h+1}^{*}](x,\pi_*(x)) +\Pbar(x,\pi_*(x))(V_{k,h+1} -V_{M,h+1}^{*}) \\
%        &\ge b(x,\pi_*(x)) +(\rbar-r)(x,\pi_*(x)) +[(\Pbar-P)V_{M,h+1}^{*}](x,\pi_*(x)) \ge0.
%        \end{align*}
%        where the last inequality is true, again, under event $\event_k$.
%        \end{proof}
%    
%    \subsection{Regret bound}
%    \label{subsec:regret_ucbvi}
%    
%    Let $b_k:=H\sqrt{\log(4E^2At_k/\delta)/(2N_k)}$. Under $\event_k$, Lemma~\ref{lem:bonus} implies that $V_M^*-V_M^{\pik} \le V_{k}-V_M^{\pik}$.
%    Then,
%    \begin{align*}
%    V_{k,1}(x_{k1}) -V_{M,1}^{\pik}(x_{k1})
%    &=\rbar_k(x_{k1},a_{k1}) +\Pbar_k(x_{k1},a_{k1})V_{k,2} +b_k(x_{k1},a_{k1}) -r(x_{k1},a_{k1}) -P(x_{k1},a_{k1})V_{M,2}^{\pik} \\
%    &=\rbar_k(x_{k1},a_{k1}) -r(x_{k1},a_{k1}) +(\Pbar_k(x_{k1},a_{k1}) -P(x_{k1},a_{k1}))V_{M,2}^*
%    \end{align*}
%    Since $\Pbar V_k -PV_M^{\pik}=(\Pbar-P)V_M^* +P(V_k-V_M^{\pik}) +(\Pbar-P)(V_k-V_M^*)$, we have
%    \begin{align*}
%    V_{k,1}(x_{k1}) -V_{M,1}^{\pik}(x_{k1})
%    &=\underbrace{\rbar_k(x_{k1},a_{k1}) -r(x_{k1},a_{k1}) +(\Pbar_k(x_{k1},a_{k1}) -P(x_{k1},a_{k1}))V_{M,2}^*}_{I_1} +b_k(x_{k1},a_{k1}) \\
%    &\qquad +P(x_{k1},a_{k1})(V_{k,2}-V_{M,2}^{\pik}) +\underbrace{(\Pbar_k(x_{k1},a_{k1})-P(x_{k1},a_{k1}))(V_{k,2}-V_{M,2}^*)}_{I_2}.
%    \end{align*}
%    Under $\event_k$, we have
%    \begin{align*}
%    I_1&\le H\sqrt{\frac{L_{t_k}}{2N_k(x_{k1},a_{k1})}}
%    \end{align*}
%    by when taking $f=V_{M,2}^{*}$ under event $\event_k$.
%    Also, by Lemma~\ref{lem:bern} with $g=V_{k,2} -V_{M,2}^{*}$,
%    \begin{align*}
%    I_2
%    &\le\frac1{H}\ex{V_{k,2}(Y) -V_{M,2}^{*}(Y) \mid Y\sim P(x_{k1}, a_{k1})} +\frac{EH^2L_{t_k}}{N_k(x,a)} \\
%    &\le\frac1{H}\ex{V_{k,2}(Y) -V_{M,2}^{\pik}(Y) \mid Y\sim P(x_{k1}, a_{k1})} +\frac{EH^2L_{t_k}}{N_k(x,a)}
%    \end{align*}
%    where the last inequality is true due to $V^*_M\ge V^{\pik}_{M}$.
%    Then,
%    \begin{align*}
%    V_{k,1}(x_{k1}) -V_{M,1}^{\pik}(x_{k1})
%    &\le b_k(x_{k1},a_{k1}) +H\sqrt{\frac{L_{t_k}}{2N_k(x_{k1},a_{k1})}} +\frac{EH^2L_{t_k}}{N_k(x_{k1},a_{k1})} \\
%    &\qquad +\left(1+\frac1H\right)\ex{V_{k,2}(Y) -V_{M,2}^{\pik}(Y) \mid Y\sim P(x_{k1}, a_{k1})}
%    \end{align*}
%    Continue to rollout $V_{k,h}-V_{M,h}^{\pik}$, we get
%    \begin{align*}
%    \ex{V_{k,1}(x_{k1})-V_{M,1}^{\pik}(x_{k1})}
%    &\le\sum_{t=t_k}^{t_{k+1}-1}\underbrace{\left(1+\frac1H\right)^{t-t_k}}_{\le e} \left(2H\sqrt{\frac{L_{t_k}}{2N_k(x_t,a_t)}} +\frac{EH^2L_{t_k}}{N_k(x_t,a_t)}\right).
%    \end{align*}
%    Then, the upper bound of expected regret of UCBVI is
%    \begin{align*}
%    \ex{\Reg(T,\mathrm{UCBVI},M)}
%    &\le e\sum_{k=1}^{\lceil T/H\rceil}\sum_{t=t_k}^{t_{k+1}-1}  \left(H\sqrt{\frac{2L_{t_k}}{N_k(x_t,a_t)}} +\frac{EH^2L_{t_k}}{N_k(x_t,a_t)}\right) +T\delta \\
%    &\le e\sum_{k=1}^{\lceil T/H\rceil}\sum_{t=t_k}^{t_{k+1}-1}  \left(H\sqrt{\frac{2L_T}{N_k(x_t,a_t)}} +\frac{EH^2L_T}{N_k(x_t,a_t)}\right) +T\delta
%    \end{align*}
%    Using the same approach to bound $\sum_{k=1}^{\lceil T/H\rceil}\sum_{t=t_k}^{t_{k+1}-1}[1/\sqrt{N_k(x_t,a_t)} +1/N_k(x_t,a_t)]$,
%    the expected regret is roughly upper bounded by $\tilde{O}\left(H\sqrt{EAT}\right)$.
%
%\section{PSRL}
%\label{sec:psrl}
%
%\subsection{Statistical Inference}
%\label{subsec:statistical_inf}
%
%Before presenting the algorithms, we want to review some interesting points in Statistical Inference that are related to our problem.
%
%Consider a statistical parametric model $\mathfrak{M}=\{p(\cdot|\theta);\theta\in\Theta\}$ where $\theta$ is a parameter of the model living in the parameter space $\Theta$, and $p(\cdot|\theta)$ is the probability measure of an event in function of the parameter $\theta$.
%
%- for discrete random variable X, we write $p(x|\theta)=\mathbb{P}_{\theta}\{X=x\}$
%- for continuous random variable X, we write $p(x|\theta)=f(x;\theta)$ where $f(x;\theta)$ is the (probability) density function
%
%We want to estimate the parameter of the model, $\theta$, using the observations. Suppose that we have
%
%- $(X_1,\dots,X_n)\rightarrow(x_1,\dots,x_n)$: $n$ observations sampled from the model
%- each observation is independent from one another
%
%To infer the parameter, there are two perspectives: frequentist and bayesian.
%
%\subsubsection{Frequentist perspective}
%\label{subsubsec:freq_perspective}
%
%One idea in frequentist perspective is to write a likelihood or log-likelihood in function of $\theta$ and observations $(x_1,\dots,x_n)$, then choose $\hat{\theta}$ that maximizes the likelihood:
%
%\begin{align*}
%l_{(x_1,\dots,x_n)}(\theta)&=\prod_{i=1}^{n}p(x_i|\theta) \\
%L_{(x_1,\dots,x_n)}(\theta)&=\sum_{i=1}^{n}\log p(x_i|\theta)
%\end{align*}
%
%and $$\hat{\theta}=\arg\max_{\theta\in\Theta}l_{(x_1,\dots,x_n)}(\theta)=\arg\max_{\theta\in\Theta}L_{(x_1,\dots,x_n)}(\theta)$$
%
%\subsubsection{Bayesian perspective}
%\label{subsubsec:bayes_perspective}
%
%One idea in this perspective is to derive the distribution of the parameter $\theta$ then draw the parameter from that distribution when needed. Concretely, we put a Prior distribution on the parameter $\theta$, use the observations to deduce the Posterior distribution, and sample the parameter $\hat{\theta}$ from the Posterior.
%
%- we make a hypothesis that the parameter has a certain distribution $p(\theta)$
%- the Likelihood of the observations: $l_{(x_1,\dots,x_n)}(\theta)=\prod_{i=1}^{n}p(x_i|\theta)$
%- from Bayes's theorem: $$p(\theta|x_1,\dots,x_n) =\frac{l_{(x_1,\dots,x_n)}(\theta)p(\theta)}{p(x_1,\dots,x_n)} \propto l_{(x_1,\dots,x_n)}(\theta)p(\theta)$$
%- when needed, we draw $\hat{\theta}\sim l_{(x_1,\dots,x_n)}(\theta)p(\theta)$
%
%\subsection{Application of Posterior Sampling}
%
%We make a hypothesis that the unknown $\mdpModel$ is drawn from a distribution $\phi(\mdpModel)$. It is our \textbf{prior distribution} about the MDP. So, we draw a MDP from that distribution, compute the respective optimal policy and play that policy in $\mdpModel$ for a whole episode to collect observations. Then, we refine our prior distribution based on Bayes' theorem and the observations collected. The refined distribution is called \textbf{posterior distribution}.
%
%Concretely, let $\mcal{O}_k$ be the observations collected overall episodes until time step $1$ of $k$th episode. So, the algorithm $\mathrm{PSRL}$ draws $M_k$ from the posterior distribution $\phi(\cdot|\mcal{O}_k)$, computes the optimal policy $\pik$ in that MDP and uses $\pik$ during the $k$th episode.
%
%\subsection{Bayesian Regret}
%\label{subsec:regret_psrl}
%
%In this section, we study the bayesian regret in posterior sampling approach and prove its bound when using PSRL algorithm. Suppose that the unknown MDP $M$ is drawn from $\phi$. The bayesian regret is defined by
%
%$$\BayReg(T,\mathrm{PSRL})=\mathbb{E}\left[\Reg(T,\mathrm{PSRL},M) |M\sim\phi\right]$$
%
%where $\Reg(T,\mathrm{PSRL},M)$ is defined in Equation$~\eqref{eq:eq_6}$. After playing $(k-1)$ episodes and collecting $\mcal{O}_{k}$, we believe that $M$ is drawn from the posterior distribution $\phi(\cdot|\mcal{O}_{k})$. So, $M$ and $M_k$ have the same distribution and for any $\sigma(\mcal{O}_{k})$-measurable function $f$, we have
%\begin{align*}
%\mathbb{E}[f(M)|\mcal{O}_{k}] =\mathbb{E}[f(M_k)|\mcal{O}_{k}]
%\end{align*}
%and applying the law of total expectation, we get $\mathbb{E}[f(M)] =\mathbb{E}[f(M_k)]$. This implies that $\mathbb{E}[\Delta_k^{opt}]=0$. Effectively, $\Delta_k^{opt}=V^*_{M,1}(x) -V^{\pik}_{M_k,1}(x)$ where the optimal value $V^*_M$ is a $\sigma(\mcal{O}_{k})$-measurable function of $M$ and the optimal value $V^{\pik}_{M_k}$ is a $\sigma(\mathcal{O}_{k})$-measurable function of $M_k$.
%
%So, we get
%\begin{align*}
%\BayReg(T,\mathrm{PSRL}) =\ex{\sum_{k=1}^{\lceil T/H\rceil}\Delta_k^{conc} |M\sim\phi}
%\end{align*}
%Following the same approach in the prove of UCRL2, we get the upper bound of Bayesian regret of PSRL as $O(HE\sqrt{AT})$.
%    
\endgroup
