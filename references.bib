%--------------------------------------------------------
%--------------------- MISC -----------------------------
%--------------------------------------------------------

% Multi-action
@article{hodge2015asymptotic,
  title={On the asymptotic optimality of greedy index heuristics for multi-action restless bandits},
  author={Hodge, David J and Glazebrook, Kevin D},
  journal={Advances in Applied Probability},
  volume={47},
  number={3},
  pages={652--667},
  year={2015},
  publisher={Cambridge University Press}
}

@article{glazebrook2011general,
  title={General notions of indexability for queueing control and asset management},
  author={Glazebrook, Kevin D and Hodge, David J and Kirkbride, Chris},
  journal={The Annals of Applied Probability},
  volume={21},
  number={3},
  pages={876--907},
  year={2011},
  publisher={Institute of Mathematical Statistics}
}

@inproceedings{meuleau1998solving,
  title={Solving very large weakly coupled Markov decision processes},
  author={Meuleau, Nicolas and Hauskrecht, Milos and Kim, Kee-Eung and Peshkin, Leonid and Kaelbling, Leslie Pack and Dean, Thomas L and Boutilier, Craig},
  booktitle={AAAI/IAAI},
  pages={165--172},
  year={1998}
}

@article{xiong2021reinforcement,
  title={Reinforcement Learning for Finite-Horizon Restless Multi-Armed Multi-Action Bandits},
  author={Xiong, Guojun and Li, Jian and Singh, Rahul},
  journal={arXiv preprint arXiv:2109.09855},
  year={2021}
}

@inproceedings{killian2021beyond,
  title={Beyond" To Act or Not to Act": Fast Lagrangian Approaches to General Multi-Action Restless Bandits},
  author={Killian, Jackson A and Perrault, Andrew and Tambe, Milind},
  booktitle={Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
  pages={710--718},
  year={2021}
}

@inproceedings{killian2021q,
  title={Q-Learning Lagrange Policies for Multi-Action Restless Bandits},
  author={Killian, Jackson A and Biswas, Arpita and Shah, Sanket and Tambe, Milind},
  booktitle={Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery \& Data Mining},
  pages={871--881},
  year={2021}
}

% Whittle Index

@article{gast2022computing,
  title={Computing Whittle (and Gittins) Index in Subcubic Time},
  author={Gast, Nicolas and Gaujal, Bruno and Khun, Kimang},
  journal={arXiv preprint arXiv:2203.05207},
  year={2022}
}

@article{akbarzadeh2020conditions,
    title={Conditions for indexability of restless bandits and an algorithm to compute Whittle index},
    author={Akbarzadeh, Nima and Mahajan, Aditya},
    journal={arXiv preprint arXiv:2008.06111},
    year={2020}
}

@book{lattimore2020bandit,
    title={Bandit algorithms},
    author={Lattimore, Tor and Szepesv{\'a}ri, Csaba},
    year={2020},
    publisher={Cambridge University Press}
}

@article{nino2007dynamic,
    title={Dynamic priority allocation via restless bandit marginal productivity indices},
    author={Ni{\~n}o-Mora, Jos{\'e}},
    journal={Top},
    volume={15},
    number={2},
    pages={161--198},
    year={2007},
    publisher={Springer}
}
@article{gast2020exponential,
    title={Exponential Convergence Rate for the Asymptotic Optimality of Whittle Index Policy},
    author={Gast, Nicolas and Gaujal, Bruno and Yan, Chen},
    journal={arXiv preprint arXiv:2012.09064},
    year={2020}
}
@article{bertsimas1996conservation,
    title={Conservation laws, extended polymatroids and multiarmed bandit problems; a polyhedral approach to indexable systems},
    author={Bertsimas, Dimitris and Ni{\~n}o-Mora, Jos{\'e}},
    journal={Mathematics of Operations Research},
    volume={21},
    number={2},
    pages={257--306},
    year={1996},
    publisher={INFORMS}
}

@article{katehakis1987multi,
    title={The multi-armed bandit problem: decomposition and computation},
    author={Katehakis, Michael N and Veinott Jr, Arthur F},
    journal={Mathematics of Operations Research},
    volume={12},
    number={2},
    pages={262--268},
    year={1987},
    publisher={INFORMS}
}

@article{sonin2008generalized,
    title={A generalized Gittins index for a Markov chain and its recursive calculation},
    author={Sonin, Isaac M},
    journal={Statistics \& Probability Letters},
    volume={78},
    number={12},
    pages={1526--1533},
    year={2008},
    publisher={Elsevier}
}

@article{chen1986linear,
    title={Linear programming for finite state multi-armed bandit problems},
    author={Chen, Yih Ren and Katehakis, Michael N},
    journal={Mathematics of Operations Research},
    volume={11},
    number={1},
    pages={180--183},
    year={1986},
    publisher={INFORMS}
}

@article{glazebrook2006some,
    title={Some indexable families of restless bandit problems},
    author={Glazebrook, Kevin D and Ruiz-Hernandez, Diego and Kirkbride, Christopher},
    journal={Advances in Applied Probability},
    volume={38},
    number={3},
    pages={643--672},
    year={2006},
    publisher={Cambridge University Press}
}

@article{glazebrook2002index,
    title={An index policy for a stochastic scheduling model with improving/deteriorating jobs},
    author={Glazebrook, KD and Mitchell, HM},
    journal={Naval Research Logistics (NRL)},
    volume={49},
    number={7},
    pages={706--721},
    year={2002},
    publisher={Wiley Online Library}
}

@article{ansell2003whittle,
    title={Whittle's index policy for a multi-class queueing system with convex holding costs},
    author={Ansell, PS and Glazebrook, Kevin D and Nino-Mora, Jos{\'e} and O'Keeffe, M},
    journal={Mathematical Methods of Operations Research},
    volume={57},
    number={1},
    pages={21--39},
    year={2003},
    publisher={Springer}
}

@article{lott2000optimality,
    title={On the optimality of an index rule in multichannel allocation for single-hop mobile networks with multiple service classes},
    author={Lott, Christopher and Teneketzis, Demosthenis},
    journal={Probability in the Engineering and Informational Sciences},
    volume={14},
    number={3},
    pages={259--297},
    year={2000},
    publisher={Cambridge University Press}
}

@inproceedings{papadimitriou1994complexity,
    title={The complexity of optimal queueing network control},
    author={Papadimitriou, Christos H and Tsitsiklis, John N},
    booktitle={Proceedings of IEEE 9th Annual Conference on Structure in Complexity Theory},
    pages={318--322},
    year={1994},
    organization={IEEE}
}

@article{villar2015multi,
    title={Multi-armed bandit models for the optimal design of clinical trials: benefits and challenges},
    author={Villar, Sof{\'\i}a S and Bowden, Jack and Wason, James},
    journal={Statistical science: a review journal of the Institute of Mathematical Statistics},
    volume={30},
    number={2},
    pages={199},
    year={2015},
    publisher={Europe PMC Funders}
}

@article{larranaga2015asymptotically,
    title={Asymptotically optimal index policies for an abandonment queue with convex holding cost},
    author={Larra{\~n}aga, Maialen and Ayesta, Urtzi and Verloop, Ina Maria},
    journal={Queueing systems},
    volume={81},
    number={2},
    pages={99--169},
    year={2015},
    publisher={Springer}
}

@article{glazebrook2009index,
    title={Index policies for the admission control and routing of impatient customers to heterogeneous service stations},
    author={Glazebrook, Kevin D and Kirkbride, Christopher and Ouenniche, Jamal},
    journal={Operations Research},
    volume={57},
    number={4},
    pages={975--989},
    year={2009},
    publisher={INFORMS}
}

@article{borkar2017whittle,
    title={Whittle indexability in egalitarian processor sharing systems},
    author={Borkar, Vivek S and Pattathil, Sarath},
    journal={Annals of Operations Research},
    pages={1--21},
    year={2017},
    publisher={Springer}
}

@article{archibald2009indexability,
    title={Indexability and index heuristics for a simple class of inventory routing problems},
    author={Archibald, Thomas W and Black, DP and Glazebrook, Kevin D},
    journal={Operations research},
    volume={57},
    number={2},
    pages={314--326},
    year={2009},
    publisher={INFORMS}
}

@inproceedings{avrachenkov2018impulsive,
    title={Impulsive control for G-AIMD dynamics with relaxed and hard constraints},
    author={Avrachenkov, Konstantin and Piunovskiy, Alexei and Zhang, Yi},
    booktitle={2018 IEEE Conference on Decision and Control (CDC)},
    pages={880--887},
    year={2018},
    organization={IEEE}
}

@article{avrachenkov2013congestion,
    title={Congestion control of TCP flows in Internet routers by means of index policy},
    author={Avrachenkov, Konstantin and Ayesta, Urtzi and Doncel, Josu and Jacko, Peter},
    journal={Computer Networks},
    volume={57},
    number={17},
    pages={3463--3478},
    year={2013},
    publisher={Elsevier}
}

@inproceedings{nino2014dynamic,
    title={A dynamic page-refresh index policy for web crawlers},
    author={Ni{\~n}o-Mora, Jos{\'e}},
    booktitle={International Conference on Analytical and Stochastic Modeling Techniques and Applications},
    pages={46--60},
    year={2014},
    organization={Springer}
}

@article{aalto2019whittle,
    title={Whittle index approach to opportunistic scheduling with partial channel information},
    author={Aalto, Samuli and Lassila, Pasi and Taboada, Ianire},
    journal={Performance Evaluation},
    volume={136},
    pages={102052},
    year={2019},
    publisher={Elsevier}
}

@article{robbins1952some,
    title={Some aspects of the sequential design of experiments},
    author={Robbins, Herbert},
    journal={Bulletin of the American Mathematical Society},
    volume={58},
    number={5},
    pages={527--535},
    year={1952},
    publisher={American Mathematical Society}
}

@article{whittle1988restless,
    title={Restless bandits: Activity allocation in a changing world},
    author={Whittle, Peter},
    journal={Journal of applied probability},
    volume={25},
    number={A},
    pages={287--298},
    year={1988},
    publisher={Cambridge University Press}
}

@article{sherman_moris,
    author = {Egidi, N. and Maponi, P.},
    title = {A Sherman-Morrison Approach to the Solution of Linear Systems},
    year = {2006}, issue_date = {May, 2006},
    publisher = {Elsevier Science Publishers B. V.},
    address = {NLD},
    volume = {189},
    number = {1–2},
    issn = {0377-0427},
    url = {https://doi.org/10.1016/j.cam.2005.02.013},
    doi = {10.1016/j.cam.2005.02.013},
    abstract = {We propose a new direct method to solve linear systems. This method is based on the Sherman-Morrison formula and uses a finite iterative formula. To compare our method with the Restarted Generalized Minimum Residual Method and the Gaussian Elimination Method with Partial Pivoting, we use two classes of test problems: linear systems having Pascal, Cauchy, and Vandermonde matrices as coefficient matrices, and randomly generated linear systems.},
    journal = {J. Comput. Appl. Math.},
    month = may,
    pages = {703–718},
    numpages = {16},
    keywords = {Direct method, Sherman-Morrison formula, Linear system}
}

@article{nakhleh2020neurwin,
    title={NeurWIN: Neural Whittle Index Network for Restless Bandits via Deep RL},
    author={Nakhleh, Khaled and Ganji, Santosh and Hsieh, Ping-Chun and Hou, I-Hong and Shakkottai, Srinivas},
    year={2020}
}

@inproceedings{avrachenkov2019learning,
    title={A learning algorithm for the Whittle index policy for scheduling web crawlers},
    author={Avrachenkov, Konstantin and Borkar, Vivek S},
    booktitle={2019 57th Annual Allerton Conference on Communication, Control, and Computing (Allerton)},
    pages={1001--1006},
    year={2019},
    organization={IEEE}
}
@inproceedings{fu2019towards,
    title={Towards q-learning the whittle index for restless bandits},
    author={Fu, Jing and Nazarathy, Yoni and Moka, Sarat and Taylor, Peter G},
    booktitle={2019 Australian \& New Zealand Control Conference (ANZCC)},
    pages={249--254},
    year={2019},
    organization={IEEE}
}
@article{nino2020fast,
    title={A fast-pivoting algorithm for Whittle’s restless bandit index},
    author={Ni{\~n}o-Mora, Jos{\'e}},
    journal={Mathematics},
    volume={8},
    number={12},
    pages={2226},
    year={2020},
    publisher={Multidisciplinary Digital Publishing Institute}
}

@article{avrachenkov2020whittle,
    title={Whittle index based Q-learning for restless bandits with average reward},
    author={Avrachenkov, Konstantin and Borkar, Vivek S},
    journal={arXiv preprint arXiv:2004.14427},
    year={2020}
}

@article{akbarzadeh2020restless,
    title={Restless bandits: indexability and computation of Whittle index},
    author={Akbarzadeh, Nima and Mahajan, Aditya},
    journal={arXiv e-prints},
    pages={arXiv--2008},
    year={2020}
}
@article{akbarzadeh2021maintenance,
    title={Maintenance of a collection of machines under partial observability: Indexability and computation of Whittle index},
    author={Akbarzadeh, Nima and Mahajan, Aditya},
    journal={arXiv preprint arXiv:2104.05151},
    year={2021}
}
@inproceedings{akbarzadeh2019restless,
    title={Restless bandits with controlled restarts: Indexability and computation of Whittle index},
    author={Akbarzadeh, Nima and Mahajan, Aditya},
    booktitle={2019 IEEE 58th Conference on Decision and Control (CDC)},
    pages={7294--7300},
    year={2019},
    organization={IEEE}
}
@article{ayesta2021computation,
    title={On the computation of Whittle’s index for Markovian restless bandits},
    author={Ayesta, Urtzi and Gupta, Manu K and Verloop, Ina Maria},
    journal={Mathematical Methods of Operations Research},
    volume={93},
    number={1},
    pages={179--208},
    year={2021},
    publisher={Springer}
}

@article{pan1972schemes,
    title={On schemes for the computation of products and inverses of matrices},
    author={Pan, V Ya},
    journal={Russian Math. Surveys},
    volume={27},
    number={5},
    pages={249--250},
    year={1972}
}

@book{woodbury1950inverting,
    title={Inverting modified matrices},
    author={Woodbury, Max A},
    year={1950},
    publisher={Statistical Research Group}
}
@book{cormen2009introduction,
    title={Introduction to algorithms},
    author={Cormen, Thomas H and Leiserson, Charles E and Rivest, Ronald L and Stein, Clifford},
    year={2009},
    publisher={MIT press}
}
@inproceedings{huang2016strassen,
    title={Strassen's algorithm reloaded},
    author={Huang, Jianyu and Smith, Tyler M and Henry, Greg M and Van De Geijn, Robert A},
    booktitle={SC'16: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
    pages={690--701},
    year={2016},
    organization={IEEE}
}
@article{huang1998fast,
    title={Fast rectangular matrix multiplication and applications},
    author={Huang, Xiaohan and Pan, Victor Y},
    journal={Journal of complexity},
    volume={14},
    number={2},
    pages={257--299},
    year={1998},
    publisher={Elsevier}
}
@inproceedings{alman2021refined,
    title={A refined laser method and faster matrix multiplication},
    author={Alman, Josh and Williams, Virginia Vassilevska},
    booktitle={Proceedings of the 2021 ACM-SIAM Symposium on Discrete Algorithms (SODA)},
    pages={522--539},
    year={2021},
    organization={SIAM}
}
@phdthesis{huang2018practical,
    title={Practical fast matrix multiplication algorithms},
    author={Huang, Jianyu and others},
    year={2018}
}
@inproceedings{coppersmith1987matrix,
    title={Matrix multiplication via arithmetic progressions},
    author={Coppersmith, Don and Winograd, Shmuel},
    booktitle={Proceedings of the nineteenth annual ACM symposium on Theory of computing},
    pages={1--6},
    year={1987}
}
@article{strassen1969gaussian,
    title={Gaussian elimination is not optimal},
    author={Strassen, Volker},
    journal={Numerische mathematik},
    volume={13},
    number={4},
    pages={354--356},
    year={1969},
    publisher={Springer}
}
@inproceedings{le2014powers,
  title={Powers of tensors and fast matrix multiplication},
  author={Le Gall, Fran{\c{c}}ois},
  booktitle={Proceedings of the 39th international symposium on symbolic and algebraic computation},
  pages={296--303},
  year={2014}
}
@inproceedings{gall2018improved,
  title={Improved rectangular matrix multiplication using powers of the Coppersmith-Winograd tensor},
  author={Gall, Fran{\c{c}}ois Le and Urrutia, Florent},
  booktitle={Proceedings of the Twenty-Ninth Annual ACM-SIAM Symposium on Discrete Algorithms},
  pages={1029--1046},
  year={2018},
  organization={SIAM}
}

@inproceedings{fuQlearningWhittleIndex2019,
  title = {Towards {{Q}}-Learning the {{Whittle Index}} for {{Restless Bandits}}},
  booktitle = {2019 {{Australian New Zealand Control Conference}} ({{ANZCC}})},
  author = {Fu, Jing and Nazarathy, Yoni and Moka, Sarat and Taylor, Peter G.},
  year = {2019},
  month = nov,
  pages = {249--254},
  doi = {10.1109/ANZCC47194.2019.8945748},
  abstract = {We consider the multi-armed restless bandit problem (RMABP) with an infinite horizon average cost objective. Each arm of the RMABP is associated with a Markov process that operates in two modes: active and passive. At each time slot a controller needs to designate a subset of the arms to be active, of which the associated processes will evolve differently from the passive case. Treated as an optimal control problem, the optimal solution of the RMABP is known to be computationally intractable. In many cases, the Whittle index policy achieves near optimal performance and can be tractably found. Nevertheless, computation of the Whittle indices requires knowledge of the transition matrices of the underlying processes, which are sometimes hidden from decision makers. In this paper, we take first steps towards a tractable and efficient reinforcement learning algorithm for controlling such a system. We setup parallel Q-learning recursions, with each recursion mapping to individual possible values of the Whittle index. We then update these recursions as we control the system, learning an approximation of the Whittle index as time evolves. Tested on several examples, our control outperforms naive priority allocations and nears the performance of the fully-informed Whittle index policy.},
  file = {/Users/kimangkhun/Zotero/storage/KKAHD669/8945748.html},
  keywords = {infinite horizon,infinite horizon average cost objective,Markov process,Markov processes,matrix algebra,multiarmed restless bandit problem,optimal control,optimal control problem,Q-learning recursions,reinforcement learning algorithm,RMABP,Whittle index policy}
}

@article{avrachenkovWhittleIndexBased2020,
  title = {Whittle Index Based {{Q}}-Learning for Restless Bandits with Average Reward},
  author = {Avrachenkov, Konstantin and Borkar, Vivek S.},
  year = {2020},
  month = apr,
  abstract = {A novel reinforcement learning algorithm is introduced for multiarmed restless bandits with average reward, using the paradigms of Q-learning and Whittle index. Specifically, we leverage the structure of the Whittle index policy to reduce the search space of Q-learning, resulting in major computational gains. Rigorous convergence analysis is provided, supported by numerical experiments. The numerical experiments show excellent empirical performance of the proposed scheme.},
  archivePrefix = {arXiv},
  eprint = {2004.14427},
  eprinttype = {arxiv},
  file = {/Users/kimangkhun/Zotero/storage/6834772B/Avrachenkov and Borkar - 2020 - Whittle index based Q-learning for restless bandit.pdf;/Users/kimangkhun/Zotero/storage/IFM3DYZV/2004.html},
  journal = {arXiv:2004.14427 [cs, math, stat]},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  primaryClass = {cs, math, stat}
}

@article{liuGammaRegretNonEpisodic2020,
  title = {$\gamma$-{{Regret}} for {{Non}}-{{Episodic Reinforcement Learning}}},
  author = {Liu, Shuang and Su, Hao},
  year = {2020},
  month = jun,
  abstract = {Reinforcement learning (RL) has traditionally been understood from an episodic perspective; the concept of non-episodic RL, where there is no restart and therefore no reliable recovery, remains elusive. A fundamental question in non-episodic RL is how to measure the performance of a learner and derive algorithms to maximize such performance. Conventional wisdom is to maximize the difference between the average reward received by the learner and the maximal long-term average reward. In this paper, we argue that if the total time budget is relatively limited compared to the complexity of the environment, such comparison may fail to reflect the finite-time optimality of the learner. We propose a family of measures, called {$\gamma$}-regret, which we believe to better capture the finite-time optimality. We give motivations and derive lower and upper bounds for such measures.},
  archivePrefix = {arXiv},
  eprint = {2002.05138},
  eprinttype = {arxiv},
  file = {/Users/kimangkhun/Zotero/storage/UHN9B9QU/Liu and Su - 2020 - $gamma$-Regret for Non-Episodic Reinforcement Lea.pdf},
  journal = {arXiv:2002.05138 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@article{zhouProvablyEfficientReinforcement2021,
  title = {Provably {{Efficient Reinforcement Learning}} for {{Discounted MDPs}} with {{Feature Mapping}}},
  author = {Zhou, Dongruo and He, Jiafan and Gu, Quanquan},
  year = {2021},
  month = feb,
  abstract = {Modern tasks in reinforcement learning have large state and action spaces. To deal with them efficiently, one often uses predefined feature mapping to represent states and actions in a low-dimensional space. In this paper, we study reinforcement learning for discounted Markov Decision Processes (MDPs), where the transition kernel can be parameterized as a linear function of certain feature mapping. We propose a novel algorithm that makes use of the feature mapping and obtains a $\textbackslash{}tilde O(d\textbackslash{}sqrt\{T\}/(1-\textbackslash{}gamma)\^2)$ regret, where $d$ is the dimension of the feature space, $T$ is the time horizon and $\gamma$ is the discount factor of the MDP. To the best of our knowledge, this is the first polynomial regret bound without accessing the generative model or making strong assumptions such as ergodicity of the MDP. By constructing a special class of MDPs, we also show that for any algorithms, the regret is lower bounded by $\textbackslash{}Omega(d\textbackslash{}sqrt\{T\}/(1-\textbackslash{}gamma)\^\{1.5\})$. Our upper and lower bound results together suggest that the proposed reinforcement learning algorithm is near-optimal up to a $(1-\textbackslash{}gamma)\^\{-0.5\}$ factor.},
  archivePrefix = {arXiv},
  eprint = {2006.13165},
  eprinttype = {arxiv},
  file = {/Users/kimangkhun/Zotero/storage/6ABIKL8G/Zhou et al. - 2021 - Provably Efficient Reinforcement Learning for Disc.pdf},
  journal = {arXiv:2006.13165 [cs, math, stat]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, math, stat}
}

@inproceedings{ortner2012regret,
  title={Regret bounds for restless markov bandits},
  author={Ortner, Ronald and Ryabko, Daniil and Auer, Peter and Munos, R{\'e}mi},
  booktitle={International conference on algorithmic learning theory},
  pages={214--228},
  year={2012},
  organization={Springer}
}

@inproceedings{strehlTheoreticalAnalysisModelBased2005,
  title = {A Theoretical Analysis of {{Model}}-{{Based Interval Estimation}}},
  booktitle = {Proc. of the 22nd Int. Conf. on {{Machine}} Learning  - {{ICML}} '05},
  author = {Strehl, Alexander L. and Littman, Michael L.},
  year = {2005},
  pages = {856--863},
  publisher = {{ACM Press}},
  address = {{Bonn, Germany}},
  doi = {10.1145/1102351.1102459},
  abstract = {Several algorithms for learning near-optimal policies in Markov Decision Processes have been analyzed and proven efficient. Empirical results have suggested that Model-based Interval Estimation (MBIE) learns efficiently in practice, effectively balancing exploration and exploitation. This paper presents the first theoretical analysis of MBIE, proving its efficiency even under worst-case conditions. The paper also introduces a new performance metric, average loss, and relates it to its less ``online'' cousins from the literature.},
  file = {/Users/kimangkhun/Zotero/storage/3MLKJZBS/Strehl and Littman - 2005 - A theoretical analysis of Model-Based Interval Est.pdf},
  isbn = {978-1-59593-180-1},
  language = {en}
}

@book{whittle1996optimal,
  title={Optimal Control: Basics and Beyond},
  author={Whittle, P.},
  isbn={9780471960997},
  lccn={95022113},
  series={Wiley Interscience Series in Systems and Optimization},
  url={https://books.google.fr/books?id=6wZhQgAACAAJ},
  year={1996},
  publisher={Wiley}
}

@article{russo2018tutorial,
  title={A Tutorial on Thompson Sampling},
  author={Russo, Daniel J and Van Roy, Benjamin and Kazerouni, Abbas and Osband, Ian and Wen, Zheng},
  journal={Foundations and Trends{\textregistered} in Machine Learning},
  volume={11},
  number={1},
  pages={1--96},
  year={2018},
  publisher={Now Publishers Inc.}
}

% GittinsPS

@article{gast2021reinforcement,
  title={Reinforcement Learning for Markovian Bandits: Is Posterior Sampling more Scalable than Optimism?},
  author={Gast, Nicolas and Gaujal, Bruno and Khun, Kimang},
  journal={arXiv preprint arXiv:2106.08771},
  year={2021}
}

@article{jung2019thompson,
  title={Thompson sampling in non-episodic restless bandits},
  author={Jung, Young Hun and Abeille, Marc and Tewari, Ambuj},
  journal={arXiv preprint arXiv:1910.05654},
  year={2019}
}

@article{wang2020restless,
  title={Restless-UCB, an Efficient and Low-complexity Algorithm for Online Restless Bandits},
  author={Wang, Siwei and Huang, Longbo and Lui, John},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={11878--11889},
  year={2020}
}

@article{jung2019regret,
  title={Regret bounds for thompson sampling in episodic restless bandit problems},
  author={Jung, Young Hun and Tewari, Ambuj},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@article{jaksch2010near,
  title={Near-optimal Regret Bounds for Reinforcement Learning.},
  author={Jaksch, Thomas and Ortner, Ronald and Auer, Peter},
  journal={Journal of Machine Learning Research},
  volume={11},
  number={4},
  year={2010}
}

@article{osband2016lower,
  title={On lower bounds for regret in reinforcement learning},
  author={Osband, Ian and Van Roy, Benjamin},
  journal={arXiv preprint arXiv:1608.02732},
  year={2016}
}

@inproceedings{azar2017minimax,
  title={Minimax regret bounds for reinforcement learning},
  author={Azar, Mohammad Gheshlaghi and Osband, Ian and Munos, R{\'e}mi},
  booktitle={International Conference on Machine Learning},
  pages={263--272},
  year={2017},
  organization={PMLR}
}

@inproceedings{bourel2020tightening,
  title={Tightening exploration in upper confidence reinforcement learning},
  author={Bourel, Hippolyte and Maillard, Odalric and Talebi, Mohammad Sadegh},
  booktitle={International Conference on Machine Learning},
  pages={1056--1066},
  year={2020},
  organization={PMLR}
}

@article{bubeck2012regret,
  title={Regret Analysis of Stochastic and Nonstochastic Multi-armed Bandit Problems},
  author={Bubeck, S{\'e}bastien and Cesa-Bianchi, Nicolo},
  journal={Machine Learning},
  volume={5},
  number={1},
  pages={1--122},
  year={2012}
}


@Book{Vershynin,
  author = 	 {Roman Vershynin},
  title = 	 {High-dimensional Probability},
  publisher = 	 {Cambridge University Press},
  year = 	 2018
}

@Article{Aalto1,
  author = 	 {Samuel Aalto and Urtzi Ayesta and Rhonda Righter},
  title = 	 {On the {G}ittins index in the M/G/1 queue},
  journal =  {Queueing Systems},
  year = 	 2009,
  volume = 	 63,
  number = 	 1,
  pages = 	 {437--458}
}

@Article{Aalto2,
  author = 	 {Samuel Aalto and Urtzi Ayesta and Rhonda Righter},
  title = 	 {Properties of the {G}ittins index with applications to optimal scheduling},
  journal =  {Probability in the Engineering and Informational Sciences},
  year = 	 2011,
  volume = 	 23,
  number = 	 3,
  pages = 	 {269--288}
}

@Article{Grosof,
  author = 	 {Isaac Grosof and Ziv Scully and Mor Harchol-Balter},
  title = 	 {{SRPT} for the multiserver systems},
  journal =  {Performance Evaluation},
  year = 	 2018,
  volume = 	 {127-128},
  pages = 	 {154--175}
}

@article{weissman2003inequalities,
  title={Inequalities for the L1 deviation of the empirical distribution},
  author={Weissman, Tsachy and Ordentlich, Erik and Seroussi, Gadiel and Verdu, Sergio and Weinberger, Marcelo J},
  journal={Hewlett-Packard Labs, Tech. Rep},
  year={2003}
}

@article{hamidi2020worst,
  title={On Worst-case Regret of Linear Thompson Sampling},
  author={Hamidi, Nima and Bayati, Mohsen},
  journal={arXiv preprint arXiv:2006.06790},
  year={2020}
}

@article{aalto2011properties,
  title={Properties of the Gittins index with application to optimal scheduling},
  author={Aalto, Samuli and Ayesta, Urtzi and Righter, Rhonda},
  journal={Probability in the Engineering and Informational Sciences},
  volume={25},
  number={3},
  pages={269--288},
  year={2011},
  publisher={Cambridge University Press}
}

@article{liu2010indexability,
  title={Indexability of restless bandit problems and optimality of whittle index for dynamic multichannel access},
  author={Liu, Keqin and Zhao, Qing},
  journal={IEEE Transactions on Information Theory},
  volume={56},
  number={11},
  pages={5547--5567},
  year={2010},
  publisher={IEEE}
}

@article{scully2018soap,
  title={SOAP: One clean analysis of all age-based scheduling policies},
  author={Scully, Ziv and Harchol-Balter, Mor and Scheller-Wolf, Alan},
  journal={Proceedings of the ACM on Measurement and Analysis of Computing Systems},
  volume={2},
  number={1},
  pages={1--30},
  year={2018},
  publisher={ACM New York, NY, USA}
}
@article{weber1990index,
  title={On an index policy for restless bandits},
  author={Weber, Richard R and Weiss, Gideon},
  journal={Journal of applied probability},
  pages={637--648},
  year={1990},
  publisher={JSTOR}
}
@article{aalto2009gittins,
  title={On the Gittins index in the M/G/1 queue},
  author={Aalto, Samuli and Ayesta, Urtzi and Righter, Rhonda},
  journal={Queueing Systems},
  volume={63},
  number={1-4},
  pages={437},
  year={2009},
  publisher={Springer}
}

@inproceedings{aalto2015whittle,
  title={Whittle index approach to size-aware scheduling with time-varying channels},
  author={Aalto, Samuli and Lassila, Pasi and Osti, Prajwal},
  booktitle={Proceedings of the 2015 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Systems},
  pages={57--69},
  year={2015}
}

@article{tekinOnlineAlgorithmsMultiArmed2010,
  title = {Online {{Algorithms}} for the {{Multi}}-{{Armed Bandit Problem}} with {{Markovian Rewards}}},
  author = {Tekin, Cem and Liu, Mingyan},
  year = {2010},
  month = jul,
  abstract = {We consider the classical multi-armed bandit problem with Markovian rewards. When played an arm changes its state in a Markovian fashion while it remains frozen when not played. The player receives a state-dependent reward each time it plays an arm. The number of states and the state transition probabilities of an arm are unknown to the player. The player's objective is to maximize its long-term total reward by learning the best arm over time. We show that under certain conditions on the state transition probabilities of the arms, a sample mean based index policy achieves logarithmic regret uniformly over the total number of trials. The result shows that sample mean based index policies can be applied to learning problems under the rested Markovian bandit model without loss of optimality in the order. Moreover, comparision between Anantharam's index policy and UCB shows that by choosing a small exploration parameter UCB can have a smaller regret than Anantharam's index policy.},
  archivePrefix = {arXiv},
  eprint = {1007.2238},
  eprinttype = {arxiv},
  file = {/Users/kimangkhun/Zotero/storage/SKU4GDGF/Tekin and Liu - 2010 - Online Algorithms for the Multi-Armed Bandit Probl.pdf;/Users/kimangkhun/Zotero/storage/4RGNBAUT/1007.html},
  journal = {arXiv:1007.2238 [cs, math]},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control},
  primaryClass = {cs, math}
}

@article{anantharamAsymptoticallyEfficientAllocation1987,
  title = {Asymptotically Efficient Allocation Rules for the Multiarmed Bandit Problem with Multiple Plays-{{Part II}}: {{Markovian}} Rewards},
  shorttitle = {Asymptotically Efficient Allocation Rules for the Multiarmed Bandit Problem with Multiple Plays-{{Part II}}},
  author = {Anantharam, V. and Varaiya, P. and Walrand, J.},
  year = {1987},
  month = nov,
  volume = {32},
  pages = {977--982},
  issn = {1558-2523},
  doi = {10.1109/TAC.1987.1104485},
  abstract = {At each instant of time we are required to sample a fixed numberm \textbackslash{}geq 1out ofNMarkov chains whose stationary transition probability matrices belong to a family suitably parameterized by a real number{\th}eta. The objective is to maximize the long run expected value of the samples. The learning loss of a sampling scheme corresponding to a parameters configurationC = ({\th}eta\_1, ..., {\th}eta\_N)is quantified by the regretR\_n(C). This is the difference between the maximum expected reward that could be achieved ifCwere known and the expected reward actually achieved. We provide a lower bound for the regret associated with any uniformly good scheme, and construct a sampling scheme which attains the lower bound for everyC. The lower bound is given explicitly in terms of the Kullback-Liebler number between pairs of transition probabilities.},
  file = {/Users/kimangkhun/Zotero/storage/ZTEYU4EN/1104485.html},
  journal = {IEEE Transactions on Automatic Control},
  keywords = {Adaptive control,Arm,Computer science,Laboratories,Markov processes,Optimal stochastic control,Probability distribution,Random variables,Resource management,Sampling methods,State-space methods,Statistical distributions,Statistics,Stochastic optimal control,Stochastic processes},
  note = {Conference Name: IEEE Transactions on Automatic Control},
  number = {11}
}

@article{moulosHoeffdingInequalityFinite2020,
  title = {A {{Hoeffding Inequality}} for {{Finite State Markov Chains}} and Its {{Applications}} to {{Markovian Bandits}}},
  author = {Moulos, Vrettos},
  year = {2020},
  month = jul,
  abstract = {This paper develops a Hoeffding inequality for the partial sums \$\textbackslash{}sum\_\{k=1\}\^n f (X\_k)\$, where \$\textbackslash\{X\_k\textbackslash\}\_\{k \textbackslash{}in \textbackslash{}mathbb\{Z\}\_\{{$>$} 0\}\}\$ is an irreducible Markov chain on a finite state space \$S\$, and \$f : S \textbackslash{}to [a, b]\$ is a real-valued function. Our bound is simple, general, since it only assumes irreducibility and finiteness of the state space, and powerful. In order to demonstrate its usefulness we provide two applications in multi-armed bandit problems. The first is about identifying an approximately best Markovian arm, while the second is concerned with regret minimization in the context of Markovian bandits.},
  archivePrefix = {arXiv},
  eprint = {2001.01199},
  eprinttype = {arxiv},
  file = {/Users/kimangkhun/Zotero/storage/278MZRAK/Moulos - 2020 - A Hoeffding Inequality for Finite State Markov Cha.pdf;/Users/kimangkhun/Zotero/storage/RWM9GJXE/2001.html},
  journal = {arXiv:2001.01199 [cs, math, stat]},
  keywords = {Computer Science - Machine Learning,Mathematics - Statistics Theory,Statistics - Machine Learning},
  primaryClass = {cs, math, stat}
}

@article{lezaudChernofftypeBoundFinite1998,
  title = {Chernoff-Type Bound for Finite {{Markov}} Chains},
  author = {Lezaud, Pascal},
  year = {1998},
  month = aug,
  volume = {8},
  pages = {849--867},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {1050-5164, 2168-8737},
  doi = {10.1214/aoap/1028903453},
  abstract = {This paper develops bounds on the distribution function of the empirical mean for irreducible finite-state Markov chains. One approach, explored by Gillman, reduces this problem to bounding the largest eigenvalue of a perturbation of the transition matrix for the Markov chain. By using estimates on eigenvalues given in Kato's book Perturbation Theory for Linear Operators, we simplify the proof of Gillman and extend it to nonreversible finite-state Markov chains and continuous time. We also set out another method, directly applicable to some general ergodic Markov kernels having a spectral gap.},
  file = {/Users/kimangkhun/Zotero/storage/KP5J7YD4/Lezaud - 1998 - Chernoff-type bound for finite Markov chains.pdf;/Users/kimangkhun/Zotero/storage/WUDZ2NZS/1028903453.html},
  journal = {Annals of Applied Probability},
  keywords = {Chernoff bound,eigenvalues,Markov chain,perturbation theory},
  language = {en},
  mrnumber = {MR1627795},
  number = {3},
  zmnumber = {0938.60027}
}

@inproceedings{gillmanChernoffBoundRandom1993,
  title = {A {{Chernoff}} Bound for Random Walks on Expander Graphs},
  booktitle = {Proceedings of 1993 {{IEEE}} 34th {{Annual Foundations}} of {{Computer Science}}},
  author = {Gillman, D.},
  year = {1993},
  month = nov,
  pages = {680--691},
  doi = {10.1109/SFCS.1993.366819},
  abstract = {We consider a finite random walk on a weighted graph G; we show that the sample average of visits to a set of vertices A converges to the stationary probability /spl pi/(A) with error probability exponentially small in the length of the random walk and the square of the size of the deviation from /spl pi/(A). The exponential bound is in terms of the expansion of G and improves previous results. We show that the method of taking the sample average from one trajectory is a more efficient estimate of /spl pi/(A) than the standard method of generating independent sample points from several trajectories. Using this more efficient sampling method, we improve the algorithms of Jerrum and Sinclair (1989) for approximating the number of perfect matchings in a dense graph and for approximating the partition function of an Ising system. We also give a fast estimate of the entropy of a random walk on an unweighted graph.{$<>$}},
  file = {/Users/kimangkhun/Zotero/storage/RL2T3SEN/366819.html},
  keywords = {algorithm theory,Chernoff bound,Convergence,entropy,Entropy,error probability,Error probability,expander graphs,finite random walk,graph theory,Graph theory,Iron,Ising system,Mathematics,partition function,Partitioning algorithms,probability,Random variables,random walks,Sampling methods,weighted graph,Writing}
}

@article{gittins1979bandit,
  title={Bandit processes and dynamic allocation indices},
  author={Gittins, John C},
  journal={Journal of the Royal Statistical Society: Series B (Methodological)},
  volume={41},
  number={2},
  pages={148--164},
  year={1979},
  publisher={Wiley Online Library}
}

@article{katehakisMultiArmedBanditProblem1987a,
  title = {The {{Multi}}-{{Armed Bandit Problem}}: {{Decomposition}} and {{Computation}}},
  shorttitle = {The {{Multi}}-{{Armed Bandit Problem}}},
  author = {Katehakis, Michael N. and Veinott, Arthur F.},
  year = {1987},
  month = May,
  volume = {12},
  pages = {262--268},
  publisher = {{INFORMS}},
  issn = {0364-765X},
  doi = {10.1287/moor.12.2.262},
  abstract = {This paper is dedicated to our friend and mentor, Cyrus Derman, on the occasion of his 60th birthday.The multi-armed bandit problem arises in sequentially allocating effort to one of N projects and sequentially assigning patients to one of N treatments in clinical trials. Gittins and Jones (Gittins, J. C., Jones, D. M. 1974. A dynamic allocation index for the sequential design of experiments. J. Gani, K. Sarkadi, L. Vince, eds. Progress in Statistics. European Meeting of Statisticians, 1972, 1, North Holland, Amsterdam, 241\textendash{}266.) have shown that one optimal policy for the N-project problem, an N-dimensional discounted Markov decision chain, is determined by the following largest-index rule. There is an index for each state of each given project that depends only on the data of that project. In each period one allocates effort to a project with largest current index. The purpose of this paper is to give a short proof of this result and a new characterization of the index of a project in state i, viz., as the maximum expected present value in state i for the restart-in-i problem in which, in each state and period, one either continues allocating effort to the project or immediately restarts the project in state i. Moreover, it is shown that an approximate largest-index rule yields an approximately optimal policy. These results lead to more efficient methods of computing the indices on-line and/or for sparse transition matrices in large state spaces than have been suggested heretofore. By using a suitable implementation of successive approximations, a policy whose expected present value is within 100{$\epsilon\%$} of the maximum possible range of values of the indices can be found on-line with at most (N + T - 1)TM operations where M is the number of operations required to calculate one approximation, T is the least integer majorizing the ratio ln {$\epsilon$}/ln a and 0 {$<$} a {$<$} 1 is the discount factor.},
  file = {/Users/kimangkhun/Zotero/storage/P85PCDYX/Katehakis and Veinott - 1987 - The Multi-Armed Bandit Problem Decomposition and .pdf;/Users/kimangkhun/Zotero/storage/B4Y9GM2L/moor.12.2.html},
  journal = {Mathematics of Operations Research},
  number = {2}
}

@incollection{duff1995q,
  title={Q-learning for bandit problems},
  author={Duff, Michael O},
  booktitle={Machine Learning Proceedings 1995},
  pages={209--217},
  year={1995},
  publisher={Elsevier}
}

@article{auer2002finite,
  title={Finite-time analysis of the multiarmed bandit problem},
  author={Auer, Peter and Cesa-Bianchi, Nicolo and Fischer, Paul},
  journal={Machine learning},
  volume={47},
  number={2},
  pages={235--256},
  year={2002},
  publisher={Springer}
}

@article{bartlett2012regal,
  title={REGAL: A regularization based algorithm for reinforcement learning in weakly communicating MDPs},
  author={Bartlett, Peter L and Tewari, Ambuj},
  journal={arXiv preprint arXiv:1205.2661},
  year={2012}
}

@incollection{fruitRegretMinimizationMDPs2017,
  title = {Regret {{Minimization}} in {{MDPs}} with {{Options}} without {{Prior Knowledge}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 30},
  author = {Fruit, Ronan and Pirotta, Matteo and Lazaric, Alessandro and Brunskill, Emma},
  editor = {Guyon, I. and Luxburg, U. V. and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
  year = {2017},
  pages = {3166--3176},
  publisher = {{Curran Associates, Inc.}},
  file = {/Users/kimangkhun/Zotero/storage/WTNS6KIW/Fruit et al. - 2017 - Regret Minimization in MDPs with Options without P.pdf;/Users/kimangkhun/Zotero/storage/CBR9K4ZL/6909-regret-minimization-in-mdps-with-options-without-prior-knowledge.html}
}

@article{talebiVarianceAwareRegretBounds2018,
  title = {Variance-{{Aware Regret Bounds}} for {{Undiscounted Reinforcement Learning}} in {{MDPs}}},
  author = {Talebi, Mohammad Sadegh and Maillard, Odalric-Ambrym},
  year = {2018},
  month = mar,
  abstract = {The problem of reinforcement learning in an unknown and discrete Markov Decision Process (MDP) under the average-reward criterion is considered, when the learner interacts with the system in a single stream of observations, starting from an initial state without any reset. We revisit the minimax lower bound for that problem by making appear the local variance of the bias function in place of the diameter of the MDP. Furthermore, we provide a novel analysis of the KL-UCRL algorithm establishing a high-probability regret bound scaling as \$\textbackslash{}widetilde \{\textbackslash{}mathcal O\}\textbackslash{}Bigl(\{\textbackslash{}textstyle \textbackslash{}sqrt\{S\textbackslash{}sum\_\{s,a\}\{\textbackslash{}bf V\}\^\textbackslash{}star\_\{s,a\}T\}\}\textbackslash{}Big)\$ for this algorithm for ergodic MDPs, where \$S\$ denotes the number of states and where \$\{\textbackslash{}bf V\}\^\textbackslash{}star\_\{s,a\}\$ is the variance of the bias function with respect to the next-state distribution following action \$a\$ in state \$s\$. The resulting bound improves upon the best previously known regret bound \$\textbackslash{}widetilde \{\textbackslash{}mathcal O\}(DS\textbackslash{}sqrt\{AT\})\$ for that algorithm, where \$A\$ and \$D\$ respectively denote the maximum number of actions (per state) and the diameter of MDP. We finally compare the leading terms of the two bounds in some benchmark MDPs indicating that the derived bound can provide an order of magnitude improvement in some cases. Our analysis leverages novel variations of the transportation lemma combined with Kullback-Leibler concentration inequalities, that we believe to be of independent interest.},
  archivePrefix = {arXiv},
  eprint = {1803.01626},
  eprinttype = {arxiv},
  file = {/Users/kimangkhun/Zotero/storage/KIFVPBWC/Talebi and Maillard - 2018 - Variance-Aware Regret Bounds for Undiscounted Rein.pdf;/Users/kimangkhun/Zotero/storage/BE55B9VT/1803.html},
  journal = {arXiv:1803.01626 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Electrical Engineering and Systems Science - Systems and Control,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@inproceedings{fruitEfficientBiasSpanConstrainedExplorationExploitation2018,
  author        = "Ronan Fruit and Matteo Pirotta and Alessandro Lazaric and Ronald Ortner",
  booktitle     = "{Proceedings of the 35th International Conference on Machine Learning}",
  ee            = "http://proceedings.mlr.press/v80/fruit18a.html",
  pages         = "1573--1581",
  publisher     = "{PMLR}",
  title         = "{Efficient Bias-Span-Constrained Exploration-Exploitation in Reinforcement Learning}",
  year          = 2018,
}

@incollection{fruitOptimalExplorationExploitationNonCommunicating2018,
  title = {Near {{Optimal Exploration}}-{{Exploitation}} in {{Non}}-{{Communicating Markov Decision Processes}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 31},
  author = {Fruit, Ronan and Pirotta, Matteo and Lazaric, Alessandro},
  editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and {Cesa-Bianchi}, N. and Garnett, R.},
  year = {2018},
  pages = {2994--3004},
  publisher = {{Curran Associates, Inc.}},
  file = {/Users/kimangkhun/Zotero/storage/RAAFZMJE/Fruit et al. - 2018 - Near Optimal Exploration-Exploitation in Non-Commu.pdf;/Users/kimangkhun/Zotero/storage/KV76ARSL/7563-near-optimal-exploration-exploitation-in-non-communicating-markov-decision-processes.html}
}

@article{tossouNearoptimalOptimisticReinforcement2019,
  title = {Near-Optimal {{Optimistic Reinforcement Learning}} Using {{Empirical Bernstein Inequalities}}},
  author = {Tossou, Aristide and Basu, Debabrota and Dimitrakakis, Christos},
  year = {2019},
  month = dec,
  abstract = {We study model-based reinforcement learning in an unknown finite communicating Markov decision process. We propose a simple algorithm that leverages a variance based confidence interval. We show that the proposed algorithm, UCRL-V, achieves the optimal regret \$\textbackslash{}tilde\{\textbackslash{}mathcal\{O\}\}(\textbackslash{}sqrt\{DSAT\})\$ up to logarithmic factors, and so our work closes a gap with the lower bound without additional assumptions on the MDP. We perform experiments in a variety of environments that validates the theoretical bounds as well as prove UCRL-V to be better than the state-of-the-art algorithms.},
  archivePrefix = {arXiv},
  eprint = {1905.12425},
  eprinttype = {arxiv},
  file = {/Users/kimangkhun/Zotero/storage/5J2D82SR/Tossou et al. - 2019 - Near-optimal Optimistic Reinforcement Learning usi.pdf;/Users/kimangkhun/Zotero/storage/4CPG8TRJ/1905.html},
  journal = {arXiv:1905.12425 [cs, stat]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Science and Game Theory,Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{thompsonLikelihoodThatOne1933a,
  title = {On the {{Likelihood}} That {{One Unknown Probability Exceeds Another}} in {{View}} of the {{Evidence}} of {{Two Samples}}},
  author = {Thompson, William R.},
  year = {1933},
  volume = {25},
  pages = {285--294},
  publisher = {{[Oxford University Press, Biometrika Trust]}},
  issn = {0006-3444},
  doi = {10.2307/2332286},
  journal = {Biometrika},
  number = {3/4}
}

@inproceedings{bellemare2017distributional,
  title={A distributional perspective on reinforcement learning},
  author={Bellemare, Marc G and Dabney, Will and Munos, R{\'e}mi},
  booktitle={International Conference on Machine Learning},
  pages={449--458},
  year={2017},
  organization={PMLR}
}

@inproceedings{dabney2018distributional,
  title={Distributional reinforcement learning with quantile regression},
  author={Dabney, Will and Rowland, Mark and Bellemare, Marc and Munos, R{\'e}mi},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={32},
  number={1},
  year={2018}
}

@inproceedings{strens2000bayesian,
  title={A Bayesian framework for reinforcement learning},
  author={Strens, Malcolm},
  booktitle={ICML},
  volume={2000},
  pages={943--950},
  year={2000}
}

@inproceedings{gopalan2015thompson,
  title={Thompson sampling for learning parameterized markov decision processes},
  author={Gopalan, Aditya and Mannor, Shie},
  booktitle={Conference on Learning Theory},
  pages={861--898},
  year={2015},
  organization={PMLR}
}

@inproceedings{osband2017posterior,
  title={Why is posterior sampling better than optimism for reinforcement learning?},
  author={Osband, Ian and Van Roy, Benjamin},
  booktitle={Int. Conf. on Machine Learning},
  pages={2701--2710},
  year={2017},
  journal={arXiv preprint arXiv:1607.00215},
  organization={PMLR}
}

@article{osband2013more,
  title={(More) efficient reinforcement learning via posterior sampling},
  author={Osband, Ian and Russo, Daniel and Van Roy, Benjamin},
  journal={Advances in Neural Information Processing Systems},
  volume={26},
  year={2013}
}

@book{putermanMarkovDecisionProcesses1994,
  title = {Markov Decision Processes: Discrete Stochastic Dynamic Programming},
  shorttitle = {Markov Decision Processes},
  author = {Puterman, Martin L.},
  year = {1994},
  edition = {1st},
  publisher = {John Wiley \& Sons, Inc.},
  address = {USA},
  abstract = {From the Publisher: The past decade has seen considerable theoretical and applied research on Markov decision processes, as well as the growing use of these models in ecology, economics, communications engineering, and other fields where outcomes are uncertain and sequential decision-making processes are needed. A timely response to this increased activity, Martin L. Puterman's new work provides a uniquely up-to-date, unified, and rigorous treatment of the theoretical, computational, and applied research on Markov decision process models. It discusses all major research directions in the field, highlights many significant applications of Markov decision processes models, and explores numerous important topics that have previously been neglected or given cursory coverage in the literature. Markov Decision Processes focuses primarily on infinite horizon discrete time models and models with discrete time spaces while also examining models with arbitrary state spaces, finite horizon models, and continuous-time discrete state models. The book is organized around optimality criteria, using a common framework centered on the optimality (Bellman) equation for presenting results. The results are presented in a "theorem-proof" format and elaborated on through both discussion and examples, including results that are not available in any other book. A two-state Markov decision process model, presented in Chapter 3, is analyzed repeatedly throughout the book and demonstrates many results and algorithms. Markov Decision Processes covers recent research advances in such areas as countable state space models with average reward criterion, constrained models, and models with risk sensitive optimality criteria. It also explores several topics that have received little or no attention in other books, including modified policy iteration, multichain models with average reward criterion, and sensitive optimality. In addition, a Bibliographic Remarks section in each chapter comments on relevant historic},
  isbn = {978-0-471-61977-2}
}

@book{bookGittins,
author = {Gittins, John and Glazebrook, Kevin and Weber, Richard},
year = {2011},
month = {02},
pages = {},
title = {Multi-Armed Bandit Allocation Indices, 2nd Edition},
volume = {33},
journal = {Technometrics},
doi = {10.1002/9780470980033.ch8}
}

@article{chakravorty2014multi,
  title={Multi-armed bandits, Gittins index, and its calculation},
  author={Chakravorty, Jhelum and Mahajan, Aditya},
  journal={Methods and applications of statistics in clinical trials: Planning, analysis, and inferential methods},
  volume={2},
  number={416-435},
  pages={455},
  year={2014},
  publisher={Wiley New York, NY}
}

@article{weberGittinsIndexMultiarmed1992,
  title = {On the {{Gittins Index}} for {{Multiarmed Bandits}}},
  author = {Weber, Richard},
  year = {1992},
  month = nov,
  volume = {2},
  pages = {1024--1033},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {1050-5164, 2168-8737},
  doi = {10.1214/aoap/1177005588},
  abstract = {This paper considers the multiarmed bandit problem and presents a new proof of the optimality of the Gittins index policy. The proof is intuitive and does not require an interchange argument. The insight it affords is used to give a streamlined summary of previous research and to prove a new result: The optimal value function is a submodular set function of the available projects.},
  file = {/Users/kimangkhun/Zotero/storage/ANEA5JAP/Weber - 1992 - On the Gittins Index for Multiarmed Bandits.pdf;/Users/kimangkhun/Zotero/storage/EPJXTZ9I/1177005588.html},
  journal = {Annals of Applied Probability},
  keywords = {Markov decision processes,Multiarmed bandit problem,optimal stopping,sequential methods,stochastic scheduling},
  language = {EN},
  mrnumber = {MR1189430},
  number = {4},
  zmnumber = {0763.60021}
}

@article{soninEliminationAlgorithmProblem1999,
  title = {The {{Elimination}} Algorithm for the Problem of Optimal Stopping},
  author = {Sonin, Isaac},
  year = {1999},
  month = mar,
  volume = {49},
  pages = {111--123},
  issn = {1432-5217},
  doi = {10.1007/PL00020910},
  abstract = {We present a new algorithm for solving the optimal stopping problem. The algorithm is based on the idea of elimination of states where stopping is nonoptimal and the corresponding correction of transition probabilities. The formal justification of this method is given by one of two presented theorems. The other theorem describes the situation when an aggregation of states is possible in the optimal stopping problem.},
  journal = {Mathematical Methods of Operations Research},
  language = {en},
  number = {1}
}

@article{nino-moraFastPivotingAlgorithmGittins2007,
  title = {A (2/3) {\emph{n}} {\textsuperscript{3}} {{Fast}}-{{Pivoting Algorithm}} for the {{Gittins Index}} and {{Optimal Stopping}} of a {{Markov Chain}}},
  author = {{Ni{\~n}o-Mora}, Jos{\'e}},
  year = {2007},
  month = nov,
  volume = {19},
  pages = {596--606},
  issn = {1091-9856, 1526-5528},
  doi = {10.1287/ijoc.1060.0206},
  abstract = {This paper presents a new fast-pivoting algorithm that computes the n Gittins index values of an n-state bandit\textemdash{}in the discounted and undiscounted cases\textemdash{}by performing 2/3 n3 + O n2 arithmetic operations, thus attaining better complexity than previous algorithms and matching that of solving a corresponding linearequation system by Gaussian elimination. The algorithm further applies to the problem of optimal stopping of a Markov chain, for which a novel Gittins-index solution approach is introduced. The algorithm draws on Gittins and Jones' (1974) index definition via calibration, on Kallenberg's (1986) proposal of using parametric linear programming, on Dantzig's simplex method, on the Varaiya et al. (1985) algorithm, and on the author's earlier work. This paper elucidates the structure of parametric simplex tableaux. Special structure is exploited to reduce the computational effort of pivot steps, decreasing the operation count by a factor of three relative to conventional pivoting, and by a factor of 3/2 relative to recent state-elimination algorithms. A computational study demonstrates significant time savings against alternative algorithms.},
  file = {/Users/kimangkhun/Zotero/storage/BM88PI2G/Niño-Mora - 2007 - A (23) n 3 Fast-Pivoting Algori.pdf},
  journal = {INFORMS Journal on Computing},
  language = {en},
  number = {4}
}

@incollection{jaakkolaConvergenceStochasticIterative1994,
  title = {Convergence of {{Stochastic Iterative Dynamic Programming Algorithms}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 6},
  author = {Jaakkola, Tommi and Jordan, Michael I. and Singh, Satinder P.},
  editor = {Cowan, J. D. and Tesauro, G. and Alspector, J.},
  year = {1994},
  pages = {703--710},
  publisher = {{Morgan-Kaufmann}},
  file = {/Users/kimangkhun/Zotero/storage/TSMBCXSZ/Jaakkola et al. - 1994 - Convergence of Stochastic Iterative Dynamic Progra.pdf;/Users/kimangkhun/Zotero/storage/3NY3E5ZG/764-convergence-of-stochastic-iterative-dynamic-programming-algorithms.html}
}

@techreport{littmanGeneralizedReinforcementLearningModel1996,
  title = {A {{Generalized Reinforcement}}-{{Learning Model}}: {{Convergence}} and {{Applications}}},
  shorttitle = {A {{Generalized Reinforcement}}-{{Learning Model}}},
  author = {Littman, Michael L. and Szepesvári, Csaba},
  year = {1996},
  address = {{USA}},
  institution = {{Brown University}},
  abstract = {Reinforcement learning is the process by which an autonomous agent uses its experience interacting with an environment to improve its behavior. The Markov decision process (MDP) model is a popular way of formalizing the reinforcement-learning problem, but it is by no means the only way. In this paper, we show how many of the important theoretical results concerning reinforcement learning in MDPs extend to a generalized MDP model that includes MDPs, two-player games and MDPs under a worst-case optimality criterion as special cases. The basis of this extension is a stochastic-approximation theorem that reduces asynchronous convergence to synchronous convergence. Keywords: Reinforcement learning, Q-learning convergence, Markov games},
  type = {Technical {{Report}}}
}

@article{tsitsiklisAsynchronousStochasticApproximation1994,
  title = {Asynchronous {{Stochastic Approximation}} and {{Q}}-{{Learning}}},
  author = {Tsitsiklis, John N.},
  year = {1994},
  month = sep,
  volume = {16},
  pages = {185--202},
  issn = {1573-0565},
  doi = {10.1023/A:1022689125041},
  abstract = {We provide some general results on the convergence of a class of stochastic approximation algorithms and their parallel and asynchronous variants. We then use these results to study the Q-learning algorithm, a reinforcement learning method for solving Markov decision problems, and establish its convergence under conditions more general than previously available.},
  file = {/Users/kimangkhun/Zotero/storage/PXRHW2ZK/Tsitsiklis - 1994 - Asynchronous Stochastic Approximation and Q-Learni.pdf},
  journal = {Machine Learning},
  language = {en},
  number = {3}
}

@techreport{murphyConjugateBayesianAnalysis2007,
  title = {Conjugate Bayesian Analysis of the Gaussian Distribution},
  author = {Murphy, Kevin P.},
  year = {2007},
  abstract = {The Gaussian or normal distribution is one of the most widely used in statistics. Estimating its parameters using},
  file = {/Users/kimangkhun/Zotero/storage/FYURYFZY/Murphy - 2007 - Conjugate bayesian analysis of the gaussian distri.pdf;/Users/kimangkhun/Zotero/storage/GCE92292/summary.html}
}


@book{finkCompendiumConjugatePriors1997,
  title = {A {{Compendium}} of {{Conjugate Priors}}},
  author = {Fink, Daniel},
  year = {1997},
  abstract = {This report reviews conjugate priors and priors closed under sampling for a variety of data generating processes where the prior distributions are univariate, bivariate, and multivariate. The effects of transformations on conjugate prior relationships are considered and cases where conjugate prior relationships can be applied under transformations are identified. Univariate and bivariate prior relationships are verified using Monte Carlo methods. Contents 1},
  file = {/Users/kimangkhun/Zotero/storage/VXQ8RNQ3/Fink - 1997 - A Compendium of Conjugate Priors.pdf;/Users/kimangkhun/Zotero/storage/JKQLK3KV/summary.html}
}

@article{ouyang2017learning,
  title={Learning unknown markov decision processes: A thompson sampling approach},
  author={Ouyang, Yi and Gagrani, Mukul and Nayyar, Ashutosh and Jain, Rahul},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@inproceedings{filippi2010optimism,
  title={Optimism in reinforcement learning and Kullback-Leibler divergence},
  author={Filippi, Sarah and Capp{\'e}, Olivier and Garivier, Aur{\'e}lien},
  booktitle={2010 48th Annual Allerton Conference on Communication, Control, and Computing (Allerton)},
  pages={115--122},
  organization={IEEE}
}

@inproceedings{zanetteProblemDependentReinforcement2018,
  title = {Problem {{Dependent Reinforcement Learning Bounds Which Can Identify Bandit Structure}} in {{MDPs}}},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Zanette, Andrea and Brunskill, Emma},
  year = {2018},
  month = jul,
  pages = {5747--5755},
  publisher = {{PMLR}},
  issn = {2640-3498},
  abstract = {In order to make good decision under uncertainty an agent must learn from observations. To do so, two of the most common frameworks are Contextual Bandits and Markov Decision Processes (MDPs). In t...},
  file = {/Users/kimangkhun/Zotero/storage/NPQJUQ5S/Zanette and Brunskill - 2018 - Problem Dependent Reinforcement Learning Bounds Wh.pdf;/Users/kimangkhun/Zotero/storage/PV9RA8XZ/zanette18a.html},
  language = {en}
}

@article{jin2018q,
  title={Is Q-learning provably efficient?},
  author={Jin, Chi and Allen-Zhu, Zeyuan and Bubeck, Sebastien and Jordan, Michael I},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@inproceedings{osbandNearoptimalReinforcementLearning2014a,
  title = {Near-Optimal Reinforcement Learning in Factored {{MDPs}}},
  booktitle = {Proc. of the 27th {{Int. Conf.}} on {{Neural Information Processing Systems}} - {{Volume}} 1},
  author = {Osband, Ian and Roy, Benjamin Van},
  year = {2014},
  month = dec,
  pages = {604--612},
  publisher = {{MIT Press}},
  address = {{Montreal, Canada}},
  abstract = {Any reinforcement learning algorithm that applies to all Markov decision processes (MDPs) will suffer {$\Omega$}(SAT) regret on some MDP, where T is the elapsed time and S and A are the cardinalities of the state and action spaces. This implies T = {$\Omega$}(SA) time to guarantee a near-optimal policy. In many settings of practical interest, due to the curse of dimensionality, S and A can be so enormous that this learning time is unacceptable. We establish that, if the system is known to be a factored MDP, it is possible to achieve regret that scales polynomially in the number of parameters encoding the factored MDP, which may be exponentially smaller than S or A. We provide two algorithms that satisfy near-optimal regret bounds in this context: posterior sampling reinforcement learning PSRL) and an upper confidence bound algorithm (UCRL-Factored).},
  series = {{{NIPS}}'14}
}

@article{rosenbergOracleEfficientReinforcementLearning2020,
  title = {Oracle-{{Efficient Reinforcement Learning}} in {{Factored MDPs}} with {{Unknown Structure}}},
  author = {Rosenberg, Aviv and Mansour, Yishay},
  year = {2020},
  month = sep,
  abstract = {We consider provably-efficient reinforcement learning (RL) in non-episodic factored Markov decision processes (FMDPs). All previous algorithms for regret minimization in this setting made the strong assumption that the factored structure of the FMDP is known to the learner in advance. In this paper, we provide the first provably-efficient algorithm that has to learn the structure of the FMDP while minimizing its regret. Our algorithm is based on the optimism in face of uncertainty principle, combined with a simple statistical method for structure learning, and can be implemented efficiently given oracle-access to an FMDP planner. It maintains its computational efficiency even though the number of possible structures is exponential.},
  archivePrefix = {arXiv},
  eprint = {2009.05986},
  eprinttype = {arxiv},
  file = {/Users/kimangkhun/Zotero/storage/9TCLTSKT/Rosenberg and Mansour - 2020 - Oracle-Efficient Reinforcement Learning in Factore.pdf;/Users/kimangkhun/Zotero/storage/2M5VE6NA/2009.html},
  journal = {arXiv:2009.05986 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{xuReinforcementLearningFactored2020a,
  title = {Reinforcement {{Learning}} in {{Factored MDPs}}: {{Oracle}}-{{Efficient Algorithms}} and {{Tighter Regret Bounds}} for the {{Non}}-{{Episodic Setting}}},
  shorttitle = {Reinforcement {{Learning}} in {{Factored MDPs}}},
  author = {Xu, Ziping and Tewari, Ambuj},
  year = {2020},
  month = jun,
  abstract = {We study reinforcement learning in non-episodic factored Markov decision processes (FMDPs). We propose two near-optimal and oracle-efficient algorithms for FMDPs. Assuming oracle access to an FMDP planner, they enjoy a Bayesian and a frequentist regret bound respectively, both of which reduce to the near-optimal bound \$\textbackslash{}widetilde\{O\}(DS\textbackslash{}sqrt\{AT\})\$ for standard non-factored MDPs. We propose a tighter connectivity measure, factored span, for FMDPs and prove a lower bound that depends on the factored span rather than the diameter \$D\$. In order to decrease the gap between lower and upper bounds, we propose an adaptation of the REGAL.C algorithm whose regret bound depends on the factored span. Our oracle-efficient algorithms outperform previously proposed near-optimal algorithms on computer network administration simulations.},
  archivePrefix = {arXiv},
  eprint = {2002.02302},
  eprinttype = {arxiv},
  file = {/Users/kimangkhun/Zotero/storage/JKLTNJUC/Xu and Tewari - 2020 - Reinforcement Learning in Factored MDPs Oracle-Ef.pdf;/Users/kimangkhun/Zotero/storage/ZKMCPP2G/2002.html},
  journal = {arXiv:2002.02302 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{guestrinEfficientSolutionAlgorithms2003,
  title = {Efficient Solution Algorithms for Factored {{MDPs}}},
  author = {Guestrin, Carlos and Koller, Daphne and Parr, Ronald and Venkataraman, Shobha},
  year = {2003},
  month = oct,
  volume = {19},
  pages = {399--468},
  issn = {1076-9757},
  abstract = {This paper addresses the problem of planning under uncertainty in large Markov Decision Processes (MDPs). Factored MDPs represent a complex state space using state variables and the transition model using a dynamic Bayesian network. This representation often allows an exponential reduction in the representation size of structured MDPs, but the complexity of exact solution algorithms for such MDPs can grow exponentially in the representation size. In this paper, we present two approximate solution algorithms that exploit structure in factored MDPs. Both use an approximate value function represented as a linear combination of basis functions, where each basis function involves only a small subset of the domain variables. A key contribution of this paper is that it shows how the basic operations of both algorithms can be performed efficiently in closed form, by exploiting both additive and context-specific structure in a factored MDP. A central element of our algorithms is a novel linear program decomposition technique, analogous to variable elimination in Bayesian networks, which reduces an exponentially large LP to a provably equivalent, polynomial-sized one. One algorithm uses approximate linear programming, and the second approximate dynamic programming. Our dynamic programming algorithm is novel in that it uses an approximation based on max-norm, a technique that more directly minimizes the terms that appear in error bounds for approximate MDP algorithms. We provide experimental results on problems with over 1040 states, demonstrating a promising indication of the scalability of our approach, and compare our algorithm to an existing state-of-the-art approach, showing, in some problems, exponential gains in computation time.},
  journal = {Journal of Artificial Intelligence Research},
  number = {1}
}

@article{tianMinimaxOptimalReinforcement2020,
  title = {Towards {{Minimax Optimal Reinforcement Learning}} in {{Factored Markov Decision Processes}}},
  author = {Tian, Yi and Qian, Jian and Sra, Suvrit},
  year = {2020},
  month = jun,
  abstract = {We study minimax optimal reinforcement learning in episodic factored Markov decision processes (FMDPs), which are MDPs with conditionally independent transition components. Assuming the factorization is known, we propose two model-based algorithms. The first one achieves minimax optimal regret guarantees for a rich class of factored structures, while the second one enjoys better computational complexity with a slightly worse regret. A key new ingredient of our algorithms is the design of a bonus term to guide exploration. We complement our algorithms by presenting several structure-dependent lower bounds on regret for FMDPs that reveal the difficulty hiding in the intricacy of the structures.},
  archivePrefix = {arXiv},
  eprint = {2006.13405},
  eprinttype = {arxiv},
  file = {/Users/kimangkhun/Zotero/storage/7CWSKY4K/Tian et al. - 2020 - Towards Minimax Optimal Reinforcement Learning in .pdf;/Users/kimangkhun/Zotero/storage/PTWW7JVY/2006.html},
  journal = {arXiv:2006.13405 [cs, math, stat]},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  primaryClass = {cs, math, stat}
}

@online{top500,
    shorthand = {@top500},
    title = {TOP500 Website},
    url = {https://www.top500.org/},
    urldate = {2020-09-07}
}

@article{graham1969,
    author = {Graham, Ronald L.},
    journal = {SIAM journal on Applied Mathematics},
    number = {2},
    pages = {416--429},
    publisher = {SIAM},
    title = {Bounds on multiprocessing timing anomalies},
    volume = {17},
    year = {1969}
}

@techreport{corehour_co2,
    AUTHOR = "Berthoud, Francoise and Bzeznik, Bruno and Gibelin, Nicolas and Laurens, Myriam and Bonamy, Cyrille and Morel, Maxence and Schwindenhammer, Xavier",
    TITLE = "{Estimation de l'empreinte carbone d'une heure.coeur de calcul}",
    URL = "https://hal.archives-ouvertes.fr/hal-02549565",
    TYPE = "Research Report",
    INSTITUTION = "{UGA - Universit{\'e} Grenoble Alpes ; CNRS ; INP Grenoble ; INRIA}",
    YEAR = "2020",
    MONTH = "April",
    PDF = "https://hal.archives-ouvertes.fr/hal-02549565v4/file/GES-h-coeur-GRICAD-2020.pdf",
    HAL_ID = "hal-02549565",
    HAL_VERSION = "v4"
}
