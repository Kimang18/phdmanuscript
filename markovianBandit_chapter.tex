\chapter{Markovian Bandit Problem}
\label{ch:mb_problem}

In this chapter, we present two structured MDPs known as rested and restless markovian bandit.

\section{Rested Markovian bandit}
\label{sec:rested_mab_pb}

Motivating the problem:
\begin{itemize}
    \item a generalization of multi-armed bandit problem?
    \item give examples
\end{itemize}

\subsection{Problem formulation}
\label{subsec:rested_pb_formul}

\begin{itemize}
    \item explain a Markovian bandit: $n$ arms denote by $M$, Arm $i$ is $\langle\gS_i, \vr_i,\mP_i\rangle$
    \item state $\vx=(x_1\ \dots\ x_n)$ and $\vx_t=(x_{1,t}\ \dots\ x_{n,t})$
    \item reward $r(\vx, a)=r_a(x_a)=r(x_a)$
    \item transition $P(\vy \mid\vx,a)=P_a(x_a,y_a)=P(x_a,y_a)$
    \item objective function: maximize the value function $\max_{\pi}V^\pi_M$ 
\end{itemize}

\subsection{Gittins Index Policy}
\label{subsec:gittins_idx}

\subsubsection{Definition}

\subsubsection{Optimality}

\subsubsection{Value of Gittins index policy}

Do we need this section?

\subsubsection{Upper bound on the number of switching}

\begin{itemize}
    \item Prove the upper bound
    \item applicability: switching cost?
\end{itemize}

\subsection{Gittins index computation}

\subsubsection{Largest index remaining}

\begin{itemize}
    \item algorithm explanation
    \item complexity
    \item pitfall?
\end{itemize}

\subsubsection{State elimination}

\begin{itemize}
    \item algorithm explanation
    \item complexity
    \item pitfall?
\end{itemize}

\section{Restless Markovian bandit}
\label{sec:restless_mab_pb}

Motivating the problem:
\begin{itemize}
    \item a generalization of rested bandit problem?
    \item give examples
\end{itemize}

\subsection{Problem formulation}
\label{subsec:restless_pb_formul}

\begin{itemize}
    \item explain a restless bandit: $n$ arms denote by $M$, Arm $i$ is $\langle\gS_i, \{0,1\}, r_i,P_i\rangle$
    \item state $\vx=(x_1\ \dots\ x_n)$ and $\vx_t=(x_{1,t}\ \dots\ x_{n,t})$
    \item action $\va=(a_1\ \dots\ a_n)$ such that $\sum_{i=1}^{n}a_i =m$
    \item reward $r(\vx, \va)=\sum_{i=1}^n r_i(x_i,a_i) =\sum_{i=1}^n r(x_i,a_i)$
    \item transition $P(\vy \mid\vx,a)=\prod_{i=1}^nP_i(y_i \mid x_i,a_i)=\prod_{i=1}^n P(y_i \mid x_i,a_i)$
    \item objective function: maximize the average reward $\max_{\pi}g^\pi_M$ 
\end{itemize}


\subsection{Whittle Index Policy}
\label{subsec:whittle_idx}

\subsubsection{Definition}

Lagrangian multipliers

\subsubsection{Optimality}

\subsection{Whittle index computation}

\subsubsection{Conditions for indexability...}

\subsubsection{Fast-pivoting algorithm}

