A Markovian bandit is a sequential decision problem in which the decision maker has to activate a set of bandit's arms at each time, and the active arms evolve in a Markovian manner.
%A multi-armed bandit is a sequential decision problem where one has to activate a set of bandit's arms at each time.
%It is called ``Markovian bandit'' when its arms evolve in a Markovian manner.
%A multi-armed bandit problem is a sequential allocation problem of a finite set of resources over its arms.
%The activated arms evolve in an active Markovian manner.
%The arms that are not activated either remain frozen -- then one falls into the category of \emph{rested} Markovian bandits -- or evolve in a passive Markovian manner -- the setting is then called \emph{restless} Markovian bandit.
%Each arm generates a reward depending on its state and the active or passive evolution.
%The decision maker wants to maximize its cumulative reward over an infinite horizon of time.
%The arms that are not activated either remain frozen -- then one falls into the category of \emph{rested} Markovian bandits -- or evolve in a passive Markovian manner -- the setting is then called \emph{restless} Markovian bandit.
There are two types of Markovian bandits: (i) \emph{rested} bandits, where the arms that are not activated remain frozen, and (ii) \emph{restless} bandits, where the arms that are not activated evolve in a Markovian manner.
In general, Markovian bandits suffer from the curse of dimensionality that often makes the exact solution computationally intractable.
So, one has to resort to tractable heuristics such as index policies.
Two celebrated indices are the Gittins index for rested bandits and the Whittle index for restless bandits.

This thesis focuses on two questions (1) index computation when all model parameters are known and (2) learning algorithms when the parameters are unknown.

%For index computation, we first cover the ambiguities in the classical condition that guarantees the existence of the Whittle index in restless bandits.
For index computation, we point out the ambiguities in the classical indexability definition and propose a definition that assures the uniqueness of the Whittle index when this latter exists. %in restless bandit arms.
%We then introduce a new univocal definition of indexability that assures the uniqueness of the Whittle index when it exists.
%We then refine this condition to assure the uniqueness of the index when it exists.
We then develop an algorithm for testing the indexability and computing the Whittle indices of a restless arm.
%This algorithm can also compute the Gittins index.
%Our algorithm is built on three tools: (1) a careful characterization of Whittle index that allows one to recursively compute the $k$th smallest index from the $(k-1)$th smallest, and to test indexability, (2) the use of the Sherman-Morrison formula to make this recursive computation efficient, and (3) a sporadic use of the fastest matrix inversion and multiplication methods to obtain a subcubic complexity.
%We show that an efficient use of the Sherman-Morrison formula leads to an algorithm that computes Whittle index in $(2/3)S^3+o(S^3)$ arithmetic operations, where $S$ is the number of states of the arm.
%Its index computation is done in $(2/3)S^3+o(S^3)$ arithmetic operations, where $S$ is the number of states of the arm.
%The careful use of fast matrix multiplication leads to the first subcubic algorithm to compute Whittle or Gittins index: By using the current fastest matrix multiplication, the theoretical complexity of our algorithm is $\landauO(S^{2.5286})$.
The theoretical complexity of our algorithm is $\landauO(S^{2.5286})$, where $S$ is the number of arm's states.
%We also develop an efficient implementation of this algorithm in python programming language.
%It can compute indices of restless arms with several thousands of states in less than a few seconds.

%For learning setup, we divide our work into two phases: (1) design algorithms with learning performance guarantee in rested Markovian bandits and (2) study the challenges when learning in restless Markovian bandit.
%For rested bandits, Gittins index policy has been proven to be optimal when exactly one arm is activated at each decision and there is a discount on reward.
%For learning in rested bandits, we show that MB-PSRL and MB-UCBVI, modified versions of PSRL and UCBVI algorithms, can leverage Gittins index policy to have a regret guarantee and a runtime scalable in the number of arms. Furthermore, we show that MB-UCRL2, a modified version of UCRL2, also has a regret guarantee scalable in the number of arms. However, MB-UCRL2 has a runtime exponential in the number of arms.
For learning in rested bandits, we propose modifications of PSRL and UCBVI algorithms that we call MB-PSRL and MB-UCBVI.
We show that they can leverage Gittins index policy to have a regret guarantee and a runtime scalable in the number of arms.
Furthermore, we show that MB-UCRL2, a modification of UCRL2, also has a regret guarantee scalable in the number of arms.
However, MB-UCRL2 has a runtime exponential in the number of arms.
When learning in restless bandits, the regret guarantee depends heavily on the structure of the bandit.
We study how the structure of arms translates into the structure of the bandit.
%We show that no learning algorithms can perform uniformly well over the general class of restless bandits whose arms are unichain.
We exhibit a subclass of restless bandits that is not learnable.
We also show that it is difficult to construct a subclass of restless bandits with a desirable learning structure by only making assumptions about arms.
%in which each arm has an internal state that evolves in a Markovian manner depending on the decision maker's actions. 
