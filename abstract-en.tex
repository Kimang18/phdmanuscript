Markovian bandits are a subclass of multi-armed bandit problems where a decision maker has to activate a set of arms at each decision time.
The activated arms evolve in an active Markovian manner.
Those that are not activated (\ie, are passive) either remain frozen -- then one falls into the category of rested Markovian bandits -- or evolve in a passive Markovian manner -- the setting is then called restless Markovian bandit.
Each arm generates a reward depending on its state and the active or passive evolution.
The decision maker wants to maximize its cumulative reward over an infinite horizon of time.
Such problems suffer from the curse of dimensionality that often makes the exact solution computationally prohibitive.
So, one has to resort to heuristics such as index policy.
Two celebrated index definitions are Gittins index for rested bandits and Whittle index for restless bandits.

In this thesis, we focus on two setups: (1) index computation when all model parameters are known and (2) learning algorithm design when the parameters are unknown.

For index computation, we first cover the ambiguities in the classical condition known as the indexability that guarantees the existence of the Whittle index in restless bandits.
We then introduce a new univocal definition of indexability that assures the uniqueness of the Whittle index when it exists.
We then develop an algorithm to test such the indexability and compute the Whittle indices of any finite-state restless bandit arm.
This algorithm can also compute the Gittins index.
Our algorithm is built on three tools: (1) a careful characterization of Whittle index that allows one to recursively compute the $k$th smallest index from the $(k − 1)$th smallest, and to test indexability, (2) the use of the Sherman-Morrison formula to make this recursive computation efficient, and (3) a sporadic use of the fastest matrix inversion and multiplication methods to obtain a subcubic complexity. We show that an efficient use of the Sherman-Morrison formula leads to an algorithm that computes Whittle index in $(2/3)S^3 + o(S^3)$ arithmetic operations, where $S$ is the number of states of the arm. The careful use of fast matrix multiplication leads to the first subcubic algorithm to compute Whittle or Gittins index: By using the current fastest matrix multiplication, the theoretical complexity of our algorithm is $\landauO(S^{2.5286})$. We also develop an efficient implementation of our algorithm in \texttt{python} programming language. It can compute indices of Markov chains with several thousands of states in less than a few seconds.

For learning setup, we divide our work into two phases: (1) design algorithms with learning performance guarantee in rested Markovian bandits and (2) study the challenges when learning in restless Markovian bandit.
For rested bandits, Gittins index policy has been proven to be optimal when exactly one arm is activated at each decision and there is a discount on reward.
We show that MB-PSRL and MB-UCBVI, respectively the modified versions of PSRL and UCBVI, can leverage Gittins index policy to have a regret bound, which is a bound on the learning performance, and a runtime scalable in the number of arms. We also show that MB-UCRL2, a modified version of UCRL2, also has a regret bound scalable in the number of arms. Yet, we give an example showing that MB-UCRL2 and any modification of UCRL2’s variants to rested bandit likely have a runtime exponential in the number of arms.
For learning in restless bandit with long-term average reward criterion, the regret of learning algorithms depends heavily on the structure of the restless bandit.
So, we study how the structure of arms translate in the structure of the bandit. We provide a few examples showing that no learning algorithms can perform uniformly well over the general class of restless bandits.
Our examples also show that defining a subclass of restless Markovian bandits that have desirable structure for learning by relying on the assumption on arms is difficult.

%in which each arm has an internal state that evolves in a Markovian manner depending on the decision maker's actions. 
