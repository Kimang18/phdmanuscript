
%\dominitoc
%\faketableofcontents
\chapter{Markov Decision Process}
\label{ch:mdp}

%\minitoc

In this chapter, we define the notion of Markov decision process (MDP) used to mathematically describe the environment in Reinforcement learning (RL) framework.
A MDP models a discrete-time decision problem where the decision maker executes available ``action'' over time steps and receives immediate incentive known as ``reward'' for each time step.
In such problem, the decision maker seeks to maximize the expected cumulative rewards by identifying a sequence of actions that produces such effect.

In Section~\ref{ch:mdp:sec:defn}, we describe...

%\let\clearpage\relax

\section{Definitions and notations}
\label{ch:mdp:sec:defn}

In this section, we give the \emph{formalism} of Markov decision process and present several notations of \emph{optimality}.
We also lay out existing results (see \cite{puterman2014markov}) that are pillar of our contribution.
We essentially follow the notations of \cite{puterman2014markov}.

A Markov decision process $M$ is defined as a $4$-tuple $M:=\langle\gS, \gA, r, p\rangle$.
$\gS$ and $\gA:=\cup_{s\in\gS}\gA_s$ denote the \emph{state} and \emph{action} space of the MDP.
When the MDP is in state $s\in\gS$, the decision maker can execute one of the available actions in $\gA_s$.
As a result of executing action $a\in\gA_s$ when the MDP is in state $s$, the MDP incurs a random \emph{reward} with \emph{expected value} $r(s,a)$ and then transitions to new state $s'\in\gS$ with probability $p(s'\mid s, a)\in[0,1]$ where $\sum_{s'\in\gS}p(s'\mid s,a)=1$.
The name \emph{Markov} comes from the fact that the random reward and the next state depend only on state $s$ and action $a$.
In this thesis, we consider the MDPs with \emph{finite} state and action spaces, $S=\abs{\gS}, \abs{\gA}\in\sN$.
Also, we assume that all random rewards are bounded and lie in $[0,1]$.

\begin{enumerate}
    \item explain a MDP: $M:=\langle\gS, \gA, r, P\rangle$, explain $\gS, \gA, r, P$, give example
    \item explain policy $\pi$: shall we go to detail?
        \begin{itemize}
            \item History dependent?
            \item Stationary? Probabilistic + deterministic?
        \end{itemize}
    \item induced stochastic process
        \begin{itemize}
            \item shall we give definition of recurrent states?
            \item shall we give definition of transient states?
        \end{itemize}
    \item MDP classification
\end{enumerate}

\section{Infinite horizon criteria}

We consider infinite time horizon:
\begin{itemize}
    \item explain what time horizon is
    \item explain decision epoch is, how it works
\end{itemize}

\subsection{Dicounted markov decision process}

The reward is discounted with discount factor $\beta$.

\begin{itemize}
    \item give assumptions: reward bounded, $\beta<1$
\end{itemize}

\subsubsection{Definition and problem formulation}

\begin{itemize}
    \item explain what is Value function $V^\pi_M$
    \item explain why value function?
    \item do we need Action value function $Q^\pi_M$ ?
    \item why action value function?
\end{itemize}

\subsubsection{Optimality equation}

\begin{itemize}
    \item explain Bellman equation
    \item explain evaluation equation 
    \item shall we explain contraction property?
    \item shall we explain bound on span?
    \item shall we give unicity of optimal policy?
\end{itemize}

\subsubsection{Value Iteration}

Shall we present Value Iteration?

\begin{itemize}
    \item Algorithm presentation
    \item Convergence proof
    \item shall we give example?
\end{itemize}

\subsection{Average reward criterion}

\subsubsection{Definition and problem formulation}

\begin{itemize}
    \item give the assumptions needed: bounded reward,...
    \item explain what is the average reward $g^\pi$
    \item explain what is bias $h^\pi$
    \item shall we explain existency of $g^\pi, h^\pi$
    \item shall we give example?
    \item define gain optimal, Bellman optimal, (also bias optimal?)
\end{itemize}

\subsubsection{Optimality equation}

\begin{itemize}
    \item explain Bellman equation
    \item explain evaluation equation
    \item explain the space of solutions of Bellman equation
    \item give the condition for unicity of Bellman optimal policy
\end{itemize}

\subsubsection{Relative Value Iteration}

Shall we present Relative Value Iteration

\begin{itemize}
    \item Algorithm presentation
    \item Convergence proof
    \item shall we give example?
\end{itemize}
