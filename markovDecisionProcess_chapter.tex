\begingroup
%\dominitoc
%\faketableofcontents

\let\clearpage\relax

\chapter{Markov Decision Process}
\label{ch:mdp}

%\minitoc

In this chapter, we define the notion of Markov decision process (MDP) which is a generic model to solve Markovian bandit problem that we will describe in Chapter~\ref{ch:mb_problem}.
MDP is also used to mathematically describe the environment in Reinforcement learning (RL) framework.
A MDP models a discrete-time decision problem where the decision maker executes available ``action'' over time steps and receives immediate incentive known as ``reward'' for each time step.
In such problem, the decision maker seeks to maximize the expected cumulative rewards by identifying a sequence of actions that produces such effect.

The reader who is familiar with MDP can go directly to Section~\ref{ch:mdp:sec:bell} which is a pillar of our contribution in Chapter~\ref{ch:index_computation}.
In Section~\ref{ch:mdp:sec:defn}, we lay out the notation about MDP's parameter and the dynamic of the decision process.
In Section~\ref{ch:mdp:sec:finite}, we give a brief presentation of finite horizon setting.
Similarly, we present classical settings for infinite horizon in Sections~\ref{ch:mdp:sec:discounted} and \ref{ch:mdp:sec:gain}.
Finally, we introduce a new notion of optimality for infinite horizon setting in Section~\ref{ch:mdp:sec:bell}.


\section{Definitions and notations}
\label{ch:mdp:sec:defn}

In this section, we give the \emph{formalism} of Markov decision process.% and present several notations of \emph{optimality}.
%We also lay out existing results (see \cite{puterman2014markov}) that are pillar of our contribution.
We essentially follow the notations of \cite{puterman2014markov}.

\subsection{State, action, reward, and state transition}

A Markov decision process $M$ is defined as a $4$-tuple $M:=\langle\gS, \gA, r, p\rangle$.
$\gS$ and $\gA:=\cup_{s\in\gS}\gA_s$ denote the \emph{state} and \emph{action} space of the MDP.
When the MDP is in state $s\in\gS$, the decision maker can execute one of the available actions in $\gA_s$.
As a result of executing action $a\in\gA_s$ in state $s$, the MDP incurs a random \emph{reward} with \emph{expected value} $r(s,a)$ and then transitions to new state $s'\in\gS$ with probability $p(s'\mid s, a)\in[0,1]$ where $\sum_{s'\in\gS}p(s'\mid s,a)=1$.
The name \emph{Markov} comes from the fact that the random reward and the next state depend only on state $s$ and action $a$ and are independent from anything else.
In this thesis, we consider the MDPs with \emph{finite} state and action spaces, $\abs{\gS}, \abs{\gA}\in\sN$.
%Also, we assume that all random rewards are bounded and lie in $[0,1]$.
%We provide an example of a MDP with two states and two actions per state in \figurename~\ref{fig:MDP_example}.

%\begin{figure}[ht]
%    \centering
%    \begin{tikzpicture}[on grid, state/.style={circle,draw}, >= stealth', auto, prob/.style = {inner sep=1pt,font=\scriptsize}]
%        \node[state,color=blue]  (A) {$s_1$};
%        \node[state,color=blue]  (B) [left =3cm of A]   {$s_0$};
%        \path[->]
%            (A) edge[loop above, color=black] node{$a_1$} (A)
%            (A) edge[loop right, color=red, dashed] node{$a_0$} (A)
%            (B) edge[bend left, color=black] node{$a_1$} (A)
%            (B) edge[bend right, color=red, dashed] node[below]{$a_0$} (A);
%    \end{tikzpicture}
%    \caption{Graphical representation of a MDP with $2$ states ($s_0$ and $s_1$) and 2 actions per state (black arrow and red dashed arrow). We have $p(s_0\mid s_0,a_0)=0, p(s_1\mid s_0,a_0)=1$ and so on.}
%    \label{fig:MDP_example}
%\end{figure}

\subsection{Sequential decision problem and policy}

In this thesis, the decision maker executes actions at \emph{discrete} time steps.
We denote the decision time by $t\in\N^+$ where $\N^+:=\N\setminus\{0\}$ is the set of positive natural numbers.
At time step $t\ge1$, the MDP is in state $s_t$ and the decision maker executes an action $a_t$.
The MDP incurs a (random) reward denoted by $r_t$ and transitions to next state denoted by $s_{t+1}$.
This mechanism is repeated and one obtains a sequence of the form $\{s_1,a_1,r_1,s_2,\dots,s_t,a_t,r_t,s_{t+1},\dots\}$ that is called \emph{observations} (also known as a ``trajectory'').
%We denote the observations up to time $t\ge1$ by
%\begin{align*}
%    O_t:=\{s_1,a_1,r_1,s_2,\dots,s_{t-1},a_{t-1},r_{t-1},s_t\}
%\end{align*}
%with a convention that $O_1:=\{s_1\}$.
%A \emph{deterministic decision rule} $d$ is a function that maps the observations up to time $t$ to available actions at decision time $t$ with certainty, $d(O_t)\in\gA$.
%The decision rules that depend on previous observations $\gO_t$ only through the current state $s_t$ of the MDP are called \emph{Markovian} decision rule, $d:\gS\mapsto\gA$.

%In this thesis, we restrict our attention to \emph{deterministic Markovian decision rule} and a policy $\pi:=\{d_1,d_2,\dots\}$ is a sequence of deterministic Markovian decision rules for each time step such that at time $t\ge1$, the decision maker follows policy $\pi$ by executing the action $d_t(s_t)\in\gA$. 
In this thesis, we define a \emph{deterministic decision rule} $\pi$ to be a mapping function from state space $\gS$ to action space $\gA$ where for each $s\in\gS$, $\pi(s)\in\gA_s$.
A \emph{policy} is a sequence of deterministic decision rules $\{\pi_1,\pi_2,\dots\}$ such that at time step $t\ge1$, the decision maker follows the policy by executing the action $\pi_t(s_t)\in\gA$.
With a slight abuse of notation, we also denote a policy by $\pi=\{\pi_1,\pi_2,\dots\}$.
We denote the set of all policies by $\Pi$.
%A \emph{stationary} policy is a sequence of decision rules $\{d_t\}_{t\ge1}$ where $d_t=d$ for all $t\ge1$ and $d:\gS\mapsto\gA$ is a mapping function from state space to action space.
We say that a policy is \emph{stationary} when the decision rules are invariant over time step, $\pi_t=\pi$ for all $t\ge1$ and $\pi:\gS\mapsto\gA$.
%By abuse of notation, we will write $d$ and $\pi$ \emph{interchangeably} in the rest of the thesis when policy $\pi=\{d, d,...\}$ is stationary.

The state of the MDP at time step $1$ is called \emph{initial state} and generally drawn from a probability distribution $\rho$ such that $\sum_{s\in\gS}\rho(s)=1$.
Given the initial state $s_1$, a policy $\pi\in\Pi$ incurs a sequence $\{s_1,a_1,r_1,\dots,s_t,a_t,r_t,\dots\}$ which is a \emph{stochastic process} with a well-defined probability distribution \cite[Section~2.1.6]{puterman2014markov}.
We will denote by $\sP^\pi(\cdot\mid s_1)$ the \emph{probability measure} associated with this stochastic process and denote by $\mathbb{E}^\pi[\cdot \mid s_1]$ the corresponding \emph{expectation}.

In the next sections, we specify the objective function of the decision problem.

%The policies that vary over time steps are called \emph{non-stationary} policies denoted by $\pi_t$ where $\pi_t(\gO_t)\in\gA_{s_t}$.
%The policies that do not vary over time steps are known as \emph{stationary} (or ``time-homogeneous'') policies.
%The time-dependent or not depending on the objective function of the decision problem.
%We talk more about this in the following sections.

\section{Finite horizon problems}
\label{ch:mdp:sec:finite}

This section describes the \emph{finite horizon setting} and contains a brief presentation of optimality criterion in the setting.

In finite horizon setting, the decision maker can collect rewards from a MDP $M$ over a \emph{fixed} number of time steps called \emph{horizon} and denoted by $H\in\N^+$.
We call the expected cumulative reward when following a policy $\pi$ as the \emph{value function}.
That is, the value of state $s$ when starting at time step $1\le h\le H$ and following policy $\pi$ is given by $w_{h:H}^\pi(s){:=}\E^\pi\Bigl[\sum_{t=h}^H r_t\mid s_h{=}s\Bigr]$.
Formally, the decision maker want to find a policy that verifies
\begin{equation}
    \label{eq:finite_obj}
    \sup_{\pi\in\Pi}\sum_{s\in\gS}\rho(s)w_{1:H}^\pi(s).
\end{equation}
%From \cite[Chapter~4]{puterman2014markov}, there always exists a sequence of \emph{optimal} policies $\{\pi_1^*,\pi_2^*,\dots,\pi^*_H\}$ that maximizes \Eqref{eq:finite_obj} for any $\rho$ and such that for all $1\le t\le H$, $d_t^*$ is a deterministic Markovian decision rule and independent of the initial state distribution.
From \cite[Chapter~4]{puterman2014markov}, there always exists an \emph{optimal} policy $\pi^*=\{\pi_1^*,\pi_2^*,\dots,\pi^*_H\}$ that maximizes \Eqref{eq:finite_obj} for any $\rho$ and such that for all $1\le t\le H$, $\pi_t^*:\gS\mapsto\gA$ is a deterministic decision rule and independent of the initial state distribution.
For each state $s$, we denote the \emph{optimal} expected cumulative reward from $s$ over time steps $h$ to $H$ by $w_{h:H}^{*}(s){:=}\max_{\pi\in\Pi}w_{h:H}^\pi(s)$.
The maximum value of \Eqref{eq:finite_obj} is given by $\sum_{s\in\gS}\rho(s)w_{1:H}^*(s)$.

From \cite[Chapter~4]{puterman2014markov}, the Bellman \emph{optimality} equations in this setting are written: for $1\le h\le H-1$ and $s\in\gS$,
\begin{equation}
    \label{eq:finite_be_opt}
    w_{h:H}^{*}(s) = \max_{a\in\gA_s}\Big(r(s,a)+\sum_{s'\in\gS}p(s'\mid s,a)w_{h+1:H}^{*}(s')\Big)
\end{equation}
and $w_{H:H}^{*}(s) =\max_{a\in\gA_s}r(s,a)$.
%For a given policy $\pi=\{d_1,\dots,d_H\}$, for each state $s$, we define the expected cumulative reward from $s$ over time steps $h$ to $H$ when following policy $\pi$ by $W_{M,h:H}^\pi(s){:=}\E^\pi\left[\sum_{t=h}^H r_t\mid s_h{=}s\right]$.
%The Bellman \emph{optimality} equations in this setting are written: for $1\le h\le H-1$,
%\begin{equation}
%    \label{eq:finite_be_opt}
%    W_{M,h:H}^{\pi^*}(s) = \max_{a\in\gA_s}\p{r(s,a)+\sum_{s'\in\gS}p(s'\mid s,a)W_{M,h+1:H}^{\pi^*}(s')}
%\end{equation}
%and $W_{M,H:H}^{\pi^*}(s) =\max_{a\in\gA_s}r(s,a)$.
%The function $W_{1:H}$ is the expected cumulative reward from time step $1$ to $H$.
%Suppose that at time step $1\le h\le H$, a policy $\pi$ executes action $a=d_h(s)$ in state $s$ where $a\in\gA_s$ .
%Then, the Bellman \emph{evaluation} equation in this setting is
%\begin{equation}
%    \label{eq:finite_be_eval}
%    W_{M,h}^{\pi}(s) = r(s,a)+\sum_{s'\in\gS}p(s'\mid s,a)W_{M,h+1}^{\pi}(s')
%\end{equation}
%with a convention that $W_{H+1}=0$.
So an optimal policy $\pi^*$ can be constructed using backward induction on \Eqref{eq:finite_be_opt}: for each state $s$
\begin{itemize}
    \item $\pi^*_H(s)=\argmax_{a\in\gA_s}r(s,a)$;
    \item for $h$ from $H-1$ to $1$, $\pi^*_h(s)=\argmax_{a\in\gA_s}\Big(r(s,a)+\sum_{s'\in\gS}p(s'\mid s,a)w_{h+1:H}^{*}(s')\Big)$
\end{itemize}
where the ties are broken arbitrarily.
%\begin{algorithm}[ht]
%    \DontPrintSemicolon
%    \SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
%    \Input{a MDP $M=\langle\gS,\gA,r,p\rangle$ and horizon $H$}
%    \Init{}{Set $u_{H+1}(s)=0$ for all $s\in\gS$}
%    \BlankLine
%    \For{$h=H$ to $1$}{
%        Set $d_h^*(s) := \displaystyle\argmax_{a\in\gA_s}\Big(r(s,a)+\sum_{s'\in\gS}p(s'\mid s,a)u_{h+1}(s')\Big)$ \tcp{ties are broken arbitrarily} \;
%        Set $u_{h}(s) = \displaystyle r\big(s, d_h^*(s)\big) +\sum_{s'\in\gS}p\big(s' \mid s, d_h^*(s)\big)u_{h+1}(s). \label{eq:finite_be_eval2}$
%    }
%    \Return $\pi^*=\{d_1^*,\dots,d_H^*\}$ and $\vu_1$
%    \caption{Backward Induction}
%    \label{algo:backward}
%\end{algorithm}
%By \cite[Theorem~4.5.1]{puterman2014markov}, Algorithm~\ref{algo:backward} computes an optimal policy and the maximum value of \Eqref{eq:finite_obj} is given by $\sum_{s\in\gS}\rho(s)u_{1}(s)$ where $\vu_1$ is the second output of the algorithm.
%Note that the last part of Equations~\eqref{eq:finite_be_eval1} and \eqref{eq:finite_be_eval2} are known as Bellman \emph{evaluation} equations.

%The backward induction has $\landauO(H\abs{\gS}^2\abs{\gA})$ computational complexity because for each time step, state, and action, the inner product between $p$ and $\vu$ costs $\abs{\gS}$ multiplications at worst.

%\KK{Add Bellman evaluation equations}

Finally, from \cite[Chapter~4]{puterman2014markov} the value function also satisfies the Bellman \emph{evaluation} equations: given policy $\pi$, for $1\le h\le H$ and $s\in\gS$,
\begin{equation}
    \label{eq:finite_be_eval2}
    w_{h:H}^\pi(s) = \displaystyle r\Big(s, \pi_h(s)\Big) +\sum_{s'\in\gS}p\Bigl(s' \mid s, \pi_h(s)\Bigr)w_{h+1:H}^\pi(s')
\end{equation}
with $w_{H+1:H}^\pi(s)=0$ (or in matrix notation $\vw_{H+1:H}^\pi=\vzero$).
%\section{Infinite horizon problems}
%
%Similar the previous section, we describe two settings for infinite horizon and provide detail about the optimality criteria in this section.

\section{Infinite horizon discounted Markov decision process}
\label{ch:mdp:sec:discounted}

In some problems, the decision maker can collect rewards over an infinite number of time steps and the immediate reward incurred by the MDP is more important then those rewards in the future time steps.
To capture this aspect, one can introduce a discount factor denoted by $\gamma$ where $\gamma\in[0,1)$.
If $\gamma=0$, the decision maker is solely interested in the immediate reward from the current state of the MDP.
The value of state $s$ when following a policy $\pi$ is defined by $v^\pi_{\gamma}(s):=\E^\pi\Bigl[\sum_{t=1}^{+\infty} \gamma^{t-1}r_t \mid s_1=s\Bigr]$.
It captures the cumulative discounted reward one would expect when following policy $\pi$ starting from state $s$.
The decision maker wants to find a policy that verifies
\begin{equation}
    \label{eq:discount_obj}
    \sup_{\pi\in\Pi}\sum_{s\in\gS}\rho(s)v^\pi_{\gamma}(s).
\end{equation}
From \cite[Chapter~6]{puterman2014markov}, there always exists a stationary optimal policy $\pi^*:\gS\mapsto\gA$ that maximizes \Eqref{eq:discount_obj} for any initial distribution $\rho$.
%For a given stationary policy $\pi:\gS\mapsto\gA$, the expectation in \Eqref{eq:discount_obj} is known as the \emph{value} of state $s$ under policy $\pi$ and denoted by $v^\pi(s)$.
For any $s$, since $0\le\gamma<1$, $v^\pi(s)$ is a geometric series that converges.
So, $v^\pi_\gamma(s)$ exists and well-defined for any state $s$ and any policy $\pi\in\Pi$.
The value function of policy $\pi$ satisfies the following Bellman evaluation equations: for each $s\in\gS$
\begin{equation}
    \label{eq:discount_be_eval}
    v^\pi_\gamma(s) =r\bigl(s,\pi(s)\bigr) +\gamma\sum_{s'\in\gS}p\bigl(s'\mid s,\pi(s)\bigr)v^\pi_\gamma(s').
\end{equation}
The \emph{optimal} value function is defined by $v^*_\gamma(s):=\max_{\pi\in\Pi}v^\pi_\gamma(s)$ for all $s\in\gS$.
By \cite[Theorem~6.2.5]{puterman2014markov}, the optimal value function $\vv^*_\gamma\in\R^{\abs{\gS}}$ is unique: any optimal policies induce the same value function.
Moreover, $\vv^*_\gamma$ satisfies the Bellman optimality equations: for each $s\in\gS$
\begin{equation}
    \label{eq:discount_be_opt}
     v^*_\gamma(s)= \max_{a\in\gA_s}\Big(r(s, a) +\gamma\sum_{s'\in\gS}p(s'\mid s,a)v^*_\gamma(s')\Big).
\end{equation}
So, the maximum value of \Eqref{eq:discount_obj} is given by $\sum_{s\in\gS}\rho(s)v^*_\gamma(s)$.
%It is well-known that an optimal policy can be constructed using iterative algorithms such as Value Iteration given in Algorithm~\ref{algo:discounted_vi}.

It is well-known that an optimal policy can be constructed as the following: for all state $s$, $\pi^*(s)=\argmax_{a\in\gA_s}\Big(r(s, a) +\gamma\sum_{s'\in\gS}p(s'\mid s,a)v^*_\gamma(s')\Big)$ where the ties are broken arbitrarily.
This requires the optimal value function $\vv^*_\gamma$ which is approximated by iterative algorithms such as Value Iteration:
\begin{itemize}
    \item initialize $v_0(s)=0$ for all $s\in\gS$
    \item iterate over $k$: $v^{k+1}_\gamma(s) = \max_{a\in\gA_s}\Big(r(s,a)+\gamma\sum_{s'\in\gS}p(s'\mid s,a)v_{\gamma}^k(s')\Big)$ for all $s\in\gS$, then update $k=k+1$ and repeat until $\norm{\vv^{k+1}_\gamma-\vv^k_\gamma}\le(1-\gamma)\varepsilon/\gamma$
\end{itemize}
where $\norm{\cdot}$ can be the maximum norm or span semi-norm on $\R^{\abs{\gS}}$ and $\varepsilon$ is the accuracy of the approximation.
From \cite[Chapter~6]{puterman2014markov}, this iterative schema always converges: $\lim_{k\to\infty}\vv^k_\gamma=\vv^*_\gamma$.
That is, this schema stops after a finite number of iterations and we get $\norm{\vv^k_\gamma-\vv^*_\gamma}\le \varepsilon$ (see \cite[Theorem~6.3.1]{puterman2014markov}). 
%\begin{algorithm}[ht]
%    \DontPrintSemicolon
%    \SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
%    \Input{a MDP $M=\langle\gS,\gA,r,p\rangle$, discount factor $\gamma\in[0,1)$, and accuracy $\varepsilon\in(0,1)$}
%    \Init{}{Set $n=0$ and $v_0(s)=0$ for all $s\in\gS$}
%    \BlankLine
%    Set $v_1(s)=\max_{a\in\gA_s} r(s,a)$ for all $s\in\gS$ \;
%    \While{$\displaystyle\max_{s\in\gS}\big(v_{n+1}(s)-v_n(s)\big) -\min_{s\in\gS}\big(v_{n+1}(s)-v_n(s)\big) \ge \frac{1-\gamma}{\gamma}\varepsilon$}{
%        Set $n=n+1$ \;
%        Set $v_{n+1}(s) = \displaystyle\max_{a\in\gA_s}\Big(r(s,a)+\sum_{s'\in\gS}p(s'\mid s,a)v_{n}(s')\Big)$ for all $s\in\gS$
%    }
%    Set $\pi(s):=\displaystyle\argmax_{a\in\gA_s}\Big(r(s,a)+\sum_{s'\in\gS}p(s'\mid s,a)v_{n+1}(s')\Big)$ for all $s\in\gS$ \tcp{ties are broken arbitrarily}\;
%    \Return $\pi$ and $\vv_{n+1}$
%    \caption{Value Iteration}
%    \label{algo:discounted_vi}
%\end{algorithm}
%
%Similar to backward induction, Algorithm~\ref{algo:discounted_vi} has $\landauO(\abs{\gS}^2\abs{\gA})$ computational complexity where $\landauO(\cdot)$ hides the total number of iterations.

This setting with discount factor $\gamma$ can be seen as a finite horizon setting whose horizon is randomly sampled from \emph{geometric distribution} with parameter $(1-\gamma)$.
That is, at each time step, the decision maker can stop interacting with the MDP with probability $(1-\gamma)$.
So, the value of state $s$ under stationary policy $\pi$ is the expected sum of rewards over the random horizon:
\begin{equation}
    \label{eq:discount_finite}
    v^\pi_\gamma(s) :=\E^\pi\left[\sum_{t=1}^{+\infty} \gamma^{t-1}r_t \mid s_1=s\right] =\ex{\E^\pi\bigg[\sum_{t=1}^H r_t\mid H, s_1=s\bigg]} =\ex{w^\pi_{1:H}(s)}
\end{equation}
where the expectation of the fourth term integrates over the randomness of the horizon $H$ whose expected value is $1/(1-\gamma)$.

%\subsection{Average reward model}
\section{Infinite horizon average reward criterion}
\label{ch:mdp:sec:gain}

In some problems, the decision maker is more interested in the average reward per decision time than the expected cumulative discounted reward.
We focus on such criterion in this section.

\subsection{Gain and bias}

Formally, the decision maker wants to identify a policy that verifies
\begin{equation}
    \label{eq:avg_obj}
    \sup_{\pi\in\Pi}\sum_{s\in\gS}\rho(s)\liminf_{T\to+\infty}\frac1T \E^\pi\left[ \sum_{t=1}^{T} r_t \mid s_1=s\right].
\end{equation}
Since the state space is finite, the infimum limit of \Eqref{eq:avg_obj} equals the supremum limit for any stationary policy $\pi$ (see \cite[Chapter~8]{puterman2014markov}).
Hence, the limit of \Eqref{eq:avg_obj} exists and is called the \emph{long-term} average reward or \emph{gain} of state $s$ under policy $\pi$.
Concretely, it is defined by
\begin{equation}
    \label{eq:gain_defn}
    g^\pi(s) := \lim_{T\to+\infty}\frac1T \E^\pi\left[ \sum_{t=1}^{T} r_t \mid s_1=s\right].
\end{equation}
This notion generalizes both finite and discounted settings when $H\to+\infty$ or $\gamma\to1$ because it is shown (see \cite[Sections~8.2.1 and 8.2.2]{puterman2014markov}) that 
%\begin{align*}
%    \E^\pi\left[\sum_{t=1}^H r_t\mid s_1{=}s\right] \underset{H\to+\infty}{\sim} Hg^\pi(s) \text{ and }
%    \E^\pi\left[\sum_{t=1}^{+\infty} \gamma^{t-1}r_t \mid s_1{=}s\right] \underset{\gamma\to1}{\sim}
%    g^\pi(s)/(1-\gamma).
%\end{align*}
\begin{align*}
    w^\pi_{1:H}(s) \underset{H\to+\infty}{\sim} Hg^\pi(s) \text{ and }
    v^\pi_\gamma(s) \underset{\gamma\to1}{\sim}
    g^\pi(s)/(1-\gamma).
\end{align*}
In consequence, if $\pi$ and $\pi'$ are two stationary policies such that $g^\pi(s)< g^{\pi'}(s)$, then for $H$ big enough and $\gamma$ close enough to $1$, $w_{1:H}^\pi(s)\le w_{1:H}^{\pi'}(s)$ and $v^\pi_\gamma(s)\le v^{\pi'}_\gamma(s)$.
%$\E^\pi\left[\sum_{t=1}^H r_t\mid s_1{=}s\right]\le \E^{\pi'}\left[\sum_{t=1}^H r_t\mid s_1{=}s\right]$ and $\E^\pi\left[\sum_{t=1}^{+\infty} \gamma^{t-1}r_t \mid s_1{=}s\right]\le \E^{\pi'}\left[\sum_{t=1}^{+\infty} \gamma^{t-1}r_t \mid s_1{=}s\right]$.

The gain of stationary policy captures the average reward obtained in \emph{steady regime} (or \emph{asymptotic regime}).
Another quantity associated to stationary policy $\pi$ is its \emph{bias function} that is defined by: for each $s\in\gS$
\begin{equation}
    \label{eq:bias_defn}
    h^\pi(s) :=\Clim_{T\to+\infty} \E^\pi\left[ \sum_{t=1}^{T} r_t -g^\pi(s_t) \mid s_1=s\right].
\end{equation}
The \emph{Cesaro-limit} denoted by $\mathrm{C}$-$\mathrm{lim}$ is used because it is well-defined for any stationary policies albeit the ``classical'' limit may not exist for stationary policy that induces a stochastic process governed by \emph{periodic} Markov chain.
The bias function captures the expected total difference between the reward and the average reward in steady regime. 
The difference of bias values $h^\pi(s)-h^\pi(s')$ captures the (dis-)advantage of starting at state $s$ rather than $s'$ when following policy $\pi$.
We denote by $sp(\vh^\pi):=\max_{s\in\gS}h^\pi(s) -\max_{s\in\gS}h^\pi(s)$ the range or \emph{span} of the bias function of policy $\pi$.

Any stationary policy $\pi$ induces a Markov reward process (MRP) where the reward function and state transition are respectively encoded by vector $\vr^\pi$ and stochastic matrix $\mP^\pi$ such that for any $s,s'\in\gS$, $r^\pi(s):=r\big(s,\pi(s)\big)$ and $P^\pi(s,s'):=p\big(s'\mid s,\pi(s)\big)$.
%We define the \emph{limiting matrix} $\bar{\mP}^\pi:=\Clim_{t\to+\infty}{(\mP^\pi)^t}$ (\cite[Appendix~A.4]{puterman2014markov}). % which describes the state transition in steady regime under policy $\pi$. That is, $\bar{P}^\pi(s,s')$ is the probability that the MDP transitions from state $s$ to $s'$ in steady regime.
%Since the MDP has finite state space, $\bar{\mP}^\pi$ always exists and well-defined for any stationary policy $\pi$.
%It describes the state transition in steady regime under policy $\pi$. That is, $\bar{P}^\pi(s,s')$ is the probability that the MDP transitions from state $s$ to $s'$ in steady regime.
%So, the gain of policy $\pi$ can be expressed by for any $s\in\gS$, $g^\pi(s)=\sum_{s'\in\gS}\bar{P}^\pi(s,s')r^\pi(s')$.
%In vector notation, $\vg^\pi=\bar{\mP}^\pi\vr^\pi$.
%However, computing $\vg^\pi$ through $\bar{\mP}^\pi$ might be inefficient.
%Instead, the gain and bias can be computed using Bellman evaluation equations given in Proposition~\ref{prop:be_eval}.
The gain and bias can be computed using Bellman evaluation equations given in Proposition~\ref{prop:be_eval}.
\begin{prop}[{\cite[Theorem~8.2.6]{puterman2014markov}}]
    \label{prop:be_eval}
    For any stationary policy $\pi$, the gain $\vg^\pi$ and bias $\vh^\pi$ are a solution of the following system of Bellman evaluation equations: for any $s\in\gS$
    \begin{align}
        g(s) -\sum_{s'\in\gS}P^\pi(s,s')g(s') &=0 \label{eq:gain_eval}\\
        g(s) -r^\pi(s) +h(s) -\sum_{s'\in\gS}P^\pi(s,s')h(s') &=0. \label{eq:bias_eval}
    \end{align}
    Suppose that $\vg$ and $\vh$ satisfy \eqref{eq:gain_eval} and \eqref{eq:bias_eval}. Then, $\vg^\pi=\vg$ and $\vh^\pi=\vh+\vu$ where $u(s)=\sum_{s'\in\gS}P^\pi(s,s')u(s')$ for all $s$.
    %Finally, if $\bar{\mP}^\pi\vh=\vzero$, then $\vh^\pi=\vh$.
\end{prop}
Equations~\eqref{eq:gain_eval} and \eqref{eq:bias_eval} uniquely define $\vg$ and determine $\vh$ up to an element of the null space of $(\mI-\mP^\pi)$ where $\mI$ is the identity matrix of size $\abs{\gS}\times\abs{\gS}$.
If $\mP^\pi$ is unichain\footnote{we provide more explanation about unichain in \ref{it:unichain} of Definition~\ref{ch:mdp:defn:mdp_class}.}, then we say that policy $\pi$ is unichain and its gain is state independent: $g^\pi(s)=g^\pi(s')$ for any $s,s'\in\gS$.
So, for unichain policy $\pi$, we just denote its gain by $g^\pi$.
Moreover, if $\pi$ is unichain, its bias $\vh^\pi$ is defined up to a constant vector.

\subsection{Classification of Markov decision process}
Since \Eqref{eq:avg_obj} compares policies based on their average reward in steady regime, we need to take into account the chain structure induced by the policies.
%We assume that the reader is familiar with the notion of \emph{transient} and (positive) \emph{recurrent} states and/or class of a Markov chain(see \cite[Appendix~A]{puterman2014markov} or \cite{levin2017markov} for more detail) and classify the MDPs according to the following definition.
In a MDP with finite state space, the states are either \emph{transient} or \emph{recurrent}.
A state is \emph{transient} if it is never visited in steady regime.
For more formal definition, the reader can refer to \cite[Appendix~A]{puterman2014markov} or \cite{levin2017markov}.
We classify the MDPs according to the following definition.
\begin{defn}[Classification of MDPs]
    \label{ch:mdp:defn:mdp_class}
    We say that a MDP is
    \begin{enumerate}[label=(\roman*)]
        \item \textbf{ergodic} if the Markov chain induced by any stationary policy has a single recurrent class that equals the state space (\ie, all states are visited infinitely often with probability $1$ independently of initial state);
        \item \label{it:unichain} \textbf{unichain} if the Markov chain induced by any stationary policy is \emph{unichain}, \ie, it has a single recurrent class plus a --possibly empty-- set of transient states;
        \item \textbf{multichain} if it is not unichain;
        \item \textbf{communicating} if for every pair of states $(s,s')\in\gS$, there exists a stationary policy under which $s'$ is accessible from $s$ in finite time with non-zero probability
        \item \textbf{weakly communicating} if the state space can be partitioned into two subsets $\gS^{\gC}$ and $\gS^{\gT}$ (with $\gS^{\gT}$ possibly empty), such that for every pair of states $(s,s')\in\gS^{\gC}$, there exists a stationary policy under which $s'$ is accessible from $s$ in finite time with non-zero probability, and all states in $\gS^{\gT}$ are transient under all stationary policies.
    \end{enumerate}
\end{defn}
With this definition, ergodic MDPs are special cases of unichain MDPs and weakly communicating MDPs generalize communicating MDPs. 
Moreover, ergodic MDPs are communicating and unichain MDPs are weakly communicating.
Figure~\ref{fig:mdp_class} summarizes this relation.

\begin{figure}[ht]
    \centering
    \begin{tikzpicture}
        \draw (0,1) rectangle (6,4.4);
        \node at (7,3) {multichain};
        \node at (7,2) {unichain};
        % \node[text width=2cm, align=center] at (-1.5,0.5) {discounted problems};
        \draw[line width=1pt, fill=white,opacity=0.3] (0,2.5) rectangle (6,4.4);
        \node[text width=5cm, align=center] at (3,4) {Not weakly communicating}; %multichain
        \draw[line width=1pt, fill=blue,opacity=0.3] (0,1) rectangle (6,3.7);
        \node[text width=4cm, align=center] at (3,3.4) {Weakly communicating};
        %\draw[line width=1pt, fill=white,opacity=0.3] (0,1) rectangle (6,2.5);
        %\node{}; %unichain
        \draw[line width=1pt, fill=red, opacity=0.3] (0.5,1.3) rectangle (5.5,3.1);
        \node[text width=2cm, align=center] at (2.7,2.8) {Communicating};
        \draw[line width=1pt, fill=green,opacity=0.3] (1,1.6) rectangle (5,2.2);
        \node[text width=2cm,align=center] at (3,1.9) {Ergodic};
        \draw[line width=0.7pt, dashed] (-1,2.5) -- (7,2.5);%  (-2,1) -- (6.5,1);
    \end{tikzpicture}
    \caption{MDP space: the rectangles below the dashed line represent the set of unichain MDPs and the ones above the dashed line represent the set of multichain MDPs.
        The blue rectangle is the set of weakly communicating MDPs.
        The red rectangle on top of the blue one is the set of communicating MDPs.
        Finally, the green rectangle on top of the red one is the set of ergodic MDPs.
    }
    \label{fig:mdp_class}
\end{figure}

Verifying if a Markov chain is unichain can be done in $\landauO(\abs{\gS}^2)$ using Tarjan's strongly connected component algorithm.
Verifying if a MDP is unichain is, however, NP-hard \cite{tsitsiklis2007np}.
%For the pattern of states that are accessible from each other, we can check if a MDP is communicating or weakly communicating in $\landauO(\abs{\gS}^2)$ by leveraging \cite[Proposition~8.3.1]{puterman2014markov}:
%we construct a stochastic matrix $\mP^{\#}$ such that for all $s,s'\in\gS$, $P^{\#}(s,s'):=\displaystyle\frac1{\abs{\gA_s}}\sum_{a\in\gA_s}p(s'\mid s,a)$.
%We check the structure of Markov chain induced by $\mP^{\#}$ using Tarjan's strongly connected component algorithm and draw a conclusion as the following:
%\begin{itemize}
%    \item the MDP is communicating if and only if $\mP^{\#}$ is recurrent;
%    \item the MDP is weakly communicating if and only if $\mP^{\#}$ is unichain. 
%\end{itemize}

\subsection{Gain optimality}
\label{ch:mdp:sec:gain}

Similarly to the discounted setting, in MDPs with finite state and action spaces, there always exists an optimal stationary policy $\pi^*$ that satisfies \Eqref{eq:avg_obj} for any initial distribution $\rho$ (see \cite[Theorem~9.1.8]{puterman2014markov}).
Such an optimal policy $\pi^*$ induces the \emph{optimal gain} denoted by $\vg^*$ and the value of \Eqref{eq:avg_obj} is given by $\sum_{s\in\gS} \rho(s)g^*(s)$.
From \cite[Chapter~9]{puterman2014markov}, the optimal gain $\vg^*$ satisfies the following system of Bellman \emph{optimality} equations: for each $s\in\gS$,
\begin{align}
    g(s) &= \max_{a\in\gA_s} \sum_{s'\in\gS}p(s'\mid s,a)g(s') \label{eq:gain_opt} \\
    g(s) +h(s) &= \max_{a\in\gA_s}\Big(r(s,a) +\sum_{s'\in\gS}p(s'\mid s,a)h(s')\Big) \label{eq:bias_opt}.
\end{align}
So, $\vg^*$ is uniquely defined by \eqref{eq:gain_opt} and \eqref{eq:bias_opt}.
Any policies achieving $\vg^*$ are said to be \emph{gain optimal} (or average optimal): for any $\pi\in\Pi$, $g^\pi(s)\le g^*(s)$ for all $s\in\gS$.
By \cite[Theorem~8.3.2]{puterman2014markov}, if a MDP is weakly communicating, then the optimal gain is state independent: $g^*(s)=g^*(s')$ for any $s,s'\in\gS$.
So, given weakly communicating MDPs, we just denote the optimal gain by $g^*$.
Note that the fact that the optimal gain is constant over states does not imply that all gain optimal policies are unichain.
Table~\ref{tab:mdp_vs_gain} describes the gain of stationary policies in different classes of MDPs.
\begin{table}[ht]
    \begin{tabular}{lll}
        \hline
        Model class          & Optimal gain         & Gain of a stationary policy \\ \hline
        Ergodic              & Constant             & Constant                    \\
        Unichain             & Constant             & Constant                    \\
        Communicating        & Constant             & Possibly nonconstant        \\
        Weakly Communicating & Constant             & Possibly nonconstant        \\
        Multichain           & Possibly nonconstant & Possibly nonconstant       \\ \hline
    \end{tabular}
    \caption{\cite[Table~8.3.1]{puterman2014markov}: Relationship between MDP class and gain}
    \label{tab:mdp_vs_gain}
\end{table}
Finally, \eqref{eq:gain_opt} and \eqref{eq:bias_opt} do not uniquely defined $\vh$.
The properties of solution space for $\vh$ given $\vg^*$ is studied in \cite{schweitzer1978functional}.
%Moreover, we have $\bar{\mP}^\pi\mP^\pi =\mP^\pi\bar{\mP}^\pi =\bar{\mP}^\pi$.
%However, computing $\vg^\pi$ through $\bar{\mP}^\pi$ might be inefficient.

\section{New notion of optimality: Bellman optimal}
\label{ch:mdp:sec:bell}

For this thesis, gain optimal policies do not capture the information that we need when dealing with Markovian bandit problem.
Hence, we introduce a new notion of optimality which is crucial for Chapter~\ref{ch:index_computation}.
%In an ergodic MDP, the single recurrent class is the state space of the MDP and gain optimal policy

\subsection{Characterization of gain optimal policies.}

To understand our new notion of optimality, we first need to characterize the gain optimal policies.
For this purpose, we define a notion of (dis-)advantage of an action with respect to a given policy.
So, an optimal action has null advantage with respect to an optimal policy.

%\KK{Put more context}
Let $\vg$ and $\vh$ be a solution of Bellman evaluation equations \eqref{eq:gain_eval} and \eqref{eq:bias_eval} for policy $\pi$.
We define the (dis-)advantage of action\footnote{Note that it is also a function of $\vh$.} $a\in\gA_s$ in state $s$ over policy $\pi$ by
\begin{equation}
    \label{eq:advan}
    B^a_s(\vg,\vh) := r(s,a) +\sum_{s'\in\gS}p(s'\mid s,a)h(s') -g(s) -h(s).
\end{equation}
If $\pi$ is unichain, then $\vh$ is determined up to a constant vector and $B^a_s(\vg,\vh)$ is then uniquely defined for any $s\in\gS$, $a\in\gA_s$, and $(\vg,\vh)$ that satisfies \eqref{eq:gain_eval} and \eqref{eq:bias_eval}.
%$\vh\in\R^{\abs{\gS}}$ that satisfies \eqref{eq:bias_eval}.

For $\vg^*$ and $\vh$ a solution of Bellman optimality equations \eqref{eq:gain_opt} and \eqref{eq:bias_opt}, %an action $a$ in state $s$ that satisfies $B^{a^*}_s(\vg^*,\vh)=0$ is potentially optimal.
an optimal action $a\in\gA_s$ for state $s$ satisfies both $B^a_s(\vg^*,\vh)=0$ and ${\sum_{s'\in\gS}p(s'\mid s,a)g^*(s')} =g^*(s)$ \cite{puterman2014markov, schweitzer1978functional}.
%However, it is not 
The following lemma characterizes gain optimal policy by showing that the policy satisfies \eqref{eq:bias_opt} on their recurrent states for any $\vh$ a solution of \eqref{eq:bias_opt}.
\begin{lem}
    \label{lem:opt_pol}
    Let $\pi:\gS\mapsto\gA$ be a policy and $\Phi^\pi$ be the set of recurrent states under policy $\pi$.
    The three properties below are equivalent.
    \begin{enumerate}[label=(\roman*)]
        \item \label{it:opt_pol1} $\mP^\pi\vg^*=\vg^*$ and for all $\vh\in \R^{\abs{\gS}}$ satisfying \eqref{eq:bias_opt}, $B^{\pi(s)}_s(\vg^*,\vh)=0$ for all $s\in\Phi^\pi$
        \item \label{it:opt_pol2} $\mP^\pi\vg^*=\vg^*$ and for some $\vh\in \R^{\abs{\gS}}$ satisfying \eqref{eq:bias_opt}, $B^{\pi(s)}_s(\vg^*,\vh)=0$ for all $s\in\Phi^\pi$
        \item \label{it:opt_pol3} $\pi$ is gain optimal.
    \end{enumerate}
\end{lem}
\begin{proof}
    First of all, given a policy $\pi$, we define the \emph{limiting matrix} $\bar{\mP}^\pi{:=}\displaystyle\Clim_{t\to+\infty}{(\mP^\pi)^t}$ (\cite[Appendix~A.4]{puterman2014markov}). % which describes the state transition in steady regime under policy $\pi$. That is, $\bar{P}^\pi(s,s')$ is the probability that the MDP transitions from state $s$ to $s'$ in steady regime.
    Since the MDP has finite state space, $\bar{\mP}^\pi$ always exists and well-defined for any stationary policy $\pi$.
    It describes the state transition in steady regime under policy $\pi$.
    That is, $\bar{P}^\pi(s,s')$ is the probability that the MDP transitions from state $s$ to $s'$ in steady regime.
    By \cite[Theorem~8.2.6]{puterman2014markov}, the gain of policy $\pi$ can be expressed by $g^\pi(s)=\sum_{s'\in\gS}\bar{P}^\pi(s,s')r^\pi(s')$ for any $s\in\gS$.
    In vector notation, $\vg^\pi=\bar{\mP}^\pi\vr^\pi$.
    We denote by $\Phi^\pi$ the set of recurrent states under policy $\pi$.
    From \cite[Section~A.4]{puterman2014markov}, we have
    \begin{itemize}
        \item $\bar{\mP}^\pi\mP^\pi =\mP^\pi\bar{\mP}^\pi =\bar{\mP}^\pi$
        \item If $s'\notin\Phi^\pi$, then $\bar{P}^\pi(s,s')=0$ for all $s\in\gS$
        \item If $\pi$ is unichain, then the rows of $\bar{\mP}^\pi$ are identical.
    \end{itemize}
    Now, we can prove the lemma.

    \ref{it:opt_pol1} $\Rightarrow$ \ref{it:opt_pol2} is trivial.

    \ref{it:opt_pol2} $\Rightarrow$ \ref{it:opt_pol3}: By definition of $B_s^a(\vg^*,\vh)$, we have $r^{\pi}(s)-g^*(s) = h(s)-\sum_{s'\in\gS} P^{\pi}(s,s')h(s')$ for any recurrent state $s$ of $\pi$.  Multiply this with $\bar{P}^\pi(j,s)$ and sum over $s\in\gS$ (if $s$ is not recurrent, then $\bar{P}^\pi(j,s)=0$) gives
    \begin{align*}
        \sum_{s\in\gS} \bar{P}^\pi(j,s)\Big(r^{\pi}(s)-g^*(s)\Big) {=} \sum_{s\in\gS} \bar{P}^\pi(j,s)h(s) {-} \underbrace{\sum_{s\in\gS} \bar{P}^\pi(j,s)\sum_{s'\in\gS} P^{\pi}(s,s')h(s')}_{=\sum_{s'\in\gS} \bar{P}^\pi(j,s')h(s') \text{ since $\bar{\mP}^\pi\mP^\pi{=}\bar{\mP}^\pi$.}}
        = \vzero.
    \end{align*}
    The gain of $\pi$ is $\bar{\mP}^\pi \vr^\pi$.
    The above equation shows that $\bar{\mP}^\pi\vr^\pi = \bar{\mP}^\pi\vg^*$. Moreover, the assumption  $\mP^\pi\vg^*=\vg^*$ implies that $\bar{\mP}^\pi\vg^*=\vg^*$ which in turn implies that $\bar{\mP}^\pi\vr^\pi=\vg^*$. This shows that the gain of $\pi$ is $\vg^*$ and therefore $\pi$ is gain optimal.
    % So, we have $\mP^\pi\vg^*=\vg^*$ and $\bar{\mP}^\pi(\vr^\pi-\vg^*)=\vzero$.

    \ref{it:opt_pol3} $\Rightarrow$ \ref{it:opt_pol1}: If $\pi$ is gain optimal, then $\mP^\pi \vg^*=\vg^*$ and $\bar{\mP}^\pi(\vr^\pi-\vg^*)=\vzero$.
    The latter rewrites as $\sum_{s\in\gS}\bar{P}^\pi(j,s)\Big(r^{\pi}(s)-g^*(s)\Big) =0$ for all state $j$. Let $\vh$ be a solution of \eqref{eq:bias_opt}.
    For all state $j$, we have    
    \begin{align*}
        %\label{eq:apx_proof_H}
        \sum_{s\in\gS}\bar{P}^\pi(j,s)B^{\pi(s)}_s(\vg^*,\vh) {=} \sum_{s\in\gS}\bar{P}^\pi(j,s)\Big(r^{\pi}(s)-g^*(s) {+}\sum_{s'\in\gS}P^{\pi}(s,s')h(s') -h(s)\Big) {=}0.
    \end{align*}
    As $\vh$ satisfied \eqref{eq:bias_opt}, for all action $a\in\gA_s$, we have $B^{a}_s(\vg^*,\vh) \le0$ for all states $s\in\gS$ and in particular $B^{\pi(s)}_s(\vg^*,\vh) \le0$.
    This shows that for any state $s$ such that $\bar{P}^\pi(j,s)>0$, one must have $B^{\pi(s)}_s(\vg^*,\vh) =0$. Such state $s$ are the recurrent states of $\pi$.
    This shows that $B^{\pi(s)}_s(\vg^*,\vh) =0$ for all $s\in\Phi^\pi$.
\end{proof}

Lemma~\ref{lem:opt_pol} shows that gain optimal policies achieve the maximum of \eqref{eq:bias_opt} on their recurrent states.
So, we say that the gain optimal policies \emph{act optimally} in their recurrent states.
With this characterization, in ergodic MDPs, a policy is gain optimal if and only if it acts optimally in all states.
However, for MDPs that admit transient states, a policy can be gain optimal without acting optimally in all states, \ie, it acts optimally in its recurrent states and possibly randomly in the transient states.
We give an example in Figure~\ref{fig:gain_vs_bellman} in which there are two gain optimal policies.
One of them achieves the maximum of $\eqref{eq:bias_opt}$ in all states.
%TODO: continue here, should I mention about unicity
\begin{figure}[ht]
    \centering
    \begin{tabular}{cc}
        \begin{minipage}{.25\linewidth}
            \begin{tikzpicture}[on grid, state/.style={circle,draw}, >= stealth', auto, prob/.style = {inner sep=1pt,font=\scriptsize}]
                \node[state,color=blue]  (A) {$2$};
                \node[state,color=blue]  (B) [left =1.5cm of A]   {$1$};
                \path[->]
                    (A) edge[loop above,color=red, dashed] node{$1$} (A)
                    (B) edge[bend left, color=black] node{$1$} (A)
                    (B) edge[bend right, color=red, dashed] node[below]{$0.5$} (A);
            \end{tikzpicture}
        \end{minipage}
        &
        \begin{minipage}{.7\linewidth}
            There are two policies $\pi^1$ where $\pi^1(1){=}\pi^1(2){=}0$ and $\pi^2$ where $\pi^2(1){=}1$ and $\pi^2(2){=}0$.
            Both policies are gain optimal.
            By solving \eqref{eq:gain_opt} and \eqref{eq:bias_opt}, we have $g^*=1$ and $h^*(1){=}h^*(2){=}c$ where $c$ can be any real number.
            Consequently, policy $\pi^1$ does not satisfies the maximum of \eqref{eq:bias_opt} on state $1$ for any $c\in\R$ but $\pi^2$ does.
            So, we want to characterize policy $\pi^2$.
        \end{minipage}
    \end{tabular}
    \caption{An example where some gain optimal policies do not act optimally in all states of the MDP.
        The black arrow shows state transition of action $1$ and the red ones for action $0$.
        The numbers along the arrows show the reward when executing the actions.
}
    \label{fig:gain_vs_bellman}
\end{figure}

\paragraph{Remark.} The characterization of gain optimal policies was analysed in \cite{puterman2014markov, schweitzer1978functional}.
However, it was expressed and proved differently from what we have done for Lemma~\ref{lem:opt_pol}.

\subsection{Definition of Bellman optimality}

The gain optimal policies give more importance to the reward in steady regime. 
That is, they act optimally in recurrent states and possibly randomly in transient states.
In this thesis, we distinguish gain optimal policies from the policies that attain the maximum of \eqref{eq:bias_opt} in all states as the following.
\begin{defn}[Bellman optimal policy]
    %Consider a MDP in which the space $\gS$ is finite and $\gA_s$ is finite for any $s\in\gS$.
    A policy $\pi$ is \emph{Bellman optimal} if there exists vector $\vh\in\R^{\abs{\gS}}$ that satisfies \eqref{eq:bias_opt} with $\vg^*$ such that $\pi(s)\in\argmax_{a\in\gA_s}\Big(r(s,a) +\sum_{s'\in\gS}p(s'\mid s,a)h(s')\Big)$ for all $s\in\gS$.
\end{defn}
By \cite[Theorem~9.1.7]{puterman2014markov}, Bellman optimal policy always exists in MDPs with finite state and action spaces and Bellman optimal implies gain optimal.
However, the converse is not true in general.
So, the notion of Bellman optimality is stronger than the notion of gain optimality.
By this definition, policy $\pi^2$ in \figurename~\ref{fig:gain_vs_bellman} is Bellman optimal.

Note that the distinction between Bellman and gain optimal disappears in discounted MDPs or ergodic MDPs for average reward criterion.

A Bellman optimal policy can be obtained using \emph{Multichain Policy Iteration} algorithm.
We refer to \cite[Section~9.2.1]{puterman2014markov} for more detail about this algorithm.


\subsection{Characterization of Bellman optimal policies.}

%The previous lemma shows that a policy is gain optimal if and only if the actions for the recurrent states of the policy satisfy \eqref{eq:bias_opt}.
The previous lemma shows that a policy is gain optimal if and only if the actions for the recurrent states of the policy satisfy \eqref{eq:bias_opt} for any $\vh$ a solution to this \eqref{eq:bias_opt}.
However, some gain optimal policies induce bias vector that is not a solution of \eqref{eq:bias_opt}.
That is, in \figurename~\ref{fig:gain_vs_bellman}, the bias of policy $\pi^1$ is $h^{\pi^1}(1)=c-0.5$ and $h^{\pi^1}(2)=c$ where $c$ is any real number.
The following lemma shows the relationship between two Bellman optimal policies that are unichain and share at least one common recurrent state.
%The previous lemma shows that a gain optimal policy satisfies \eqref{eq:bias_opt} on its recurrent states for any $\vh$ a solution of \eqref{eq:bias_opt}.
%An interesting question to ask is whether a Bellman optimal policy satisfies \eqref{eq:bias_opt} on all states for any $\vh$ a solution of \eqref{eq:bias_opt}.
%The following lemma clarifies this question for two Bellman optimal policies that are unichain and share at least one common recurrent state.

\begin{lem}
    \label{lem:equi_bias}
    Suppose that two policies $\pi$ and $\theta$ are Bellman optimal, unichain and have at least one common recurrent state: $\Phi^\pi\cap\Phi^\theta\neq\emptyset$.
    
    Then for any $\vh^\pi$ and $\vh^\theta$ solutions of \eqref{eq:bias_eval} for $\pi$ and $\theta$ respectively, there exists a constant $c$ such that for all state $s$: $h^\pi(s) -h^\theta(s) =c$. Moreover, in this case, ${B_s^{\theta(s)}(\vh^\pi)=B_s^{\pi(s)}(\vh^\theta)=0}$ for all $s$.
\end{lem}
\begin{proof}
    Since $\pi$ and $\theta$ are Bellman optimal, $\vh^\pi,\vh^\theta$ satisfy \eqref{eq:bias_opt} together with $\vg^*$.
    In consequence, we have
    \begin{align*}
        \vh^\pi \ge \vr^\theta -\vg^* +\mP^\theta\vh^\pi.
    \end{align*}
    By Lemma~\ref{lem:opt_pol}~\ref{it:opt_pol1}, the above inequality is an equality for all $s\in\Phi^\theta$ because $\theta$ is gain optimal.

    As $\vh^\theta$ satisfies \eqref{eq:bias_eval}, we have
    \begin{align*}
        \vh^\theta -\vh^\pi &\le \vr^\theta -\vg^* +\mP^\theta\vh^\theta -(\vr^\theta -\vg^* +\mP^\theta\vh^\pi) = \mP^\theta(\vh^\theta -\vh^\pi),
    \end{align*}
    with equality for all state $s\in\Phi^\theta$. This shows that for all $t$, $\vh^\theta -\vh^\pi\le (\mP^\theta)^t(\vh^\theta -\vh^\pi)$ which implies that $\vh^\theta -\vh^\pi\le \bar{\mP}^\theta(\vh^\theta -\vh^\pi)$ with equality for all states $s\in\Phi^\theta$. Similarly, $\vh^\pi -\vh^\theta \le \bar{\mP}^\pi(\vh^\pi -\vh^\theta)$ with equality for any state $s\in\Phi^\pi$.

    Let $c^\pi_s=\sum_{s'\in\gS}\bar{P}^\pi(s,s')\Big(h^\pi(s')-h^\theta(s')\Big)$ and $c^\theta_s=\sum_{s'\in\gS}\bar{P}^\theta(s,s')\Big(h^\pi(s')-h^\theta(s')\Big)$. By what we have just shown, for all state $s$, we have
    \begin{align*}
        c^\theta_s \underbrace{\le}_{\text{equality if $s\in\Phi^\theta$}} h^\pi(s)-h^\theta(s) \underbrace{\le}_{\text{equality if $s\in\Phi^\pi$}} c^\pi_s
    \end{align*}
    As both policies are unichain, $c^\pi_s$ and $c^\theta_s$ do not depend on $s$.
    Moreover, if there exists $s\in\Phi^\theta\cap\Phi^\pi$, we have $c^\pi_s=c^\theta_s = c$. In consequence,  $h^\pi(s)-h^\theta(s)=c$ for all state $s$. 
\end{proof}

\subsection{Unicity of Bellman optimal policy.}
\label{ssec:unicity}

In some problems, it is crucial to check if the MDP has a single Bellman optimal policy.
This task is not trivial due to the multiplicity of the solution to \eqref{eq:bias_opt} given $\vg^*$.
So, the following lemma shows a condition which can be used to verify if a given Bellman optimal and unichain policy is the unique one.

\begin{lem}
    \label{lem:unicity_BO}
    Let $\pi$ be a Bellman optimal policy that is unichain. If $\pi$ is not the unique Bellman optimal policy, then there exists a state $s$ and an action $a\in\gA_s\setminus\{\pi(s)\}$ such that $B_s^a(\vh^\pi)=0$.
\end{lem}

\begin{proof}
    Let $\theta\neq\pi$ be another Bellman optimal policy. Since $\theta$ is gain optimal and $\vh^\pi$ satisfies \eqref{eq:bias_opt}, Lemma~\ref{lem:opt_pol} implies that $B_s^{\theta(s)}(\vh^\pi) =0$ for all $s\in\Phi^\theta$. If there exists $s\in\Phi^\theta$ such that $\theta(s)\neq\pi(s)$, then the proof is concluded.  Otherwise, $\theta(s)=\pi(s)$ for all $s\in\Phi^\theta$.
    This show that $\theta$ and $\pi$ coincide for all recurrent states of $\theta$ and that $\Phi^\theta=\Phi^\pi$. Moreover, as $\pi$ is unichain, $\theta$ is also unichain. Hence, Lemma~\ref{lem:equi_bias} implies that $B_s^{\theta(s)}(\vh^\pi)=0$ for all $s$. Since $\theta\neq\pi$, there exists at least one state $s\in\gS$ such that $\theta(s)\neq\pi(s)$.
\end{proof}
Lemma~\ref{lem:unicity_BO} implies that a Bellman optimal and unichain policy $\pi$ is the unique one if and only if for all $s$, $B_s^a(\vh^\pi)<0$ for all $a\in\gA_s\setminus\{\pi(s)\}$.
Note that if a MDP has a single Bellman optimal policy, that policy must be unichain.
Indeed, if the Bellman optimal policy is multichain, we can construct unichain policies that act optimally in the recurrent classes of the multichain policy.
These handmade unichain policies are Bellman optimal.

\endgroup
