\begingroup
\let\clearpage\relax

\chapter{Introduction}
\label{chapter:introduction}

\section{Topic of the thesis}

Markov decision processes (MDPs) are powerful models for solving stochastic optimization problems. They suffer, however, from what is called the \emph{curse of dimensionality}, which basically says that the state size of a Markov process is exponential in the number of system components. This implies that the complexity of computing an optimal policy is generally exponential in the number of system components. The same holds for general-purpose reinforcement learning (RL) algorithms: they all have a regret and a runtime exponential in the number of components, so they also suffer from the same curse.

Very few MDPs are known to escape from this curse of dimensionality. One of the most famous examples is the Markovian bandit problem in which a decision maker faces $n$ MDPs (the $n$ components, which we will call the $n$ arms in the rest of the thesis) and must decide which arms to activate at each decision epoch.
Markovian bandits have been applied to many resource allocations and scheduling problems such as wireless communication \cite{raghunathan2008index, liu2010indexability, aalto2019whittle}, web crawling \cite{nino2014dynamic, avrachenkov2022whittle}, congestion control \cite{avrachenkov2013congestion, avrachenkov2018impulsive}, queueing systems \cite{glazebrook2009index, aalto2009gittins, archibald2009indexability, aalto2011properties, larranaga2015asymptotically, borkar2017whittle, scully2018soap}, and clinical trials \cite{villar2015multi}.

Markovian bandits are well-structured MDPs.
They form a subclass of multi-armed bandit problems in which each arm has an internal state that evolves in a Markovian manner as a function of the decision makerâ€™s actions.
In such a problem, the decision maker observes the state of all arms at each decision time and chooses which to activate.
When the state of an arm evolves only when this arm is chosen, one falls into the category of \emph{rested} Markovian bandits.
When the state of an arm can also evolve when the arm is not chosen, the problem is called a \emph{restless} bandit problem.
It has been shown over the years that index policy, a strategy that requires computation load linearly in number of arms, performs exceptionally well in Markovian bandit problems \cite{glazebrook2002index, ansell2003whittle, glazebrook2006some, avrachenkov2013congestion, akbarzadeh2019restless}.
Moreover, there is a subclass of Markovian bandits in which index policy is shown to be optimal \cite{gittins1979bandit}.

Two celebrated index definitions are \emph{Gittins index} \cite{gittins1979bandit} for rested bandits and \emph{Whittle index} \cite{whittle1988restless} for restless bandits.
Yet, as mentioned in \cite[Chapter~14]{whittle1996optimal}, the existence of Whittle index is guaranteed only for restless bandits that satisfy a so-called indexability property.
To the best of our knowledge, there is no efficient general-purpose algorithm to test the indexability in restless bandits.
Meanwhile, few RL algorithms that leverage index policy in learning with Markovian bandits despite its desirable computational complexity compared to dynamic programming solutions, .
These raise two grand questions in this thesis:
\begin{itemize}
    \item {\color{myblue}\bfseries\large How to efficiently test the indexability and compute the Whittle index?}
    \item {\color{myblue}\bfseries\large Can index policy be a pillar for RL algorithms when learning in Markovian bandits?}
\end{itemize}

\section{Contributions}

Inspired by the challenging questions above, this thesis has several goals that can be regrouped into two main parts: (1) efficiently test the indexability and compute the Whittle index for Markovian bandits and (2) learn with efficiency in Markovian bandits.

Firstly, we design an efficient single algorithm that tests indexability and computes the Whittle index for both discounted and undiscounted restless bandits and the Gittins index for discounted rested bandits.

Secondly, we adapt some generic RL algorithms to Markovian bandit problems to remove the exponentiality from their regret.
We study different aspects of the adapted RL algorithms depending on the nature of Markovian bandit. In discounted rested bandit, policy computation and regret are the focus. In undiscounted restless bandit, the implication of the structure of arms in the structure of bandit and regret come under the spotlight.

To make these contributions explicit, we divide this thesis into three parts.

\subsection{Part~{\ref{part:background}}: Background}

In this part, we recall the existing problem setups and results in the literature.
This part serves as the basis for understanding our contributions and positioning them among the vast literature of RL and Markovian bandits.

We present the formalism of Markov decision process (MDP) in Chapter~\ref{ch:mdp}: we give the notations, optimization criteria, and the existing theoretical results in MDP.
This chapter is the central pillar of all the chapters that follow.

Chapter~\ref{ch:rl} consists of a summary of existing learning setups in RL, regret definition that is used as a performance metric of learning algorithms, baseline regret or minimax regret lower bound, and several existing results in RL with generic tabular MDPs.

We present the formalism of Markovian bandit in Chapter~\ref{ch:mb}. We give the notations, existing setups and optimization criteria in Markovian bandit literature.
We finish this chapter by outlining all the questions discussed in the following chapters.

\subsection{Part~{\ref{part:idx}}: Indexability}

In this part, we present our contributions to the computational side of Markovian bandit literature.

In Chapter~\ref{ch:indexability}, we point out the possible ambiguities in the classical definition of indexability in restless Markovian bandits by providing a few simple counter-examples.
This leads us to introduce a new notion of Bellman optimality in the MDP with a long-term average reward criterion.
This new notion is then used in our definition of indexability, which is thoroughly detailed in the chapter.
Then, we give the corresponding Whittle index definition and illustrative examples.
%Then, we give the corresponding definition of Whittle index accompanied by illustrative examples.
Finally, we complete the chapter by studying the properties of the Bellman optimality that are useful for indexability tests and index computation.

In Chapter~\ref{ch:index_computation}, we present a new algorithm to test the indexability presented in the previous chapter and compute the Whittle index of any finite state restless arm.
It is a single algorithm that tests the indexability and computes the Whittle index in either discounted or undiscounted restless bandits, and the Gittins index in discounted rested bandits.
Moreover, to the best of our knowledge, this algorithm is the first to achieve subcubic theoretical computational complexity.
Indeed, if the considered arm has $S$ states, then the best variant of our algorithm performs $\landauO(S^{2.5286})$ arithmetic operations\footnote{multiplications and additions of real numbers, regardless of their values.}.
%That is, if the considered arm has $S$ states, then our algorithm perform $\landauO(S^3)$ (the constant for $S^3$ is explicit in the chapter) arithmetic operations\footnote{multiplications and additions of real numbers, regardless of their values.}.
This is made possible by the sporadic use of the fastest matrix multiplication method of \cite{coppersmith1987matrix} and the Sherman-Morrison formula.
Thanks to the current implementation of matrix multiplication in python, our algorithm is implemented to run in subcubic time in python programming language.
We also present a few numerical experiments that witness the subcubic achievement of our algorithm.
%The best variant of our implementation is about 20 times faster than the current best algorithm\footnote{However, the comparison has its limit because the execution time is recorded on different machines and from an implementation in different programming languages.}.
The code of all experiments is available at \url{https://gitlab.inria.fr/markovianbandit/efficient-whittle-index-computation}.
All variants of our implementation are available in the form of an open-source python package installable by a simple command line: \texttt{pip install markovianbandit-pkg}.
Finally, we believe that our algorithm has room for improvement, such as exploiting the sparse structure of the arms in Markovian bandits. We leave this question to future work.

\subsection{Part~{\ref{part:learning}}: Learning in Markovian bandits}

In this part, we present our contributions to the learning side of Markovian bandit literature.

In Chapter~\ref{ch:learning_rested}, we consider an episodic RL problem in which the unknown environment is a rested Markovian bandit having $n$ arms and $S$ states per arm, and the episode length is geometrically distributed.
So, the bandit has $S^n$ states in total.
Given that Gittins index policy is optimal and computationally efficient when the bandit is known \cite{gittins1979bandit}, we compare the optimism in face of uncertainty (OFU) principle method with posterior sampling in terms of runtime and learning performance encoded by regret.
To do so, we adapt UCRL2 \cite{jaksch2010near} and UCBVI \cite{azar2017minimax}, two different algorithms from the OFU family, and PSRL \cite{osband2013more}, an algorithm from the posterior sampling family, to rested bandit with discount.
The adapted versions are respectively called MB-UCRL2, MB-UCBVI, and MB-PSRL, where ``MB'' stands for Markovian bandit.
We show that the three MB-* algorithms have a regret upper bounded like $\tilde{\landauO}(S\sqrt{nK})$, where $K$ is the number of episodes.
This is an exponential improvement in terms of the number of arms $n$.
We also derive the baseline regret for learning algorithms in rested bandit with discount.
That is, any algorithm learning in discounted rested bandit suffers a regret that is at least $\Omega(\sqrt{SnK})$.
For the computational aspect, we show that UCRL2 and its variants that use extended value iteration \cite{jaksch2010near} cannot leverage Gittins index policy to achieve efficient policy computation.
This is because such algorithms rely on the confidence bonus on arms' transition; which does not allow them to guarantee the OFU principle when working on each arm independently of the other arms.
Lastly, we perform several numerical experiments that advocate the good behavior of MB-PSRL.
These experiments are reproducible by following this link \url{https://gitlab.inria.fr/kkhun/learning-in-rested-markovian-bandit}.
While acknowledging that posterior sampling has a weaker regret guarantee, we conclude that this approach has the upper hand in problem adaptability when compared to OFU method.
This is vital when working with problems having a special structure such as weakly coupled MDPs.

Chapter~\ref{ch:learning_restless} considers the RL problems in which the unknown environment is a restless Markovian bandit with a long-term average reward criterion.
We study how the structure of arms translates into the structure of the restless bandit.
In particular, we show that no RL algorithms can perform uniformly well in restless bandits whose arms are unichain and have a bounded span of any local bias function.
This inspires us to study the restless bandit whose arms all have no local transient state.
We provide examples that show that a restless bandit whose arms are ergodic, can be multichain.
Also, an ergodic restless bandit can have an arbitrarily large mixing time, although all of its arms are ergodic and have an upper bounded mixing time.
We also provide a piece of positive result showing that if all arms are ergodic, then the restless bandit is communicating.
Moreover, if all arms have an ergodicity coefficient smaller than $1$, then the corresponding restless bandit also has an ergodicity coefficient smaller than $1$.
Finally, we discuss a few issues such as which baseline policy to use in regret definition when learning general restless bandits.
All of our arguments imply that defining a subclass of restless bandits with desirable properties for learning is essential but complex.

\section{Organization of the thesis}

We provide the thesis structure in \figurename~\ref{fig:flo}.

\tikzset{rect/.style={rectangle, rounded corners, minimum width=4.5cm, minimum
height=1cm,text centered, draw=amblu, text=myblue, fill=RoyalBlue!5, drop shadow},
arrow/.style={thick,->,>=stealth}}

\begin{figure}[h]
\begin{center}
\begin{tikzpicture}[node distance=1cm]

\node (mdp) [rect, label={[font=\small\sffamily,color=gray25,name=part1]above:{Part I: Background}}] {Chapter 2: Markov Decision Process};
\node (rl) [rect, below right = 0.75cm and -2.5cm of mdp] {Chapter 3: Reinforcement Learning};
\node (mb) [rect, below left = 0.75cm and -2.1cm of mdp] {Chapter 4: Markovian Bandits};

\node (idx) [rect, below = 1.25cm of mb, label={[font=\small\sffamily,color=gray25,name=part2]above left:{Part II: Indexability}}] {Chapter 5: Indexability};
\node (widx) [rect, below = 0.75cm of idx] {Chapter 6: Index Computation};

\node (rested) [rect, below = 3cm of rl, label={[font=\small\sffamily,color=gray25,name=part3]above right:{Part III: Learning}}] {Chapter 7: Learning Rested Bandit};
\node (restless) [rect, below = 0.75cm of rested] {Chapter 8: Learning Restless Bandit};

%\node (litrev) [rect, below left = 0.75cm and -1.1cm  of mdp,
%label={[font=\small\sffamily,name=label3]above left:{Theoretical analysis}}] {Literature Review};
%\node (result) [rect, below right = 2.50cm and -1.1cm  of mdp] {Results};

\draw [arrow] (mdp) -| (rl);
\draw [arrow] (mdp) -| (mb);
\draw [arrow] (mb) -- (idx);
\draw [arrow] (mb) -- (rested);
\draw [arrow] (idx) -- (widx);
\draw [arrow,dashed] (widx) -- (rested);
\draw [arrow] (rl) -- (rested);
\draw [arrow] (rested) -- (restless);

%\coordinate (aux) at (mb.south -| rested);

\begin{scope}[on background layer]
\node[draw,dashed,amdove,rounded corners,fill=yellow!40,fit=(mdp) (rl) (mb) (part1)]{};
\node[draw,dashed,amdove,rounded corners,fill=yellow!40,fit=(idx) (widx) (part2)]{};
\node[draw,dashed,amdove,rounded corners,fill=yellow!40,fit=(rested) (restless) (part3)]{};
\end{scope}
\end{tikzpicture}
\end{center}
\caption{Thesis organization} \label{fig:flo}
\end{figure}

To facilitate the comprehension of this thesis, we suggest the following flow of reading: One would want to start with Chapters~\ref{ch:mdp} and \ref{ch:mb}, then Part~\ref{part:idx}. Next, one should jump back to Chapter~\ref{ch:rl} before diving into Part~\ref{part:learning} and finishing the conclusion.

We highlight that Chapters~\ref{ch:mdp} and \ref{ch:mb} are required to understand our contributions in Part~\ref{part:idx}, and
Part~\ref{part:background} is required to understand our contributions in Part~\ref{part:learning}.

%To introduce my work, I will write a nice introduction in the following.
%Citation example for the Top500 website~\cite{top500} and some random paper~\cite{graham1969}.
%
%\section{Hello World}
%\lipsum[1-5]

\endgroup
