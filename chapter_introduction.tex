\begingroup
\let\clearpage\relax

\chapter{Introduction}
\label{chapter:introduction}

\section{Topic of the thesis}

Markov decision processes (MDPs) are powerful models to solve stochastic optimization problems. They suffer, however, from what is called the \emph{curse of dimensionality}, which basically says that the state size of a Markov process is exponential in the number of system components. This implies that the complexity of computing an optimal policy is, in general, exponential in the number of system components. The same holds for general-purpose reinforcement learning (RL) algorithms: they all have a regret and a runtime exponential in the number of components, so they also suffer from the same curse.

Very few MDPs are known to escape from this curse of dimensionality. One of the most famous examples is the Markovian bandit problem in which a decision maker faces $n$ MDPs (the $n$ components, that we will call the $n$ arms in the rest of the thesis), and must decide which arms to activate at each decision epoch.
Markovian bandits have been applied to many resource allocations and scheduling problems such as wireless communication \cite{raghunathan2008index, liu2010indexability, aalto2019whittle}, web crawling \cite{nino2014dynamic, avrachenkov2022whittle}, congestion control \cite{avrachenkov2013congestion, avrachenkov2018impulsive}, queueing systems \cite{glazebrook2009index, aalto2009gittins, archibald2009indexability, aalto2011properties, larranaga2015asymptotically, borkar2017whittle, scully2018soap}, and clinical trials \cite{villar2015multi}.

Markovian bandits are well-structured MDPs.
They form a subclass of multi-armed bandit problems in which each arm has an internal state that evolves over time in a Markovian manner, as a function of the decision makerâ€™s actions.
In such a problem, at each decision time, the decision maker observes the state of all arms and chooses which one to activate.
When the state of an arm evolves only when this arm is chosen, one falls into the category of \emph{rested} Markovian bandits.
When the state of an arm can also evolve when the arm is not chosen, the problem is called a \emph{restless} bandit problem.
It has been shown over the years that index policy, a strategy that requires computation load linearly in number of arms, performs extremely well in Markovian bandit problems \cite{glazebrook2002index, ansell2003whittle, glazebrook2006some, avrachenkov2013congestion, akbarzadeh2019restless}.
Moreover, there is a subclass of Markovian bandits in which index policy is shown to be optimal \cite{gittins1979bandit}.

Two celebrated index definitions are \emph{Gittins index} \cite{gittins1979bandit} for rested bandits and \emph{Whittle index} \cite{whittle1988restless} for restless bandits.
Yet, as mentioned in \cite[Chapter~14]{whittle1996optimal}, the existence of Whittle index is guaranteed only for restless bandits that satisfy a so-called indexability property.
To the best of our knowledge, there is no efficient general-purpose algorithm to test the indexability in restless bandits.
Meanwhile, despite its desirable computational complexity compared to the solutions of dynamic programming, there are very few RL algorithms that leverage index policy in learning with Markovian bandits.
These raise two grand questions in this thesis:
\begin{itemize}
    \item {\color{myblue}\bfseries\large How to efficiently test the indexability and compute the Whittle index?}
    \item {\color{myblue}\bfseries\large Can index policy be a pillar for RL algorithms when learning in Markovian bandits?}
\end{itemize}

\section{Contributions}

Inspired by the challenging questions above, this thesis has several goals that can be regrouped into two main parts: (1) efficiently test the indexability and compute the Whittle index for Markovian bandits and (2) learning with efficiency in Markovian bandits.

%Markovian bandit can be viewed as a classical Markov decision process (MDP) embedded with a special structure.
%Such MDPs suffer, however, from what is called ``the curse of dimensionality'', which basically says that the state size of the Markov process is exponential in the number of arms.
%This implies that the complexity of computing an optimal policy based on classical iterative algorithms such as value iteration or dynamic programming is exponential in the number of arms.
%Outstandingly, an optimal policy for rested Markovian bandit with discount was found by Gittins \cite{gittins1979bandit} in about 1970.
%The proposed policy is an index policy that demands a computation on each arm individually.
%So, Gittins index policy provides an exponential computational improvement.
%In consequence, most of the recent solutions for restless Markovian bandits resort to divide-and-conquer approach: decompose the original multidimensional problem into multiple one-dimensional problems.
%Yet, it is shown that the undiscounted restless bandit problem is PSPACE-hard \cite[Theorem~4]{papadimitriou1994complexity}.

%In about 1980, Whittle proposed an index-based policy, now called \emph{Whittle index policy}, in his seminal paper \cite{whittle1988restless} and argued that the proposed policy is asymptotically optimal as the number of arms grows to infinity.
%This asymptotic optimality was later established in \cite{weber1990index}.
%Since it is an index policy, Whittle's heuristic also provides an exponential computational improvement in restless bandit problems as its complexity is linear in the number of arms.
%Moreover, despite being a heuristic, Whittle index policy performs extremely well in many applications \cite{glazebrook2002index, ansell2003whittle, glazebrook2006some, avrachenkov2013congestion, akbarzadeh2019restless}.

%Yet, there are two critical challenges in using Whittle index policy.
%First, as mentioned in \cite[Chapter~14]{whittle1996optimal}, the existence of Whittle index is guaranteed only for restless bandits that satisfy a so-called indexability property.
%To the best of our knowledge, there is no efficient general purpose algorithm to test indexability in undiscounted restless bandit.

%Second, Whittle index computation for general indexable restless bandits is unsettled.
%In \cite{nino2007dynamic}, Nino-Mora proposed an algorithm, called the adaptive greedy algorithm, to compute Whittle indices when the model satisfies a technical condition called partial conservation laws (PCL).
%That is, if the restless bandit is PCL-indexable, the Whittle index can be computed by the adaptive greedy algorithm.
%However, being PCL-indexable is more restrictive than just being indexable.
%Last but not least, different implementations (see \eg, \cite{nino2020fast, akbarzadeh2020conditions}) of the adaptive greedy algorithm are proposed for discounted restless bandit, and they run in cubic time (we will see the meaning of cubic time in Chapter~\ref{ch:mb}).
%To the best of our knowledge no efficient and explicit implementations is proposed for undiscounted case.

%There have been many studies on applying reinforcement learning (RL) methods for bandit problems.
%For instance, \cite{dann2017unifying} proposed a method called Uniform-PAC for contextual bandits, \cite{zanette2018problem} prescribed an agnostic approach that guaranteed RL algorithms' performance, or \cite{jiang2017contextual} introduced contextual decision processes for RL exploration with function approximation.
%However, none of these studies are adaptable for Markovian bandits \cite{nakhleh2021neurwin}.
%On the other hand, there have been significant advances in the theoretical understanding of RL for generic tabular MDPs.
%A common performance measure of RL algorithms is the \emph{regret} that compares the performance of an oracle, an entity that knows everything, with the performance of the learning algorithm.
%Broadly speaking, there are two families of model-based RL algorithms that achieve optimal regret in generic tabular MDPs.
%One uses the optimism in face of uncertainty (OFU) principle and the other uses posterior sampling.
%The recent algorithms from both approaches achieve a regret that is upper bounded linearly or sublinearly in the size of the MDP state space.  
%So, directly applied to learning Markovian bandit problem, they suffer from the curse of dimensionality inflected by the state size of the Markovian bandit model.

Firstly, we design an efficient single algorithm that tests indexability and computes the Whittle index for both discounted and undiscounted restless bandits as well as the Gittins index for discounted rested bandits.

Secondly, we adapt some generic RL algorithms to Markovian bandit problem in order to remove the exponentiality from their regret.
We study different aspects of the adapted RL algorithms depending on the nature of Markovian bandit: in discounted rested bandit, policy computation and regret are the focus and in undiscounted restless bandit, the implication of the structure of arms in the structure of bandit and in regret come under the spotlight.

To make these contributions explicit, we divide this thesis into three parts.

\subsection{Part~{\ref{part:background}}: Background}

In this part, we recall the existing problem setups and results in the literature.
This part serves as the basis to understand our contributions and to position them among the vast literature of RL and Markovian bandits.

We present the formalism of Markov decision process (MDP) in Chapter~\ref{ch:mdp}: we give the notations, optimization criteria as well as the existing theoretical results in MDP.
This chapter is the main pillar of all the chapters that follow.

Chapter~\ref{ch:rl} consists of a summary of existing learning setups in RL, regret definition that is used as a performance metric of learning algorithms, baseline regret or minimax regret lower bound, as well as several existing results in RL with generic tabular MDPs.

We present the formalism of Markovian bandit in Chapter~\ref{ch:mb}. We give the notations, existing setups and optimization criteria in Markovian bandit literature.
We finish this chapter by outlining all the questions that are discussed in the chapters that follow.

\subsection{Part~{\ref{part:idx}}: Indexability}

In this part, we present our contributions for the computational side of Markovian bandit literature.

In Chapter~\ref{ch:indexability}, we point out the possible ambiguities in the classical definition of indexability in restless Markovian bandit by providing a few simple counter-examples.
This leads us to introduce a new notion of Bellman optimality in the MDP with average reward criterion.
This new notion is then used in our definition of indexability that is thoroughly detailed in the chapter.
Then, we give the corresponding definition of Whittle index accompanied by illustrative examples.
We complete the chapter by studying the properties of the Bellman optimality that are useful for indexability test and index computation.

In Chapter~\ref{ch:index_computation}, we present a new algorithm to test indexability presented in the previous chapter and compute the Whittle index of any finite state restless arm.
It is a single algorithm that tests indexability and computes the Whittle index in either discounted or undiscounted restless bandits as well as the Gittins index in discounted rested bandits.
Moreover, to the best of our knowledge, this algorithm is the first algorithm to achieve subcubic theoretical computational complexity.
That is, if the considered arm has $S$ states, then the best variant of our algorithm performs $\landauO(S^{2.5286})$ arithmetic operations\footnote{multiplications and additions of real numbers, regardless of their values.}.
%That is, if the considered arm has $S$ states, then our algorithm perform $\landauO(S^3)$ (the constant for $S^3$ is explicit in the chapter) arithmetic operations\footnote{multiplications and additions of real numbers, regardless of their values.}.
This is made possible by a sporadic use of the fastest matrix multiplication method of \cite{coppersmith1987matrix} and the Sherman-Morrison formula.
Thanks to the current implementation of matrix multiplication in python, our algorithm is implemented to run in subcubic time in python programming language.
We also present a few numerical experiments that witness the subcubic achievement of our algorithm.
%The best variant of our implementation is about 20 times faster than the current best algorithm\footnote{However, the comparison has its limit because the execution time is recorded on different machines and from an implementation in different programming languages.}.
The code of all experiments is available at \url{https://gitlab.inria.fr/markovianbandit/efficient-whittle-index-computation}.
All variants of our implementation are available in the form of open-source python package installable by a simple command line: \texttt{pip install markovianbandit-pkg}.
Finally, we believe that there is still room for improvement in our algorithm such as exploiting the sparse structure of the arms in Markovian bandits.
We leave this question to the future work.

\subsection{Part~{\ref{part:learning}}: Learning in Markovian bandits}

In this part, we present our contributions for the learning side of Markovian bandit literature.

In Chapter~\ref{ch:learning_rested}, we consider an episodic RL problem in which the unknown environment is a rested Markovian bandit having $n$ arms and $S$ states per arm and the episode length is geometrically distributed.
So, the bandit has $S^n$ states in total.
Given that Gittins index policy is optimal and computationally efficient when the bandit is known \cite{gittins1979bandit}, we compare the optimism in face of uncertainty (OFU) principle method with posterior sampling in terms of runtime and learning performance encoded by regret.
To do so, we adapt UCRL2 \cite{jaksch2010near} and UCBVI \cite{azar2017minimax}, two different algorithms from OFU family, and PSRL \cite{osband2013more}, an algorithm from posterior sampling family, to rested bandit with discount.
The adapted versions are respectively called MB-UCRL2, MB-UCBVI, and MB-PSRL where ``MB'' stands for Markovian bandit.
We show that the three MB-* algorithms have a regret upper bounded like $\landauO(S\sqrt{nK})$ where $K$ is the number of episodes.
This is an exponential improvement in terms of the number of arms $n$.
We also derive the baseline regret for learning algorithms in rested bandit with discount.
That is, any algorithm learning in discounted rested bandit suffers a regret that is at least $\Omega(\sqrt{SnK})$.
For computational aspect, we show that UCRL2 and its variants that use extended value iteration \cite{jaksch2010near} cannot leverage Gittins index policy to achieve efficient policy computation.
This is because such algorithms rely on the confidence bonus on arms' transition; which does not allow them to guarantee the OFU principle when working on each arm independently of the other arms.
Lastly, we perform several numerical experiments that advocate the good behavior of MB-PSRL.
These experiments are reproducible by following this link \url{https://gitlab.inria.fr/kkhun/learning-in-rested-markovian-bandit}.
While acknowledging that posterior sampling has a weaker regret guarantee, we conclude that this approach has an upper hand in terms of problem adaptability when compared to OFU method.
This is vital when working with the problems having a special structure such as weakly coupled MDPs.

In Chapter~\ref{ch:learning_restless}, we consider the RL problems in which the unknown environment is a restless Markovian bandit with average reward criterion.
We study how the structure of arms translate in the structure of restless bandit.
In particular, we show that no RL algorithms can perform uniformly well in restless bandits whose arms all are unichain and have a bounded span of any local bias function.
This inspires us to study the restless bandit whose arms all have no local transient state.
We provide examples that show that a restless bandit, whose arms all are ergodic, can be multichain.
Also, an ergodic restless bandit can have an arbitrarily large mixing time although its arms all are ergodic and have an upper bounded mixing time.
We also provide a piece of positive result showing that if all arms are ergodic, then the restless bandit is communicating.
Moreover, if all arms have an ergodicity coefficient smaller than $1$, then the corresponding restless bandit also has an ergodicity coefficient smaller than $1$.
Finally, we discuss a few issues such as which baseline policy to use in regret definition when learning in general restless bandit.
All of our arguments imply that defining a subclass of restless bandits having desirable properties for learning is important but difficult.

\section{Organization of the thesis}

We provide the thesis organization in \figurename~\ref{fig:flo}.

\tikzset{rect/.style={rectangle, rounded corners, minimum width=4.5cm, minimum
height=1cm,text centered, draw=amblu, text=myblue, fill=RoyalBlue!5, drop shadow},
arrow/.style={thick,->,>=stealth}}

\begin{figure}[h]
\begin{center}
\begin{tikzpicture}[node distance=1cm]

\node (mdp) [rect, label={[font=\small\sffamily,color=gray25,name=part1]above:{Part I: Background}}] {Chapter 2: Markov Decision Process};
\node (rl) [rect, below right = 0.75cm and -2.5cm of mdp] {Chapter 3: Reinforcement Learning};
\node (mb) [rect, below left = 0.75cm and -2.1cm of mdp] {Chapter 4: Markovian Bandits};

\node (idx) [rect, below = 1.25cm of mb, label={[font=\small\sffamily,color=gray25,name=part2]above left:{Part II: Indexability}}] {Chapter 5: Indexability};
\node (widx) [rect, below = 0.75cm of idx] {Chapter 6: Index Computation};

\node (rested) [rect, below = 3cm of rl, label={[font=\small\sffamily,color=gray25,name=part3]above right:{Part III: Learning}}] {Chapter 7: Learning Rested Bandit};
\node (restless) [rect, below = 0.75cm of rested] {Chapter 8: Learning Restless Bandit};

%\node (litrev) [rect, below left = 0.75cm and -1.1cm  of mdp,
%label={[font=\small\sffamily,name=label3]above left:{Theoretical analysis}}] {Literature Review};
%\node (result) [rect, below right = 2.50cm and -1.1cm  of mdp] {Results};

\draw [arrow] (mdp) -| (rl);
\draw [arrow] (mdp) -| (mb);
\draw [arrow] (mb) -- (idx);
\draw [arrow] (mb) -- (rested);
\draw [arrow] (idx) -- (widx);
\draw [arrow,dashed] (widx) -- (rested);
\draw [arrow] (rl) -- (rested);
\draw [arrow] (rested) -- (restless);

%\coordinate (aux) at (mb.south -| rested);

\begin{scope}[on background layer]
\node[draw,dashed,amdove,rounded corners,fill=yellow!40,fit=(mdp) (rl) (mb) (part1)]{};
\node[draw,dashed,amdove,rounded corners,fill=yellow!40,fit=(idx) (widx) (part2)]{};
\node[draw,dashed,amdove,rounded corners,fill=yellow!40,fit=(rested) (restless) (part3)]{};
\end{scope}
\end{tikzpicture}
\end{center}
\caption{Thesis organization} \label{fig:flo}
\end{figure}

To facilitate the comprehension of this thesis, we suggest the following flow of reading: One would want to start with Chapter~\ref{ch:mdp} and \ref{ch:mb}, then Part~\ref{part:idx}. Next, one would want to jump back to Chapter~\ref{ch:rl} before diving into Part~\ref{part:learning} and finishing the conclusion.

We highlight that Chapter~\ref{ch:mdp} and \ref{ch:mb} are required to understand our contributions in Part~\ref{part:idx}, and
Part~\ref{part:background} is required to understand our contributions in Part~\ref{part:learning}.

%To introduce my work, I will write a nice introduction in the following.
%Citation example for the Top500 website~\cite{top500} and some random paper~\cite{graham1969}.
%
%\section{Hello World}
%\lipsum[1-5]

\endgroup
