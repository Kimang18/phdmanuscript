\chapter{Introduction}
\label{chapter:introduction}

Markovian bandits form a subclass of multi-armed bandit problems in which each arm has an internal state that evolves over time in a Markovian manner, as a function of the decision makerâ€™s actions.
In such a problem, at each decision time, the decision maker observes the state of all arms and chooses which one to activate.
When the state of an arm evolves only when this arm is chosen, one falls into the category of rested Markovian bandits.
When the state of an arm can also evolve when the arm is not chosen, the problem is called a restless bandit problem.
Markovian bandits have been applied to many resource allocations and scheduling problems such as wireless communication \cite{raghunathan2008index, liu2010indexability, aalto2019whittle}, web crawling \cite{nino2014dynamic, avrachenkov2022whittle}, congestion control \cite{avrachenkov2013congestion, avrachenkov2018impulsive}, queueing systems \cite{glazebrook2009index, aalto2009gittins, archibald2009indexability, aalto2011properties, larranaga2015asymptotically, borkar2017whittle, scully2018soap}, and clinical trials \cite{villar2015multi}.

Markovian bandit can be viewed as a classical Markov decision process (MDP) embedded with a special structure.
Such MDPs suffer, however, from what is called ``the curse of dimensionality'', which basically says that the state size of the Markov process is exponential in the number of arms.
This implies that the complexity of computing an optimal policy based on classical iterative algorithms such as value iteration or dynamic programming is exponential in the number of arms.
Outstandingly, an optimal policy for rested Markovian bandit with discount was found by Gittins \cite{gittins1979bandit} in about 1970.
The proposed policy is an index policy that demands a computation on each arm individually.
So, Gittins index policy provides an exponential improvement.
In consequence, most of the later studied solutions for restless Markovian bandits resort to divide-and-conquer approach: decompose the original multi-dimensional problem into multiple one-dimensional problems.
Yet, it is shown that the restless bandit with no discount is PSPACE-hard \cite[Theorem~4]{papadimitriou1994complexity}.

In about 1980, <++>



\section{Topic of the thesis}

\section{Outline of the thesis}

\begin{enumerate}
    \item Part 1 Background: about Markov decision process and reinforcement learning
    \item Part 2 Indexability: Markovian bandit and our first contribution
    \item Part 3 Learning in Markovian bandit: rested MB and restless MB
\end{enumerate}

Chapter~\ref{ch:mdp}, \ref{ch:mb_problem} are required to understand our contribution in Chapter~\ref{ch:index_computation}.
Chapter~\ref{ch:mdp}, \ref{ch:mb_problem}, and \ref{ch:rl} are required to understand our contribution in Chapter~\ref{ch:learning_rested} and \ref{ch:learning_restless}.

We suggest the following flow of reading: start with Chapter~\ref{ch:mdp} and \ref{ch:mb_problem}, then Chapter~\ref{ch:index_computation}. Next, jump back to Chapter~\ref{ch:rl}. Finally, finish Part~\ref{part:learning} and the conclusion.

%To introduce my work, I will write a nice introduction in the following.
%Citation example for the Top500 website~\cite{top500} and some random paper~\cite{graham1969}.
%
%\section{Hello World}
%\lipsum[1-5]
